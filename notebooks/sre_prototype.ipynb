{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SREnity - Enterprise SRE Agent Prototype\n",
        "\n",
        "This notebook contains the development and testing of the SREnity agentic RAG system for production incident resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (1.109.1)\n",
            "Requirement already satisfied: langchain in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (0.3.31)\n",
            "Requirement already satisfied: qdrant-client in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (1.15.1)\n",
            "Requirement already satisfied: python-dotenv in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (1.1.1)\n",
            "Requirement already satisfied: pandas in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (2.3.4)\n",
            "Requirement already satisfied: requests in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (2.32.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (4.14.2)\n",
            "Requirement already satisfied: ragas in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (0.2.10)\n",
            "Requirement already satisfied: rank-bm25 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (0.2.2)\n",
            "Requirement already satisfied: tavily-python in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (0.7.12)\n",
            "Requirement already satisfied: cohere in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (5.19.0)\n",
            "Requirement already satisfied: langsmith in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (0.4.37)\n",
            "Requirement already satisfied: markdownify in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (1.2.0)\n",
            "Requirement already satisfied: rapidfuzz in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (3.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from openai) (2.12.2)\n",
            "Requirement already satisfied: sniffio in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from qdrant-client) (1.75.1)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from qdrant-client) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from qdrant-client) (6.33.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from requests) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: datasets in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from ragas) (4.2.0)\n",
            "Requirement already satisfied: tiktoken in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from ragas) (0.12.0)\n",
            "Requirement already satisfied: langchain_openai in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from ragas) (0.3.35)\n",
            "Requirement already satisfied: nest-asyncio in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from ragas) (0.3.4)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from ragas) (5.6.3)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from cohere) (1.12.1)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from cohere) (2.41.4)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from cohere) (0.22.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from cohere) (2.32.4.20250913)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langsmith) (3.11.3)\n",
            "Requirement already satisfied: packaging>=23.2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langsmith) (0.25.0)\n",
            "Requirement already satisfied: six<2,>=1.15 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from markdownify) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from tiktoken->ragas) (2025.9.18)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from tokenizers<1,>=0.15->cohere) (0.35.3)\n",
            "Requirement already satisfied: filelock in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from datasets->ragas) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from datasets->ragas) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from datasets->ragas) (0.4.0)\n",
            "Requirement already satisfied: xxhash in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from datasets->ragas) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->ragas) (2025.9.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.1.10)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install openai langchain langchain-community qdrant-client python-dotenv pandas numpy requests beautifulsoup4 ragas rank-bm25 tavily-python cohere langsmith markdownify rapidfuzz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and Setup\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Set up minimal logging\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "from src.utils.config import get_config\n",
        "config = get_config()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading - GitLab Runbooks\n",
        "\n",
        "**Data Source:** Production runbooks from https://runbooks.gitlab.com/ - comprehensive SRE documentation covering infrastructure, databases, CI/CD pipelines, monitoring, and incident response procedures. These are real-world operational guides used by GitLab's SRE team.\n",
        "\n",
        "**Multi-Service Foundation:** 696 enterprise runbooks covering Redis, PostgreSQL, Elasticsearch, CI/CD, monitoring, and more. This notebook focuses on **Redis service** (145 docs) to demonstrate the RAG pipeline, but the architecture supports filtering by any service combination for real-world multi-system incidents.\n",
        "\n",
        "**Smart loading:** Checks for existing `data/runbooks/gitlab_runbooks.json` file and loads/downloads accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved runbooks...\n",
            "Loaded 696 total documents\n",
            "Filtered to 33 Redis documents\n"
          ]
        }
      ],
      "source": [
        "# Smart GitLab Runbook Loading with Service Filtering\n",
        "from src.utils.document_loader import download_gitlab_runbooks, save_documents, load_saved_documents\n",
        "from pathlib import Path\n",
        "\n",
        "def filter_by_service(documents, services=['redis']):\n",
        "    \"\"\"Filter documents by service type\"\"\"\n",
        "    filtered = []\n",
        "    for doc in documents:\n",
        "        source = doc.metadata.get('source', '').lower()\n",
        "        if any(service in source for service in services):\n",
        "            filtered.append(doc)\n",
        "    return filtered\n",
        "\n",
        "# Check if runbooks file exists\n",
        "runbooks_file = Path(\"../data/runbooks/gitlab_runbooks.json\")\n",
        "\n",
        "if runbooks_file.exists():\n",
        "    print(\"Loading saved runbooks...\")\n",
        "    documents = load_saved_documents()\n",
        "    print(f\"Loaded {len(documents)} total documents\")\n",
        "else:\n",
        "    print(\"Downloading fresh runbooks...\")\n",
        "    documents = download_gitlab_runbooks()\n",
        "    print(f\"Downloaded {len(documents)} documents\")\n",
        "    \n",
        "    print(\"Saving documents...\")\n",
        "    filepath = save_documents(documents)\n",
        "    print(f\"Saved to {filepath}\")\n",
        "\n",
        "# Filter to Redis services only\n",
        "documents = filter_by_service(documents, ['redis'])\n",
        "print(f\"Filtered to {len(documents)} Redis documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Pipeline - Document Processing\n",
        "\n",
        "This section implements the core RAG pipeline:\n",
        "1. **Document Chunking** - Split Redis documents into manageable chunks\n",
        "2. **Vector Database Setup** - Create embeddings and store in Qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing HTML documents to markdown...\n",
            "HTML to Markdown conversion results:\n",
            "  Original: 290,437 - 575,312 chars\n",
            "  Markdown: 52,226 - 96,814 chars\n",
            "  Reduction: 81.5%\n",
            "Chunking preprocessed documents...\n",
            "Created 685 chunks (631,830 tokens, avg 922 tokens/chunk)\n"
          ]
        }
      ],
      "source": [
        "# Document Preprocessing and Chunking\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import tiktoken\n",
        "from src.utils.document_loader import preprocess_html_documents\n",
        "\n",
        "def chunk_documents_with_tiktoken(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"Split documents using tiktoken for accurate token counting\"\"\"\n",
        "    \n",
        "    # Get tiktoken encoding for the configured model\n",
        "    encoding = tiktoken.encoding_for_model(config.openai_model)\n",
        "    \n",
        "    # Create text splitter with tiktoken length function\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=lambda text: len(encoding.encode(text)),\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    # Split documents\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_tokens = sum(len(encoding.encode(chunk.page_content)) for chunk in chunks)\n",
        "    avg_tokens = total_tokens / len(chunks) if chunks else 0\n",
        "    \n",
        "    print(f\"Created {len(chunks)} chunks ({total_tokens:,} tokens, avg {avg_tokens:.0f} tokens/chunk)\")\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Preprocess HTML documents to markdown\n",
        "print(\"Preprocessing HTML documents to markdown...\")\n",
        "processed_documents = preprocess_html_documents(documents)\n",
        "\n",
        "# Chunk the preprocessed documents\n",
        "print(\"Chunking preprocessed documents...\")\n",
        "chunks = chunk_documents_with_tiktoken(processed_documents, chunk_size=1000, chunk_overlap=200)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector database not found. Creating new one...\n",
            "Creating vector store at: ../qdrant_db\n",
            "Stored 685 chunks in Qdrant at ../qdrant_db\n"
          ]
        }
      ],
      "source": [
        "# Qdrant Vector Database Setup\n",
        "from src.utils.config import get_model_factory\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from pathlib import Path\n",
        "\n",
        "def create_embeddings_and_store(chunks):\n",
        "    \"\"\"Create embeddings and store in Qdrant\"\"\"\n",
        "    \n",
        "    # Get model factory and create embeddings\n",
        "    model_factory = get_model_factory()\n",
        "    embeddings = model_factory.get_embeddings()\n",
        "\n",
        "    # Log the Qdrant URL configuration\n",
        "    print(f\"Creating vector store at: {config.qdrant_url}\")\n",
        "    \n",
        "    # Create vector store with local file storage\n",
        "    vector_store = Qdrant.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        path=config.qdrant_url\n",
        "    )\n",
        "\n",
        "    print(f\"Stored {len(chunks)} chunks in Qdrant at {config.qdrant_url}\")\n",
        "    return vector_store\n",
        "\n",
        "def load_existing_vector_store():\n",
        "    \"\"\"Load existing Qdrant vector store\"\"\"\n",
        "    model_factory = get_model_factory()\n",
        "    embeddings = model_factory.get_embeddings()\n",
        "    \n",
        "    # Load existing vector store from file path\n",
        "    vector_store = Qdrant.from_existing_collection(\n",
        "        embedding=embeddings,\n",
        "        path=config.qdrant_url,\n",
        "        collection_name=config.qdrant_collection_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Loaded existing vector store from {config.qdrant_url}\")\n",
        "    return vector_store\n",
        "\n",
        "# Check if vector database exists, otherwise create it\n",
        "qdrant_path = Path(config.qdrant_url)\n",
        "\n",
        "if qdrant_path.exists():\n",
        "    print(\"Vector database exists. Loading...\")\n",
        "    vector_store = load_existing_vector_store()\n",
        "else:\n",
        "    print(\"Vector database not found. Creating new one...\")\n",
        "    vector_store = create_embeddings_and_store(chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthetic Data Generation (SDG)\n",
        "\n",
        "This section creates test data for runbook helper evaluation:\n",
        "1. **Question Generation** - Create how-to questions from Redis runbook chunks\n",
        "2. **Answer Generation** - Generate expected answers from runbook content\n",
        "3. **Test Dataset** - Create evaluation dataset with ground truth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating synthetic test data using RAGAS...\n",
            "Generating 8 test questions from 33 documents...\n",
            "Using preprocessed markdown documents for SDG...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Applying CustomNodeFilter:  91%|█████████ | 193/212 [02:48<00:19,  1.01s/it]ERROR:ragas.testset.transforms.engine:unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7hMVLGqXydr4X3d1KFkKl6Re on tokens per min (TPM): Limit 200000, Used 200000, Requested 3844. Please try again in 1.153s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Generating personas: 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]                                             \n",
            "Generating Scenarios: 100%|██████████| 3/3 [00:07<00:00,  2.34s/it]\n",
            "Generating Samples: 100%|██████████| 9/9 [00:04<00:00,  2.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 9 test samples\n"
          ]
        }
      ],
      "source": [
        "# Synthetic Data Generation using RAGAS\n",
        "from ragas.testset.synthesizers.generate import TestsetGenerator\n",
        "from src.utils.config import get_model_factory\n",
        "from langchain_core.documents import Document\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def generate_test_dataset(documents, num_questions=10):\n",
        "    \"\"\"Generate synthetic test data using RAGAS\"\"\"\n",
        "    \n",
        "    print(f\"Generating {num_questions} test questions from {len(documents)} documents...\")\n",
        "    \n",
        "    # Use preprocessed documents (already converted to markdown)\n",
        "    print(f\"Using preprocessed markdown documents for SDG...\")\n",
        "    \n",
        "    # Get model factory for LLM and embeddings\n",
        "    model_factory = get_model_factory()\n",
        "    \n",
        "    # Generate test dataset using TestsetGenerator\n",
        "    generator = TestsetGenerator.from_langchain(\n",
        "        llm=model_factory.get_llm(),\n",
        "        embedding_model=model_factory.get_embeddings()\n",
        "    )\n",
        "    \n",
        "    test_data = generator.generate_with_langchain_docs(\n",
        "        documents=documents,  # Use preprocessed documents\n",
        "        testset_size=num_questions\n",
        "    )\n",
        "    \n",
        "    print(f\"Generated {len(test_data)} test samples\")\n",
        "    return test_data\n",
        "\n",
        "# Generate test dataset (start with smaller size to test)\n",
        "print(\"Creating synthetic test data using RAGAS...\")\n",
        "test_dataset = generate_test_dataset(processed_documents, num_questions=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 9 test questions\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(eval_sample, user_input='What is WAL-G in the...</td>\n",
              "      <td>(synthesizer_name, single_hop_specifc_query_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(eval_sample, user_input='What is the signific...</td>\n",
              "      <td>(synthesizer_name, single_hop_specifc_query_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(eval_sample, user_input='How chef help in set...</td>\n",
              "      <td>(synthesizer_name, single_hop_specifc_query_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(eval_sample, user_input='What is the signific...</td>\n",
              "      <td>(synthesizer_name, multi_hop_abstract_query_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(eval_sample, user_input='What are the key asp...</td>\n",
              "      <td>(synthesizer_name, multi_hop_abstract_query_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(eval_sample, user_input='What are the key asp...</td>\n",
              "      <td>(synthesizer_name, multi_hop_abstract_query_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(eval_sample, user_input='How do you delete ca...</td>\n",
              "      <td>(synthesizer_name, multi_hop_specific_query_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(eval_sample, user_input='How does the RDB for...</td>\n",
              "      <td>(synthesizer_name, multi_hop_specific_query_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(eval_sample, user_input=\"How does the use of ...</td>\n",
              "      <td>(synthesizer_name, multi_hop_specific_query_sy...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  \\\n",
              "0  (eval_sample, user_input='What is WAL-G in the...   \n",
              "1  (eval_sample, user_input='What is the signific...   \n",
              "2  (eval_sample, user_input='How chef help in set...   \n",
              "3  (eval_sample, user_input='What is the signific...   \n",
              "4  (eval_sample, user_input='What are the key asp...   \n",
              "5  (eval_sample, user_input='What are the key asp...   \n",
              "6  (eval_sample, user_input='How do you delete ca...   \n",
              "7  (eval_sample, user_input='How does the RDB for...   \n",
              "8  (eval_sample, user_input=\"How does the use of ...   \n",
              "\n",
              "                                                   1  \n",
              "0  (synthesizer_name, single_hop_specifc_query_sy...  \n",
              "1  (synthesizer_name, single_hop_specifc_query_sy...  \n",
              "2  (synthesizer_name, single_hop_specifc_query_sy...  \n",
              "3  (synthesizer_name, multi_hop_abstract_query_sy...  \n",
              "4  (synthesizer_name, multi_hop_abstract_query_sy...  \n",
              "5  (synthesizer_name, multi_hop_abstract_query_sy...  \n",
              "6  (synthesizer_name, multi_hop_specific_query_sy...  \n",
              "7  (synthesizer_name, multi_hop_specific_query_sy...  \n",
              "8  (synthesizer_name, multi_hop_specific_query_sy...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Display cached test questions\n",
        "df = pd.DataFrame(test_dataset)\n",
        "print(f\"Loaded {len(df)} test questions\")\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 9 test questions to ../data/sdg/redis_test_questions_ragas.json\n",
            "Test dataset cached with correct field mapping\n"
          ]
        }
      ],
      "source": [
        "# Updated Cache SDG Questions with correct RAGAS field mapping\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def save_test_dataset_fixed(test_dataset, filename=\"redis_test_questions_ragas.json\"):\n",
        "    \"\"\"Save test dataset to file for reuse with correct RAGAS field mapping\"\"\"\n",
        "    data_dir = Path(\"../data/sdg\")\n",
        "    data_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Convert test dataset to serializable format\n",
        "    test_data = []\n",
        "    samples = list(test_dataset)  # Convert EvaluationDataset to list\n",
        "    \n",
        "    for sample in samples:\n",
        "        if hasattr(sample, 'eval_sample'):\n",
        "            eval_sample = sample.eval_sample\n",
        "            if hasattr(eval_sample, 'model_dump'):\n",
        "                sample_dict = eval_sample.model_dump()\n",
        "            else:\n",
        "                sample_dict = eval_sample.__dict__\n",
        "        else:\n",
        "            # Fallback for direct access\n",
        "            if hasattr(sample, 'model_dump'):\n",
        "                sample_dict = sample.model_dump()\n",
        "            else:\n",
        "                sample_dict = sample.__dict__\n",
        "        \n",
        "        test_data.append({\n",
        "            'question': sample_dict.get('user_input', 'N/A'),\n",
        "            'answer': sample_dict.get('reference', 'N/A'),\n",
        "            'contexts': sample_dict.get('reference_contexts', ['N/A']),  # Keep as list for RAGAS\n",
        "            'ground_truths': [sample_dict.get('reference', 'Generated by RAGAS')]  # List for RAGAS\n",
        "        })\n",
        "    \n",
        "    filepath = data_dir / filename\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"Saved {len(test_data)} test questions to {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Save the generated test dataset with correct field mapping\n",
        "if 'test_dataset' in locals():\n",
        "    cache_file = save_test_dataset_fixed(test_dataset)\n",
        "    print(f\"Test dataset cached with correct field mapping\")\n",
        "else:\n",
        "    print(\"No test_dataset found to cache\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Pipeline - Retrieval and Testing\n",
        "\n",
        "This section implements the core RAG functionality:\n",
        "1. **Naive Retrieval** - Basic semantic search using vector similarity\n",
        "2. **Incident Testing** - Test the system with sample Redis incidents\n",
        "3. **Response Generation** - Generate runbook recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How to monitor Redis memory usage?\n",
            "==================================================\n",
            "\n",
            "**Document 1:**\n",
            "- **Source:** https://runbooks.gitlab.com/redis-cluster-sessions/\n",
            "- **Title:** Redis Cluster Sessions Service | Runbooks\n",
            "- **Content:** ## Monitoring/Alerting\n",
            "[Section titled “Monitoring/Alerting”](#monitoringalerting)\n",
            "Generally the same as other Redis clusters, but with special handling for monitoring maximum memory (as a proportion of the configured limit, not the system limit), and alerting if redis\\_evicted\\_keys\\_total raises a...\n",
            "\n",
            "\n",
            "**Document 2:**\n",
            "- **Source:** https://runbooks.gitlab.com/redis/redis/\n",
            "- **Title:** Troubleshooting | Runbooks\n",
            "- **Content:** ### Redis latency monitoring framework\n",
            "[Section titled “Redis latency monitoring framework”](#redis-latency-monitoring-framework)\n",
            "Redis provides a latency diagnostic tool: <https://redis.io/topics/latency-monitor>\n",
            "You may need to enable it with `CONFIG SET latency-monitor-threshold 100`.\n",
            "From <https...\n",
            "\n",
            "\n",
            "**Document 3:**\n",
            "- **Source:** https://runbooks.gitlab.com/redis/redis/\n",
            "- **Title:** Troubleshooting | Runbooks\n",
            "- **Content:** ### redis-cli\n",
            "[Section titled “redis-cli”](#redis-cli)\n",
            "Run:\n",
            "- [INFO command](https://redis.io/commands/info)\n",
            "- [MEMORY\\_STATS command](https://redis.io/commands/memory-stats)\n",
            "NOTE: DO NOT USE [MONITOR] COMMAND! It streams back every command processed by the Redis server which can double the use of r...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Redis Runbook Assistant - Retrieval System\n",
        "from src.utils.config import get_model_factory\n",
        "\n",
        "def naive_retrieval(query, vector_store, k=5):\n",
        "    \"\"\"Basic semantic retrieval using vector similarity\"\"\"\n",
        "    docs = vector_store.similarity_search(query, k=k)\n",
        "    return docs\n",
        "\n",
        "def format_retrieved_docs(docs):\n",
        "    \"\"\"Format retrieved documents for display\"\"\"\n",
        "    formatted = []\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        source = doc.metadata.get('source', 'Unknown')\n",
        "        title = doc.metadata.get('title', 'No title')\n",
        "        content_preview = doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content\n",
        "        \n",
        "        formatted.append(f\"\"\"\n",
        "**Document {i}:**\n",
        "- **Source:** {source}\n",
        "- **Title:** {title}\n",
        "- **Content:** {content_preview}\n",
        "\"\"\")\n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "# Test retrieval with runbook-style questions\n",
        "test_query = \"How to monitor Redis memory usage?\"\n",
        "print(f\"Query: {test_query}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "retrieved_docs = naive_retrieval(test_query, vector_store, k=3)\n",
        "formatted_results = format_retrieved_docs(retrieved_docs)\n",
        "print(formatted_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Redis Runbook Assistant\n",
            "============================================================\n",
            "\n",
            "**Question 1: How to backup Redis data using RDB snapshots?**\n",
            "--------------------------------------------------\n",
            "# Backup Redis Data Using RDB Snapshots\n",
            "\n",
            "## Direct Answer\n",
            "To backup Redis data using RDB snapshots, you will need to ensure that the Redis instance is configured to create RDB snapshots, and then you can manually copy the `dump.rdb` file from the Redis container to a safe location.\n",
            "\n",
            "## Step-by-Step Instructions\n",
            "\n",
            "### Step 1: Verify RDB Snapshot Configuration\n",
            "1. Ensure that your Redis instance is configured to create RDB snapshots. This is typically managed by the `gitlab-redis-backup` cookbook, which runs a cron-like job for snapshotting.\n",
            "2. Confirm that the Redis instance is not configured to run RDB snapshots on primary nodes unless necessary.\n",
            "\n",
            "### Step 2: Identify the Redis Container\n",
            "1. List the Redis pods to find the one you want to back up:\n",
            "   ```bash\n",
            "   kubectl get pods -n redis\n",
            "   ```\n",
            "2. Identify the pod name of the Redis instance you want to back up.\n",
            "\n",
            "### Step 3: Get the Container ID\n",
            "1. Use the following command to get the container ID of the Redis instance:\n",
            "   ```bash\n",
            "   POD_ID=$(kubectl get pods -n redis -o jsonpath='{.items[0].metadata.name}')\n",
            "   CONTAINER_ID=$(kubectl get pod $POD_ID -n redis -o jsonpath='{.status.containerStatuses[0].containerID}' | cut -d'/' -f3)\n",
            "   ```\n",
            "\n",
            "### Step 4: Confirm the RDB Dump File Exists\n",
            "1. Execute the following command to check if the `dump.rdb` file exists in the Redis container:\n",
            "   ```bash\n",
            "   kubectl exec -it $POD_ID -n redis -- ls -lh /data/dump.rdb\n",
            "   ```\n",
            "\n",
            "### Step 5: Find the Host Path for the RDB Dump\n",
            "1. Find the host path for the container's `/data` volume:\n",
            "   ```bash\n",
            "   DATA_DIR_HOST_PATH=$(crictl inspect $CONTAINER_ID | jq -r '.status.mounts[] | select(.containerPath == \"/data\") | .hostPath')\n",
            "   ```\n",
            "\n",
            "### Step 6: Copy the RDB Dump File\n",
            "1. Confirm that the RDB dump file is fresh enough:\n",
            "   ```bash\n",
            "   sudo ls -lh $DATA_DIR_HOST_PATH/dump.rdb\n",
            "   ```\n",
            "2. Copy the RDB dump file to a directory where you have access without sudo (e.g., your home directory):\n",
            "   ```bash\n",
            "   sudo cp -pi $DATA_DIR_HOST_PATH/dump.rdb ~/\n",
            "   ```\n",
            "\n",
            "### Step 7: Download the RDB Dump File to Your Laptop\n",
            "1. Use `gcloud compute scp` to download the RDB dump file to your local machine:\n",
            "   ```bash\n",
            "   gcloud compute scp --project $GCP_PROJECT $GKE_HOSTNAME:~/dump.rdb .\n",
            "   ```\n",
            "\n",
            "## Key Commands\n",
            "- **List Redis Pods**: \n",
            "  ```bash\n",
            "  kubectl get pods -n redis\n",
            "  ```\n",
            "- **Get Container ID**: \n",
            "  ```bash\n",
            "  CONTAINER_ID=$(crictl ps --quiet --pod $POD_ID --name redis)\n",
            "  ```\n",
            "- **Check RDB File**: \n",
            "  ```bash\n",
            "  kubectl exec -it $POD_ID -n redis -- ls -lh /data/dump.rdb\n",
            "  ```\n",
            "- **Copy RDB File**: \n",
            "  ```bash\n",
            "  sudo cp -pi $DATA_DIR_HOST_PATH/dump.rdb ~/\n",
            "  ```\n",
            "- **Download to Local**: \n",
            "  ```bash\n",
            "  gcloud compute scp --project $GCP_PROJECT $GKE_HOSTNAME:~/dump.rdb .\n",
            "  ```\n",
            "\n",
            "## Important Notes\n",
            "- **Backup Frequency**: Ensure that the backup process is scheduled appropriately to avoid data loss.\n",
            "- **Storage Space**: Verify that there is enough storage space available on the target directory before copying the RDB file.\n",
            "- **Cleanup**: Remember to delete the RDB file from the temporary location after you have successfully downloaded it to avoid unnecessary storage usage:\n",
            "  ```bash\n",
            "  rm ~/dump.rdb\n",
            "  ```\n",
            "- **Monitoring**: Keep an eye on alerts related to RDB snapshots, such as `redisRdbSaveDelayed`, to ensure backups are being created as expected. Check logs with:\n",
            "  ```bash\n",
            "  sudo journalctl -u redis-rdb-backup\n",
            "  ```\n",
            "\n",
            "**Sources Used:** 5 documents\n",
            "============================================================\n",
            "\n",
            "**Question 2: How to configure Redis connection pools for high traffic?**\n",
            "--------------------------------------------------\n",
            "# Configuring Redis Connection Pools for High Traffic\n",
            "\n",
            "## 1. Direct Answer\n",
            "To configure Redis connection pools for high traffic in a 3-node Redis cluster, ensure that your application is set up to efficiently manage connections, utilize connection pooling libraries, and optimize Redis settings for performance. This will help handle increased traffic without overwhelming the Redis instances.\n",
            "\n",
            "## 2. Step-by-Step Instructions\n",
            "\n",
            "### Step 1: Choose a Connection Pooling Library\n",
            "Select a connection pooling library that is compatible with your application framework. Common libraries include:\n",
            "- **Ruby on Rails**: Use `ConnectionPool` gem.\n",
            "- **Java**: Use `JedisPool` or `Lettuce`.\n",
            "- **Python**: Use `redis-py` with connection pooling.\n",
            "\n",
            "### Step 2: Configure Connection Pool Settings\n",
            "Adjust the connection pool settings based on your application's traffic patterns. Key parameters to configure include:\n",
            "- **Max Connections**: Set a limit on the maximum number of connections to prevent overwhelming Redis.\n",
            "- **Min Connections**: Maintain a minimum number of connections to ensure availability.\n",
            "- **Connection Timeout**: Set a timeout for acquiring a connection to avoid blocking.\n",
            "\n",
            "Example configuration for Ruby on Rails:\n",
            "```ruby\n",
            "# config/initializers/redis.rb\n",
            "require 'connection_pool'\n",
            "require 'redis'\n",
            "\n",
            "$redis = ConnectionPool.new(size: 10, timeout: 5) { Redis.new(url: ENV['REDIS_URL']) }\n",
            "```\n",
            "\n",
            "### Step 3: Optimize Redis Configuration\n",
            "Modify Redis configuration settings to improve performance under high traffic:\n",
            "- **maxclients**: Increase the maximum number of clients that can connect to Redis.\n",
            "- **tcp-keepalive**: Set a keepalive interval to maintain connections.\n",
            "- **timeout**: Set a timeout for idle connections to free up resources.\n",
            "\n",
            "Example Redis configuration (`redis.conf`):\n",
            "```conf\n",
            "maxclients 10000\n",
            "tcp-keepalive 60\n",
            "timeout 300\n",
            "```\n",
            "\n",
            "### Step 4: Monitor Redis Performance\n",
            "Use monitoring tools to track Redis performance metrics, such as:\n",
            "- **Connected Clients**: Ensure the number of connected clients is within limits.\n",
            "- **Memory Usage**: Monitor memory saturation to prevent issues.\n",
            "- **Command Latency**: Check for slow commands that may indicate performance bottlenecks.\n",
            "\n",
            "### Step 5: Test Under Load\n",
            "Conduct load testing to simulate high traffic scenarios. Use tools like Apache JMeter or Locust to ensure that your connection pool configuration can handle the expected load without degrading performance.\n",
            "\n",
            "## 3. Key Commands\n",
            "\n",
            "- **Redis CLI to Check Configuration**:\n",
            "```bash\n",
            "redis-cli CONFIG GET maxclients\n",
            "redis-cli CONFIG GET tcp-keepalive\n",
            "redis-cli CONFIG GET timeout\n",
            "```\n",
            "\n",
            "- **To Monitor Redis Performance**:\n",
            "```bash\n",
            "redis-cli INFO clients\n",
            "redis-cli INFO memory\n",
            "redis-cli MONITOR\n",
            "```\n",
            "\n",
            "## 4. Important Notes\n",
            "\n",
            "- **Single-Threaded Nature**: Remember that Redis is single-threaded, so ensure that your application efficiently uses connection pooling to avoid blocking the main thread.\n",
            "- **Network Constraints**: Be aware that network bandwidth may become a bottleneck before CPU usage, especially with large data transfers.\n",
            "- **Scaling**: If you anticipate significant growth in traffic, consider horizontal scaling by adding more Redis nodes to your cluster.\n",
            "- **Testing**: Always test your configuration changes in a staging environment before deploying to production to avoid unexpected issues.\n",
            "\n",
            "By following these steps, you can effectively configure Redis connection pools to handle high traffic efficiently.\n",
            "\n",
            "**Sources Used:** 5 documents\n",
            "============================================================\n",
            "\n",
            "**Question 3: How to troubleshoot Redis replication lag issues?**\n",
            "--------------------------------------------------\n",
            "# Troubleshooting Redis Replication Lag Issues\n",
            "\n",
            "## Direct Answer\n",
            "To troubleshoot Redis replication lag issues, you need to monitor latency, check for slow commands, and ensure that your Redis configuration is optimized for performance. Use the Redis latency monitoring tools and commands to identify the root cause of the lag.\n",
            "\n",
            "## Step-by-Step Instructions\n",
            "\n",
            "### Step 1: Enable Latency Monitoring\n",
            "1. Connect to your Redis instance using `redis-cli`.\n",
            "2. Set the latency monitor threshold to 100 milliseconds:\n",
            "   ```bash\n",
            "   CONFIG SET latency-monitor-threshold 100\n",
            "   ```\n",
            "\n",
            "### Step 2: Use LATENCY DOCTOR\n",
            "1. Run the LATENCY DOCTOR command to analyze latency spikes:\n",
            "   ```bash\n",
            "   LATENCY DOCTOR\n",
            "   ```\n",
            "2. Review the output for any latency spikes and follow the recommendations provided.\n",
            "\n",
            "### Step 3: Check Slow Log Configuration\n",
            "1. Ensure that your Slow Log is configured to log commands that take longer than 1000 milliseconds:\n",
            "   ```bash\n",
            "   CONFIG SET slowlog-log-slower-than 1000\n",
            "   ```\n",
            "2. Check the Slow Log for slow commands:\n",
            "   ```bash\n",
            "   SLOWLOG GET 10\n",
            "   ```\n",
            "\n",
            "### Step 4: Monitor Latency History\n",
            "1. Use the `--latency-history` command to monitor the responsiveness of your Redis server:\n",
            "   ```bash\n",
            "   /opt/gitlab/embedded/bin/redis-cli --latency-history -h <your_redis_host>\n",
            "   ```\n",
            "2. Analyze the average latency over time to identify any trends.\n",
            "\n",
            "### Step 5: Check for Blocking Operations\n",
            "1. Investigate if there are any large objects being deleted, expired, or evicted, as these can cause blocking operations.\n",
            "2. If necessary, consider fragmenting large objects into smaller ones.\n",
            "\n",
            "### Step 6: Monitor Client Connections\n",
            "1. Check the number of active TCP connections to your Redis instance:\n",
            "   ```bash\n",
            "   sudo lsof -i tcp:6379 | grep ESTABLISHED | sed -E \"s/.*6379->(.*):.* \\(ESTABLISHED\\)/\\1/g\" | sort | uniq -c | sort -nr\n",
            "   ```\n",
            "\n",
            "### Step 7: Check Redis Configuration\n",
            "1. Ensure that the `client-output-buffer-limit` is set appropriately. If replication flapping is observed, temporarily disable it:\n",
            "   ```bash\n",
            "   REDISCLI_AUTH=$(sudo grep ^masterauth /var/opt/gitlab/redis/redis.conf | cut -d\\\" -f2)\n",
            "   /opt/gitlab/embedded/bin/redis-cli config set client-output-buffer-limit \"slave 0 0 0\"\n",
            "   ```\n",
            "2. After stabilizing the cluster, revert the change to the original configuration.\n",
            "\n",
            "### Step 8: Review Redis Logs\n",
            "1. Check the Redis Sentinel logs for any failover events or errors:\n",
            "   ```bash\n",
            "   cat /var/log/gitlab/sentinel/current\n",
            "   ```\n",
            "\n",
            "## Key Commands\n",
            "- Enable latency monitoring:\n",
            "  ```bash\n",
            "  CONFIG SET latency-monitor-threshold 100\n",
            "  ```\n",
            "- Run LATENCY DOCTOR:\n",
            "  ```bash\n",
            "  LATENCY DOCTOR\n",
            "  ```\n",
            "- Configure Slow Log:\n",
            "  ```bash\n",
            "  CONFIG SET slowlog-log-slower-than 1000\n",
            "  ```\n",
            "- Check Slow Log:\n",
            "  ```bash\n",
            "  SLOWLOG GET 10\n",
            "  ```\n",
            "- Monitor latency history:\n",
            "  ```bash\n",
            "  /opt/gitlab/embedded/bin/redis-cli --latency-history -h <your_redis_host>\n",
            "  ```\n",
            "- Check active TCP connections:\n",
            "  ```bash\n",
            "  sudo lsof -i tcp:6379 | grep ESTABLISHED | sed -E \"s/.*6379->(.*):.* \\(ESTABLISHED\\)/\\1/g\" | sort | uniq -c | sort -nr\n",
            "  ```\n",
            "- Temporarily disable client-output-buffer-limit:\n",
            "  ```bash\n",
            "  REDISCLI_AUTH=$(sudo grep ^masterauth /var/opt/gitlab/redis/redis.conf | cut -d\\\" -f2)\n",
            "  /opt/gitlab/embedded/bin/redis-cli config set client-output-buffer-limit \"slave 0 0 0\"\n",
            "  ```\n",
            "\n",
            "## Important Notes\n",
            "- **Avoid using the MONITOR command**: It can overload your Redis instance and cause performance issues.\n",
            "- **Avoid using the KEYS command**: This can also overload your Redis instance.\n",
            "- **Monitor your Redis instance regularly**: Use Grafana or Prometheus to keep track of performance metrics and identify issues early.\n",
            "- **Check for network connectivity issues**: Ensure that there are no network problems affecting the communication between master and replicas.\n",
            "\n",
            "**Sources Used:** 5 documents\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Redis Runbook Assistant - Question Answering\n",
        "def answer_runbook_question(question, vector_store, model_factory, custom_docs=None):\n",
        "    \"\"\"Answer how-to questions using Redis runbooks\"\"\"\n",
        "    \n",
        "    # Retrieve relevant documents (use custom docs if provided, otherwise use vector store)\n",
        "    if custom_docs is not None:\n",
        "        retrieved_docs = custom_docs\n",
        "    else:\n",
        "        retrieved_docs = naive_retrieval(question, vector_store, k=5)\n",
        "    \n",
        "    # Prepare context from retrieved documents\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "    \n",
        "    # Create prompt for LLM\n",
        "    prompt = f\"\"\"\n",
        "You are a Redis expert helping SREs find the right procedures. Based on the question and relevant runbook documentation, provide a clear, step-by-step answer.\n",
        "\n",
        "**Question:**\n",
        "{question}\n",
        "\n",
        "**Relevant Runbook Documentation:**\n",
        "{context}\n",
        "\n",
        "**Please provide:**\n",
        "1. **Direct Answer** - Clear response to the question\n",
        "2. **Step-by-Step Instructions** - Detailed procedure from the runbooks\n",
        "3. **Key Commands** - Specific commands or configurations needed\n",
        "4. **Important Notes** - Warnings, prerequisites, or additional context\n",
        "\n",
        "Format your response clearly with headers and numbered steps.\n",
        "\"\"\"\n",
        "\n",
        "    # Generate response using LLM\n",
        "    llm = model_factory.get_llm()\n",
        "    response = llm.invoke(prompt)\n",
        "    \n",
        "    return response.content, retrieved_docs\n",
        "\n",
        "# Test with runbook-style questions\n",
        "sample_questions = [\n",
        "    \"How to backup Redis data using RDB snapshots?\",\n",
        "    \"How to configure Redis connection pools for high traffic?\",\n",
        "    \"How to troubleshoot Redis replication lag issues?\"\n",
        "]\n",
        "\n",
        "print(\"Testing Redis Runbook Assistant\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, question in enumerate(sample_questions, 1):\n",
        "    print(f\"\\n**Question {i}: {question}**\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    response, docs = answer_runbook_question(question, vector_store, get_model_factory())\n",
        "    print(response)\n",
        "    print(f\"\\n**Sources Used:** {len(docs)} documents\")\n",
        "    print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAGAS Evaluation Framework\n",
        "\n",
        "This section implements comprehensive evaluation of our RAG pipeline using RAGAS metrics:\n",
        "\n",
        "1. **Evaluation Dataset Creation** - Run RAG pipeline on test questions\n",
        "2. **RAGAS Metrics** - Measure faithfulness, relevancy, precision, recall\n",
        "3. **Performance Analysis** - Compare naive vs advanced retrieval\n",
        "4. **Results Summary** - Certification-ready metrics table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cached test questions...\n",
            "Loaded 9 cached test questions from ../data/sdg/redis_test_questions_ragas.json\n",
            "Found 9 test questions\n",
            "\n",
            "Sample test questions:\n",
            "1. What is WAL-G in the context of PostgreSQL backups?\n",
            "2. What is the significance of RAILS_INSTANCE_NAME in the context of configuring GitLab Rails?\n",
            "3. How chef help in setting up instances for Redis?\n"
          ]
        }
      ],
      "source": [
        "# RAGAS Evaluation Dataset Creation\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_cached_test_questions(filename=\"redis_test_questions_ragas.json\"):\n",
        "    \"\"\"Load cached test questions for evaluation\"\"\"\n",
        "    filepath = Path(\"../data/sdg\") / filename\n",
        "    if not filepath.exists():\n",
        "        print(f\"Cache file not found: {filepath}\")\n",
        "        return None\n",
        "    \n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        test_data = json.load(f)\n",
        "    \n",
        "    # Convert old format to RAGAS format if needed\n",
        "    if test_data and 'context' in test_data[0]:  # Old format\n",
        "        print(\"Converting old format to RAGAS format...\")\n",
        "        for item in test_data:\n",
        "            if 'context' in item:\n",
        "                item['contexts'] = [item.pop('context')]\n",
        "            if 'ground_truth' in item:\n",
        "                item['ground_truths'] = [item.pop('ground_truth')]\n",
        "    \n",
        "    print(f\"Loaded {len(test_data)} cached test questions from {filepath}\")\n",
        "    return test_data\n",
        "\n",
        "def create_evaluation_dataset(test_questions, vector_store, model_factory):\n",
        "    \"\"\"Create evaluation dataset by running RAG pipeline on test questions\"\"\"\n",
        "    evaluation_data = []\n",
        "    \n",
        "    print(f\"Creating evaluation dataset from {len(test_questions)} test questions...\")\n",
        "    \n",
        "    for i, test_item in enumerate(test_questions, 1):\n",
        "        print(f\"Processing question {i}/{len(test_questions)}: {test_item['question'][:50]}...\")\n",
        "        \n",
        "        # Run RAG pipeline\n",
        "        question = test_item['question']\n",
        "        ground_truths = test_item['ground_truths']\n",
        "        \n",
        "        # Get answer and contexts from our RAG pipeline\n",
        "        answer, retrieved_docs = answer_runbook_question(question, vector_store, model_factory)\n",
        "        contexts = [doc.page_content for doc in retrieved_docs]\n",
        "        \n",
        "        evaluation_data.append({\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'contexts': contexts,\n",
        "            'ground_truths': ground_truths,\n",
        "            'reference': ground_truths[0] if ground_truths else ''  # RAGAS expects 'reference' column\n",
        "        })\n",
        "    \n",
        "    return evaluation_data\n",
        "\n",
        "# Load cached test questions\n",
        "print(\"Loading cached test questions...\")\n",
        "cached_questions = load_cached_test_questions()\n",
        "\n",
        "if cached_questions:\n",
        "    print(f\"Found {len(cached_questions)} test questions\")\n",
        "    # Display sample questions\n",
        "    print(\"\\nSample test questions:\")\n",
        "    for i, q in enumerate(cached_questions[:3], 1):\n",
        "        print(f\"{i}. {q['question']}\")\n",
        "else:\n",
        "    print(\"No cached questions found. Please run SDG first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating evaluation dataset...\n",
            "Creating evaluation dataset from 9 test questions...\n",
            "Processing question 1/9: What is WAL-G in the context of PostgreSQL backups...\n",
            "Processing question 2/9: What is the significance of RAILS_INSTANCE_NAME in...\n",
            "Processing question 3/9: How chef help in setting up instances for Redis?...\n",
            "Processing question 4/9: What is the significance of database connection in...\n",
            "Processing question 5/9: What are the key aspects of Patroni cluster manage...\n",
            "Processing question 6/9: What are the key aspects of Patroni Cluster Manage...\n",
            "Processing question 7/9: How do you delete cache keys on GitLab.com using R...\n",
            "Processing question 8/9: How does the RDB format affect data storage in Red...\n",
            "Processing question 9/9: How does the use of Sidekiq in GitLab's architectu...\n",
            "Converting to RAGAS evaluation format...\n",
            "Evaluation dataset created with 9 samples\n",
            "Sample structure: dict_keys(['question', 'answer', 'contexts', 'ground_truths', 'reference'])\n",
            "\n",
            "Running RAGAS evaluation metrics...\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:  97%|█████████▋| 35/36 [01:37<00:10, 10.66s/it]ERROR:ragas.executor:Exception raised in Job[8]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
            "Evaluating: 100%|██████████| 36/36 [02:10<00:00,  3.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ RAGAS evaluation completed successfully!\n",
            "\n",
            "================================================================================\n",
            "RAGAS EVALUATION RESULTS - REDIS RUNBOOK ASSISTANT\n",
            "================================================================================\n",
            "\n",
            "📊 SUMMARY METRICS TABLE:\n",
            "           Metric  Mean Score  Std Dev  Min Score  Max Score\n",
            "     Faithfulness       0.700    0.360      0.019      1.000\n",
            " Answer Relevancy       0.912    0.080      0.769      0.993\n",
            "Context Precision       0.590    0.461      0.000      1.000\n",
            "   Context Recall       0.841    0.248      0.400      1.000\n",
            "\n",
            "📋 DETAILED RESULTS (9 samples):\n",
            "                                                                                                                                                user_input                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          retrieved_contexts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 response                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reference  faithfulness  answer_relevancy  context_precision  context_recall\n",
            "                                                                                                       What is WAL-G in the context of PostgreSQL backups?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/)\\n- [PostgreSQL Backups: WAL-G](/patroni/postgresql-backups-wale-walg/)\\n- [postgresql-buffermapping-lwlock-contention](/patroni/postgresql-buffermapping-lwlock-contention/)\\n- [PostgreSQL](/patroni/postgresql-disk-space/)\\n- [postgresql-locking](/patroni/postgresql-locking/)\\n- [How to evaluate load from queries](/patroni/postgresql-query-load-evaluation/)\\n- [PostgreSQL Trigram Indexes](/patroni/postgresql-rebuild-trigram-indexes/)\\n- [Adding a PostgreSQL replica](/patroni/postgresql-replica/)\\n- [Credential rotation](/patroni/postgresql-role-credential-rotation/)\\n- [PostgreSQL subtransactions](/patroni/postgresql-subtransactions/)\\n- [PostgreSQL VACUUM](/patroni/postgresql-vacuum/)\\n- [Primary Database Node CPU Saturation Analysis](/patroni/primary_db_node_cpu_saturation/)\\n- [Primary Database Node WAL Generation Saturation Analysis](/patroni/primary_db_node_wal_generation_saturation/)\\n- [How to provision the benchmark environment](/patroni/provisioning_bench_env/)\\n- [SQL query analysis and optimization for Postgres](/patroni/query-analysis/)\\n- [Rails SQL Apdex alerts](/patroni/rails-sql-apdex-slow/)\\n- [Rotating Rails' PostgreSQL password](/patroni/rotating-rails-postgresql-password/)\\n- [Scale Down Patroni](/patroni/scale-down-patroni/)\\n- [Scale Up Patroni](/patroni/scale-up-patroni/)\\n- [High-level performance analysis and troubleshooting of a Postgres node](/patroni/single-node-high-level/)\\n- [Handling Unhealthy Patroni Replica](/patroni/unhealthy_patroni_node_handling/)\\n- [Roles/Users grants and permission Runbook](/patroni/user_grants_permission/)\\n- [using-wale-gpg](/patroni/using-wale-gpg/)\\n- [Postgres wait events analysis (a.k.a. Active Session History; ASH dashboard)](/patroni/wait-events-analisys/)\\n- [WAL logs analysis](/patroni/wal_log_analysis/)\\n- [Zero Downtime Postgres Database Decomposition](/patroni/zero-downtime-database-decomposition/)\\n- patroni-ci\\n- [CI Postgres (Patroni) Service](/patroni-ci/)\\n- [Recovering from CI Patroni cluster lagging too much or becoming completely broken](/patroni-ci/recovering_patroni_ci_intense_lagging_or_replication_stopped/)\\n- patroni-registry\\n- [Registry Postgres (Patroni) Service](/patroni-registry/)\\n- patroni-sec\\n- [Sec Postgres (Patroni) Service](/patroni-sec/)\\n- pd-event-logger-7760xa\\n- [events](/pd-event-logger-7760xa/events/)\\n- [Logs PagerDuty incident events to ElasticSearch Service](/pd-event-logger-7760xa/)\\n- pgbouncer\\n- alerts\\n- [component\\_saturation\\_slo\\_out\\_of\\_bounds:pgbouncer\\_single\\_core](/pgbouncer/alerts/pgbouncer_single_core/)\\n- [PGBouncer Primary Database Pool Service](/pgbouncer/), - [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/)\\n- [PostgreSQL Backups: WAL-G](/patroni/postgresql-backups-wale-walg/)\\n- [postgresql-buffermapping-lwlock-contention](/patroni/postgresql-buffermapping-lwlock-contention/)\\n- [PostgreSQL](/patroni/postgresql-disk-space/)\\n- [postgresql-locking](/patroni/postgresql-locking/)\\n- [How to evaluate load from queries](/patroni/postgresql-query-load-evaluation/)\\n- [PostgreSQL Trigram Indexes](/patroni/postgresql-rebuild-trigram-indexes/)\\n- [Adding a PostgreSQL replica](/patroni/postgresql-replica/)\\n- [Credential rotation](/patroni/postgresql-role-credential-rotation/)\\n- [PostgreSQL subtransactions](/patroni/postgresql-subtransactions/)\\n- [PostgreSQL VACUUM](/patroni/postgresql-vacuum/)\\n- [Primary Database Node CPU Saturation Analysis](/patroni/primary_db_node_cpu_saturation/)\\n- [Primary Database Node WAL Generation Saturation Analysis](/patroni/primary_db_node_wal_generation_saturation/)\\n- [How to provision the benchmark environment](/patroni/provisioning_bench_env/)\\n- [SQL query analysis and optimization for Postgres](/patroni/query-analysis/)\\n- [Rails SQL Apdex alerts](/patroni/rails-sql-apdex-slow/)\\n- [Rotating Rails' PostgreSQL password](/patroni/rotating-rails-postgresql-password/)\\n- [Scale Down Patroni](/patroni/scale-down-patroni/)\\n- [Scale Up Patroni](/patroni/scale-up-patroni/)\\n- [High-level performance analysis and troubleshooting of a Postgres node](/patroni/single-node-high-level/)\\n- [Handling Unhealthy Patroni Replica](/patroni/unhealthy_patroni_node_handling/)\\n- [Roles/Users grants and permission Runbook](/patroni/user_grants_permission/)\\n- [using-wale-gpg](/patroni/using-wale-gpg/)\\n- [Postgres wait events analysis (a.k.a. Active Session History; ASH dashboard)](/patroni/wait-events-analisys/)\\n- [WAL logs analysis](/patroni/wal_log_analysis/)\\n- [Zero Downtime Postgres Database Decomposition](/patroni/zero-downtime-database-decomposition/)\\n- patroni-ci\\n- [CI Postgres (Patroni) Service](/patroni-ci/)\\n- [Recovering from CI Patroni cluster lagging too much or becoming completely broken](/patroni-ci/recovering_patroni_ci_intense_lagging_or_replication_stopped/)\\n- patroni-registry\\n- [Registry Postgres (Patroni) Service](/patroni-registry/)\\n- patroni-sec\\n- [Sec Postgres (Patroni) Service](/patroni-sec/)\\n- pd-event-logger-7760xa\\n- [events](/pd-event-logger-7760xa/events/)\\n- [Logs PagerDuty incident events to ElasticSearch Service](/pd-event-logger-7760xa/)\\n- pgbouncer\\n- alerts\\n- [component\\_saturation\\_slo\\_out\\_of\\_bounds:pgbouncer\\_single\\_core](/pgbouncer/alerts/pgbouncer_single_core/)\\n- [PGBouncer Primary Database Pool Service](/pgbouncer/), - [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/)\\n- [PostgreSQL Backups: WAL-G](/patroni/postgresql-backups-wale-walg/)\\n- [postgresql-buffermapping-lwlock-contention](/patroni/postgresql-buffermapping-lwlock-contention/)\\n- [PostgreSQL](/patroni/postgresql-disk-space/)\\n- [postgresql-locking](/patroni/postgresql-locking/)\\n- [How to evaluate load from queries](/patroni/postgresql-query-load-evaluation/)\\n- [PostgreSQL Trigram Indexes](/patroni/postgresql-rebuild-trigram-indexes/)\\n- [Adding a PostgreSQL replica](/patroni/postgresql-replica/)\\n- [Credential rotation](/patroni/postgresql-role-credential-rotation/)\\n- [PostgreSQL subtransactions](/patroni/postgresql-subtransactions/)\\n- [PostgreSQL VACUUM](/patroni/postgresql-vacuum/)\\n- [Primary Database Node CPU Saturation Analysis](/patroni/primary_db_node_cpu_saturation/)\\n- [Primary Database Node WAL Generation Saturation Analysis](/patroni/primary_db_node_wal_generation_saturation/)\\n- [How to provision the benchmark environment](/patroni/provisioning_bench_env/)\\n- [SQL query analysis and optimization for Postgres](/patroni/query-analysis/)\\n- [Rails SQL Apdex alerts](/patroni/rails-sql-apdex-slow/)\\n- [Rotating Rails' PostgreSQL password](/patroni/rotating-rails-postgresql-password/)\\n- [Scale Down Patroni](/patroni/scale-down-patroni/)\\n- [Scale Up Patroni](/patroni/scale-up-patroni/)\\n- [High-level performance analysis and troubleshooting of a Postgres node](/patroni/single-node-high-level/)\\n- [Handling Unhealthy Patroni Replica](/patroni/unhealthy_patroni_node_handling/)\\n- [Roles/Users grants and permission Runbook](/patroni/user_grants_permission/)\\n- [using-wale-gpg](/patroni/using-wale-gpg/)\\n- [Postgres wait events analysis (a.k.a. Active Session History; ASH dashboard)](/patroni/wait-events-analisys/)\\n- [WAL logs analysis](/patroni/wal_log_analysis/)\\n- [Zero Downtime Postgres Database Decomposition](/patroni/zero-downtime-database-decomposition/)\\n- patroni-ci\\n- [CI Postgres (Patroni) Service](/patroni-ci/)\\n- [Recovering from CI Patroni cluster lagging too much or becoming completely broken](/patroni-ci/recovering_patroni_ci_intense_lagging_or_replication_stopped/)\\n- patroni-registry\\n- [Registry Postgres (Patroni) Service](/patroni-registry/)\\n- patroni-sec\\n- [Sec Postgres (Patroni) Service](/patroni-sec/)\\n- pd-event-logger-7760xa\\n- [events](/pd-event-logger-7760xa/events/)\\n- [Logs PagerDuty incident events to ElasticSearch Service](/pd-event-logger-7760xa/)\\n- pgbouncer\\n- alerts\\n- [component\\_saturation\\_slo\\_out\\_of\\_bounds:pgbouncer\\_single\\_core](/pgbouncer/alerts/pgbouncer_single_core/)\\n- [PGBouncer Primary Database Pool Service](/pgbouncer/), - [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/)\\n- [PostgreSQL Backups: WAL-G](/patroni/postgresql-backups-wale-walg/)\\n- [postgresql-buffermapping-lwlock-contention](/patroni/postgresql-buffermapping-lwlock-contention/)\\n- [PostgreSQL](/patroni/postgresql-disk-space/)\\n- [postgresql-locking](/patroni/postgresql-locking/)\\n- [How to evaluate load from queries](/patroni/postgresql-query-load-evaluation/)\\n- [PostgreSQL Trigram Indexes](/patroni/postgresql-rebuild-trigram-indexes/)\\n- [Adding a PostgreSQL replica](/patroni/postgresql-replica/)\\n- [Credential rotation](/patroni/postgresql-role-credential-rotation/)\\n- [PostgreSQL subtransactions](/patroni/postgresql-subtransactions/)\\n- [PostgreSQL VACUUM](/patroni/postgresql-vacuum/)\\n- [Primary Database Node CPU Saturation Analysis](/patroni/primary_db_node_cpu_saturation/)\\n- [Primary Database Node WAL Generation Saturation Analysis](/patroni/primary_db_node_wal_generation_saturation/)\\n- [How to provision the benchmark environment](/patroni/provisioning_bench_env/)\\n- [SQL query analysis and optimization for Postgres](/patroni/query-analysis/)\\n- [Rails SQL Apdex alerts](/patroni/rails-sql-apdex-slow/)\\n- [Rotating Rails' PostgreSQL password](/patroni/rotating-rails-postgresql-password/)\\n- [Scale Down Patroni](/patroni/scale-down-patroni/)\\n- [Scale Up Patroni](/patroni/scale-up-patroni/)\\n- [High-level performance analysis and troubleshooting of a Postgres node](/patroni/single-node-high-level/)\\n- [Handling Unhealthy Patroni Replica](/patroni/unhealthy_patroni_node_handling/)\\n- [Roles/Users grants and permission Runbook](/patroni/user_grants_permission/)\\n- [using-wale-gpg](/patroni/using-wale-gpg/)\\n- [Postgres wait events analysis (a.k.a. Active Session History; ASH dashboard)](/patroni/wait-events-analisys/)\\n- [WAL logs analysis](/patroni/wal_log_analysis/)\\n- [Zero Downtime Postgres Database Decomposition](/patroni/zero-downtime-database-decomposition/)\\n- patroni-ci\\n- [CI Postgres (Patroni) Service](/patroni-ci/)\\n- [Recovering from CI Patroni cluster lagging too much or becoming completely broken](/patroni-ci/recovering_patroni_ci_intense_lagging_or_replication_stopped/)\\n- patroni-registry\\n- [Registry Postgres (Patroni) Service](/patroni-registry/)\\n- patroni-sec\\n- [Sec Postgres (Patroni) Service](/patroni-sec/)\\n- pd-event-logger-7760xa\\n- [events](/pd-event-logger-7760xa/events/)\\n- [Logs PagerDuty incident events to ElasticSearch Service](/pd-event-logger-7760xa/)\\n- pgbouncer\\n- alerts\\n- [component\\_saturation\\_slo\\_out\\_of\\_bounds:pgbouncer\\_single\\_core](/pgbouncer/alerts/pgbouncer_single_core/)\\n- [PGBouncer Primary Database Pool Service](/pgbouncer/), - [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/)\\n- [PostgreSQL Backups: WAL-G](/patroni/postgresql-backups-wale-walg/)\\n- [postgresql-buffermapping-lwlock-contention](/patroni/postgresql-buffermapping-lwlock-contention/)\\n- [PostgreSQL](/patroni/postgresql-disk-space/)\\n- [postgresql-locking](/patroni/postgresql-locking/)\\n- [How to evaluate load from queries](/patroni/postgresql-query-load-evaluation/)\\n- [PostgreSQL Trigram Indexes](/patroni/postgresql-rebuild-trigram-indexes/)\\n- [Adding a PostgreSQL replica](/patroni/postgresql-replica/)\\n- [Credential rotation](/patroni/postgresql-role-credential-rotation/)\\n- [PostgreSQL subtransactions](/patroni/postgresql-subtransactions/)\\n- [PostgreSQL VACUUM](/patroni/postgresql-vacuum/)\\n- [Primary Database Node CPU Saturation Analysis](/patroni/primary_db_node_cpu_saturation/)\\n- [Primary Database Node WAL Generation Saturation Analysis](/patroni/primary_db_node_wal_generation_saturation/)\\n- [How to provision the benchmark environment](/patroni/provisioning_bench_env/)\\n- [SQL query analysis and optimization for Postgres](/patroni/query-analysis/)\\n- [Rails SQL Apdex alerts](/patroni/rails-sql-apdex-slow/)\\n- [Rotating Rails' PostgreSQL password](/patroni/rotating-rails-postgresql-password/)\\n- [Scale Down Patroni](/patroni/scale-down-patroni/)\\n- [Scale Up Patroni](/patroni/scale-up-patroni/)\\n- [High-level performance analysis and troubleshooting of a Postgres node](/patroni/single-node-high-level/)\\n- [Handling Unhealthy Patroni Replica](/patroni/unhealthy_patroni_node_handling/)\\n- [Roles/Users grants and permission Runbook](/patroni/user_grants_permission/)\\n- [using-wale-gpg](/patroni/using-wale-gpg/)\\n- [Postgres wait events analysis (a.k.a. Active Session History; ASH dashboard)](/patroni/wait-events-analisys/)\\n- [WAL logs analysis](/patroni/wal_log_analysis/)\\n- [Zero Downtime Postgres Database Decomposition](/patroni/zero-downtime-database-decomposition/)\\n- patroni-ci\\n- [CI Postgres (Patroni) Service](/patroni-ci/)\\n- [Recovering from CI Patroni cluster lagging too much or becoming completely broken](/patroni-ci/recovering_patroni_ci_intense_lagging_or_replication_stopped/)\\n- patroni-registry\\n- [Registry Postgres (Patroni) Service](/patroni-registry/)\\n- patroni-sec\\n- [Sec Postgres (Patroni) Service](/patroni-sec/)\\n- pd-event-logger-7760xa\\n- [events](/pd-event-logger-7760xa/events/)\\n- [Logs PagerDuty incident events to ElasticSearch Service](/pd-event-logger-7760xa/)\\n- pgbouncer\\n- alerts\\n- [component\\_saturation\\_slo\\_out\\_of\\_bounds:pgbouncer\\_single\\_core](/pgbouncer/alerts/pgbouncer_single_core/)\\n- [PGBouncer Primary Database Pool Service](/pgbouncer/)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ### 1. Direct Answer\\nWAL-G (Write-Ahead Logging - Go) is a backup and restore tool for PostgreSQL that utilizes the Write-Ahead Logging (WAL) mechanism to create efficient and reliable backups. It supports full and incremental backups, allowing for point-in-time recovery and is designed to work with cloud storage solutions.\\n\\n### 2. Step-by-Step Instructions\\nTo set up and use WAL-G for PostgreSQL backups, follow these steps:\\n\\n#### Step 1: Install WAL-G\\n1. **Download WAL-G**: Obtain the latest release from the [WAL-G GitHub repository](https://github.com/wal-g/wal-g/releases).\\n2. **Install WAL-G**: Follow the installation instructions specific to your operating system.\\n\\n#### Step 2: Configure WAL-G\\n1. **Set Environment Variables**: Configure the necessary environment variables for WAL-G. This typically includes:\\n   - `WALG_S3_PREFIX`: The S3 bucket URL where backups will be stored.\\n   - `AWS_ACCESS_KEY_ID`: Your AWS access key.\\n   - `AWS_SECRET_ACCESS_KEY`: Your AWS secret key.\\n   - `PGHOST`, `PGUSER`, `PGPASSWORD`, `PGDATABASE`: PostgreSQL connection parameters.\\n\\n   Example:\\n   ```bash\\n   export WALG_S3_PREFIX=s3://your-bucket-name/wal-g\\n   export AWS_ACCESS_KEY_ID=your_access_key\\n   export AWS_SECRET_ACCESS_KEY=your_secret_key\\n   export PGHOST=localhost\\n   export PGUSER=your_user\\n   export PGPASSWORD=your_password\\n   export PGDATABASE=your_database\\n   ```\\n\\n#### Step 3: Perform Backups\\n1. **Full Backup**: To create a full backup, run:\\n   ```bash\\n   wal-g backup-push /path/to/your/postgresql/data\\n   ```\\n2. **Incremental Backup**: To create an incremental backup, ensure you have a previous full backup and run:\\n   ```bash\\n   wal-g backup-push /path/to/your/postgresql/data\\n   ```\\n\\n#### Step 4: Restore Backups\\n1. **Restore from Backup**: To restore a backup, first stop the PostgreSQL server, then run:\\n   ```bash\\n   wal-g backup-fetch /path/to/your/postgresql/data LATEST\\n   ```\\n2. **Recover WAL Files**: If you need to recover to a specific point in time, ensure you have the necessary WAL files and use:\\n   ```bash\\n   wal-g wal-fetch <WAL_FILE_NAME> /path/to/your/postgresql/data/pg_wal/\\n   ```\\n\\n### 3. Key Commands\\n- **Backup Push**: `wal-g backup-push /path/to/data`\\n- **Backup Fetch**: `wal-g backup-fetch /path/to/data LATEST`\\n- **WAL Fetch**: `wal-g wal-fetch <WAL_FILE_NAME> /path/to/pg_wal/`\\n\\n### 4. Important Notes\\n- **Prerequisites**: Ensure that PostgreSQL is configured to use WAL archiving. This typically involves setting `archive_mode` to `on` and specifying an `archive_command`.\\n- **Permissions**: Ensure that the user running WAL-G has the necessary permissions to access the PostgreSQL data directory and the S3 bucket.\\n- **Testing**: Always test your backup and restore process in a staging environment before relying on it in production.\\n- **Monitoring**: Regularly monitor your backup processes and logs to ensure that backups are being created successfully and without errors.\\n\\nFor more detailed information, refer to the [PostgreSQL Backups: WAL-G](https://patroni/postgresql-backups-wale-walg/) runbook.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   WAL-G is associated with PostgreSQL backups, specifically mentioned in the context of alerts related to backup failures such as 'walgBaseBackupDelayed' and 'WALGBaseBackupFailed'.         0.019             0.920              0.000           1.000\n",
            "                                                               What is the significance of RAILS_INSTANCE_NAME in the context of configuring GitLab Rails?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [mv data.json.tmp data.json\\nvault kv patch chef/env/<ENV>/shared/gitlab-omnibus-secrets @data.json\\nrm data.json\\nOR\\nglsh vault edit-secret chef env/<ENV>/shared/gitlab-omnibus-secrets\\n#  Add the following object in .\"omnibus-gitlab\".gitlab_rb.\"gitlab-rails\".redis_yml_override :\\n#  \"<RAILS_INSTANCE_NAME_OMNIBUS>\": {\\n#    \"password\": <RAILS_REDACTED>\\n#  }\\n```\\nUpdate roles/-base.json with the relevant connection details. An example MR can be found [here](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/merge_requests/3546).\\n### 2. Verification of config in a VM\\n[Section titled “2. Verification of config in a VM”](#2-verification-of-config-in-a-vm)\\nCheck the confirmation detail by using `gitlab-rails console` inside a console instance. You may need to run `chef-client` to update the node and render the updated configuration files. This is important as the pipeline does not check the correctness of the config files. This may impact the deploy-node as GitLab Rails connects to Redis instances on start-up. There was a past incident of such an [issue](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/16322) for reference.\\n```\\n[ gstg ] production> Gitlab::Redis::FeatureFlag.with{|c| c.ping} # replace Gitlab::Redis::FeatureFlag with <RAILS_CLASS_NAME>\\n=> \"PONG\"\\n[ gstg ] production>\\n```\\n### 3. Configure Gitlab Rails\\n[Section titled “3. Configure Gitlab Rails”](#3-configure-gitlab-rails)\\na. Update secret\\n```\\nvault kv put k8s/env/<ENV>/ns/gitlab/redis-cluster-<INSTANCE_TYPE>-rails password=<RAILS_REDACTED>\\n```\\nFor example,\\n```\\nvault kv get k8s/env/<ENV>/ns/gitlab/redis-cluster-<INSTANCE_TYPE>-rails\\n======================== Secret Path ========================\\nk8s/data/env/gprd/ns/gitlab/redis-cluster-ratelimiting-rails\\n======= Metadata =======\\nKey                Value\\n---                -----\\ncreated_time       2023-03-18T00:33:29.790293426Z\\ncustom_metadata    <nil>\\ndeletion_time      n/a\\ndestroyed          false\\nversion            1\\n====== Data ======\\nKey         Value\\n---         -----\\npassword    <RAILS_REDACTED>\\n```\\nNote the version of the password in `vault kv get` and make sure it tallies with the external secret definition in [k8s-workload](https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/blob/master/releases/gitlab-external-secrets/values/values.yaml.gotmpl):\\n```\\ngitlab-redis-cluster-<INSTANCE_TYPE>-rails-credential-v1:\\nrefreshInterval: 0\\nsecretStoreName: gitlab-secrets\\ntarget:\\ncreationPolicy: Owner\\ndeletionPolicy: Delete\\ndata:\\n- remoteRef:\\nkey: env/{{ $env }}/ns/gitlab/redis-cluster-<INSTANCE_TYPE>-rails\\nproperty: password\\nversion: \"1\"\\nsecretKey: password\\n```\\nNote that when rotating secrets (eg having v1 in `gstg` and v2 in `gprd`), follow a safe and controlled rollout as described [here](https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/vault/usage.md?ref_type=heads#rotating-kubernetes-secrets).\\nb. Update Gitlab Rails `.Values.global.redis` accordingly.\\nEither add a new key to `.Values.global.redis.<RAILS_INSTANCE_NAME>` or `.Values.global.redis.redisYmlOverride.<RAILS_INSTANCE_NAME>`. An example MR can be found [here](https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/2753).\\n### 4. Troubleshooting\\n[Section titled “4. Troubleshooting”](#4-troubleshooting)\\n#### No metrics on dashboard\\n[Section titled “No metrics on dashboard”](#no-metrics-on-dashboard), - workhorse\\n- [Workhorse Service](/workhorse/)\\n- [Workhorse Apdex Degradation](/workhorse/workhorse-apdex-degradation/)\\n- workspaces\\n- [Remote Development Workspaces Service](/workspaces/)\\n- zoekt\\n- [Exact code search Service](/zoekt/)\\n(() => {\\nconst scroller = document.getElementById('starlight\\_\\_sidebar');\\nif (!window.\\_starlightScrollRestore || !scroller) return;\\nscroller.scrollTop = window.\\_starlightScrollRestore;\\ndelete window.\\_starlightScrollRestore;\\n})();\\n[GitLab](https://gitlab.com/gitlab-com/runbooks)[Code Context](https://code-context.runway.gitlab.net/runbooks-docs)\\nSelect theme   DarkLightAuto\\nStarlightThemeProvider.updatePickers();\\nconst a=document.getElementById(\"starlight\\_\\_sidebar\"),n=a?.querySelector(\"sl-sidebar-state-persist\"),o=\"sl-sidebar-state\",i=()=>{let t=[];const e=n?.dataset.hash||\"\";try{const s=sessionStorage.getItem(o),r=JSON.parse(s||\"{}\");Array.isArray(r.open)&&r.hash===e&&(t=r.open)}catch{}return{hash:e,open:t,scroll:a?.scrollTop||0}},c=t=>{try{sessionStorage.setItem(o,JSON.stringify(t))}catch{}},d=()=>c(i()),l=(t,e)=>{const s=i();s.open[e]=t,c(s)};n?.addEventListener(\"click\",t=>{if(!(t.target instanceof Element))return;const e=t.target.closest(\"summary\")?.closest(\"details\");if(!e)return;const s=e.querySelector(\"sl-sidebar-restore\"),r=parseInt(s?.dataset.index||\"\");isNaN(r)||l(!e.open,r)});addEventListener(\"visibilitychange\",()=>{document.visibilityState===\"hidden\"&&d()});addEventListener(\"pageHide\",d);\\nOn this page\\n- [Overview](#_top)\\n- [Setting up instances](#setting-up-instances)\\n- [1. Generate Redis passwords](#1-generate-redis-passwords)\\n- [2. Create Chef roles](#2-create-chef-roles)\\n- [3. Provision VMs](#3-provision-vms)\\n- [4. Initialising the cluster](#4-initialising-the-cluster)\\n- [5. Validation](#5-validation)\\n- [Configuring the applications](#configuring-the-applications)\\n- [1. Configure console instances](#1-configure-console-instances)\\n- [2. Verification of config in a VM](#2-verification-of-config-in-a-vm)\\n- [3. Configure Gitlab Rails](#3-configure-gitlab-rails)\\n- [4. Troubleshooting](#4-troubleshooting)\\n## On this page\\n- [Overview](#_top)\\n- [Setting up instances](#setting-up-instances)\\n- [1. Generate Redis passwords](#1-generate-redis-passwords)\\n- [2. Create Chef roles](#2-create-chef-roles)\\n- [3. Provision VMs](#3-provision-vms)\\n- [4. Initialising the cluster](#4-initialising-the-cluster)\\n- [5. Validation](#5-validation)\\n- [Configuring the applications](#configuring-the-applications)\\n- [1. Configure console instances](#1-configure-console-instances)\\n- [2. Verification of config in a VM](#2-verification-of-config-in-a-vm)\\n- [3. Configure Gitlab Rails](#3-configure-gitlab-rails)\\n- [4. Troubleshooting](#4-troubleshooting)\\n# Provisioning Redis Cluster\\nThis document outlines the steps for provisioning a Redis Cluster. Former attempts are documented here:\\n- [`redis-cluster-ratelimiting`](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2256)\\n- [`redis-cluster-chat-cache`](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2358)\\n## Setting up instances\\n[Section titled “Setting up instances”](#setting-up-instances)\\nFirst, this guide defines a few variables in `<>`, namely:\\n- `ENV`: `pre`, `gstg`, `gprd`\\n- `GCP_PROJECT`: `gitlab-production` or `gitlab-staging-1`\\n- `INSTANCE_TYPE`: `feature-flag`, - [4. Troubleshooting](#4-troubleshooting)\\n# Provisioning Redis Cluster\\nThis document outlines the steps for provisioning a Redis Cluster. Former attempts are documented here:\\n- [`redis-cluster-ratelimiting`](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2256)\\n- [`redis-cluster-chat-cache`](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2358)\\n## Setting up instances\\n[Section titled “Setting up instances”](#setting-up-instances)\\nFirst, this guide defines a few variables in `<>`, namely:\\n- `ENV`: `pre`, `gstg`, `gprd`\\n- `GCP_PROJECT`: `gitlab-production` or `gitlab-staging-1`\\n- `INSTANCE_TYPE`: `feature-flag`\\n- `RAILS_INSTANCE_NAME`: The name that GitLab Rails would recognise. This matches `redis.xxx.yml` or the 2nd-top-level key in `redis.yml` (top-level key being `production`)\\n- `RAILS_INSTANCE_NAME_OMNIBUS`: `RAILS_INSTANCE_NAME` but using underscore, i.e. `feature_flag` instead of `feature-flag`\\n- `RAILS_CLASS_NAME`: The class which would connect to the new Redis Cluster. e.g. `Gitlab::Redis::FeatureFlag`.\\n- `REPLICA_REDACTED`: Generated in [Generate Redis passwords step](#1-generate-redis-passwords)\\n- `RAILS_REDACTED`: Generated in [Generate Redis passwords step](#1-generate-redis-passwords)\\n- `EXPORTER_REDACTED`: Generated in [Generate Redis passwords step](#1-generate-redis-passwords)\\n- `CONSOLE_REDACTED`: Generated in [Generate Redis passwords step](#1-generate-redis-passwords)\\nWhen configuring the application, note that the name of the instance must match the object name in lowercase and kebab-case/snake-case in the application.\\nE.g. We have `redis-cluster-chat-cache` service but in GitLab Rails, the object is `Gitlab::Redis::Chat`. Hence `chat` should be used when configuring the secret for the application in console and Kubernetes.\\n**Note:** To avoid mistakes in manually copy-pasting the variables in `<>` above during a provisioning session, it is recommended to prepare this doc with all the variables replaced beforehand.\\n### 1. Generate Redis passwords\\n[Section titled “1. Generate Redis passwords”](#1-generate-redis-passwords)\\nGenerate four passwords, `REPLICA_REDACTED`, `RAILS_REDACTED`, `EXPORTER_REDACTED`, and `CONSOLE_REDACTED` using:\\n```\\nfor I in REPLICA_REDACTED RAILS_REDACTED EXPORTER_REDACTED CONSOLE_REDACTED; do echo $I; openssl rand -hex 32; done\\n```\\nUpdate both `redis-cluster` and `redis-exporter` gkms vault secrets using these commands in the [`chef-repo`](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/tree/master):\\n```\\n./bin/gkms-vault-edit redis-cluster <ENV>\\n```\\nUpdate the JSON payload to include the new instance details:\\n```\\n{\\n...,\\n\"redis-cluster-<INSTANCE_TYPE>\": {\\n\"redis_conf\": {\\n\"masteruser\": \"replica\",\\n\"masterauth\": \"<REPLICA_REDACTED>\",\\n\"user\": [\\n\"default off\",\\n\"replica on ~* &* +@all ><REPLICA_REDACTED>\",\\n\"console on ~* &* +@all ><CONSOLE_REDACTED>\",\\n\"redis_exporter on +client +ping +info +config|get +cluster|info +slowlog +latency +memory +select +get +scan +xinfo +type +pfcount +strlen +llen +scard +zcard +hlen +xlen +eval allkeys ><EXPORTER_REDACTED>\",\\n\"rails on ~* &* +@all -debug ><RAILS_REDACTED>\"\\n]\\n}\\n}\\n}\\n```\\nDo the same for\\n```\\n./bin/gkms-vault-edit redis-exporter <ENV>\\n```\\nModify the existing JSON\\n```\\n{\\n\"redis_exporter\": {\\n\"redis-cluster-<INSTANCE_TYPE>\": {\\n\"env\": {\\n\"REDIS_PASSWORD\": \"<EXPORTER_REDACTED>\"\\n}\\n}\\n}\\n}\\n```, ```\\nUse the following command to connect the remaining nodes to the cluster. Update `{01, 02, 03, ... n}-{02,03,..m}` where `n` is the number of shards and `m` is the number of instances per shard.\\n```\\nfor i in {01,02,03}-{02,03}; do\\nsudo gitlab-redis-cli --cluster add-node \\\\n$DEPLOYMENT-shard-$i-db-$ENV.c.$PROJECT.internal:6379 \\\\n$DEPLOYMENT-shard-01-01-db-$ENV.c.$PROJECT.internal:6379\\nsleep 2\\ndone\\n```\\nUse the following command, to assign the replicas within each shard. Update `{01, 02, 03, ... n}-{02,03,..n}` where necessary depending on the cluster-size.\\n```\\nfor i in {01,02,03}; do\\nfor j in {02,03}; do\\nnode_id=\"$(sudo gitlab-redis-cli cluster nodes | grep $DEPLOYMENT-shard-$i-01-db-$ENV.c.$PROJECT.internal | awk '{ print $1 }')\";\\nsudo gitlab-redis-cli -h $DEPLOYMENT-shard-$i-$j-db-$ENV.c.$PROJECT.internal \\\\ncluster replicate $node_id\\ndone\\ndone\\n```\\n### 5. Validation\\n[Section titled “5. Validation”](#5-validation)\\nWait for a few seconds as the nodes need time to gossip. Check the status via:\\n```\\n$ sudo gitlab-redis-cli --cluster info $DEPLOYMENT-shard-01-01-db-$ENV.c.$PROJECT.internal:6379\\nredis-cluster-ratelimiting-shard-01-01-db-gprd.c.gitlab-production.internal:6379 (9b0828e3...) -> 0 keys | 5461 slots | 2 slaves.\\n10.217.21.3:6379 (ac03fcee...) -> 0 keys | 5461 slots | 2 slaves.\\n10.217.21.4:6379 (f8341afd...) -> 0 keys | 5462 slots | 2 slaves.\\n[OK] 0 keys in 3 masters.\\n0.00 keys per slot on average.\\n$ sudo gitlab-redis-cli cluster info | head -n7\\ncluster_state:ok\\ncluster_slots_assigned:16384\\ncluster_slots_ok:16384\\ncluster_slots_pfail:0\\ncluster_slots_fail:0\\ncluster_known_nodes:9\\ncluster_size:3\\n```\\n## Configuring the applications\\n[Section titled “Configuring the applications”](#configuring-the-applications)\\n### 1. Configure console instances\\n[Section titled “1. Configure console instances”](#1-configure-console-instances)\\na. Proxy and authenticate to Hashicorp Vault:\\n```\\nglsh vault proxy\\nexport VAULT_PROXY_ADDR=\"socks5://localhost:18200\"\\nglsh vault login\\n```\\n```\\nvault kv get -format=json chef/env/<ENV>/shared/gitlab-omnibus-secrets | jq '.data.data' > data.json\\ncat data.json | jq --arg PASSWORD <RAILS_REDACTED> '.\"omnibus-gitlab\".gitlab_rb.\"gitlab-rails\".redis_yml_override.<RAILS_INSTANCE_NAME_OMNIBUS>.password = $PASSWORD' > data.json.tmp\\ndiff -u data.json data.json.tmp\\nmv data.json.tmp data.json\\nvault kv patch chef/env/<ENV>/shared/gitlab-omnibus-secrets @data.json\\nrm data.json\\nOR\\nglsh vault edit-secret chef env/<ENV>/shared/gitlab-omnibus-secrets\\n#  Add the following object in .\"omnibus-gitlab\".gitlab_rb.\"gitlab-rails\".redis_yml_override :\\n#  \"<RAILS_INSTANCE_NAME_OMNIBUS>\": {\\n#    \"password\": <RAILS_REDACTED>\\n#  }\\n```\\nUpdate roles/-base.json with the relevant connection details. An example MR can be found [here](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/merge_requests/3546).\\n### 2. Verification of config in a VM\\n[Section titled “2. Verification of config in a VM”](#2-verification-of-config-in-a-vm), Note that when rotating secrets (eg having v1 in `gstg` and v2 in `gprd`), follow a safe and controlled rollout as described [here](https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/vault/usage.md?ref_type=heads#rotating-kubernetes-secrets).\\nb. Update Gitlab Rails `.Values.global.redis` accordingly.\\nEither add a new key to `.Values.global.redis.<RAILS_INSTANCE_NAME>` or `.Values.global.redis.redisYmlOverride.<RAILS_INSTANCE_NAME>`. An example MR can be found [here](https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/2753).\\n### 4. Troubleshooting\\n[Section titled “4. Troubleshooting”](#4-troubleshooting)\\n#### No metrics on dashboard\\n[Section titled “No metrics on dashboard”](#no-metrics-on-dashboard)\\nThis was encountered when provisioning the [production instance of redis-cluster-chat-cache](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2358#note_1406105979). To resolve this, run chef-repo on Prometheus with:\\n```\\nknife ssh roles:gprd-infra-prometheus-server \"sudo chef-client\"\\n```\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis/provisioning-redis-cluster.md)\\n[Previous\\nMemory space analysis with cupcake-rdb](/redis/memory-space-analysis-cupcake-rdb/)   [Next\\nTroubleshooting](/redis/redis/)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           # Direct Answer\\nThe `RAILS_INSTANCE_NAME` is significant in configuring GitLab Rails as it serves as the identifier for the Redis instance that GitLab Rails will connect to. This name must match the corresponding Redis configuration in the `redis.yml` file and is used to set up the connection details, including the password for the Redis instance.\\n\\n# Step-by-Step Instructions\\n\\n### 1. Define Variables\\nBefore proceeding, ensure you have the following variables defined:\\n- `ENV`: Environment (e.g., `pre`, `gstg`, `gprd`)\\n- `INSTANCE_TYPE`: Type of Redis instance (e.g., `feature-flag`)\\n- `RAILS_INSTANCE_NAME`: The name GitLab Rails recognizes (e.g., `feature-flag`).\\n- `RAILS_INSTANCE_NAME_OMNIBUS`: The underscore version of `RAILS_INSTANCE_NAME` (e.g., `feature_flag`).\\n\\n### 2. Update GitLab Rails Configuration\\n1. **Update Secrets in Vault:**\\n   - Use the following command to update the GitLab Rails secrets in Vault:\\n     ```bash\\n     glsh vault proxy\\n     export VAULT_PROXY_ADDR=\"socks5://localhost:18200\"\\n     glsh vault login\\n     vault kv get -format=json chef/env/<ENV>/shared/gitlab-omnibus-secrets | jq '.data.data' > data.json\\n     cat data.json | jq --arg PASSWORD <RAILS_REDACTED> '.\"omnibus-gitlab\".gitlab_rb.\"gitlab-rails\".redis_yml_override.<RAILS_INSTANCE_NAME_OMNIBUS>.password = $PASSWORD' > data.json.tmp\\n     diff -u data.json data.json.tmp\\n     mv data.json.tmp data.json\\n     vault kv patch chef/env/<ENV>/shared/gitlab-omnibus-secrets @data.json\\n     rm data.json\\n     ```\\n   - Alternatively, you can edit the secret directly:\\n     ```bash\\n     glsh vault edit-secret chef env/<ENV>/shared/gitlab-omnibus-secrets\\n     ```\\n     Add the following object in `.omnibus-gitlab.gitlab_rb.gitlab-rails.redis_yml_override`:\\n     ```json\\n     \"<RAILS_INSTANCE_NAME_OMNIBUS>\": {\\n       \"password\": <RAILS_REDACTED>\\n     }\\n     ```\\n\\n2. **Update Roles Configuration:**\\n   - Update `roles/-base.json` with the relevant connection details for the Redis instance. Refer to an example MR for guidance: [Example MR](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/merge_requests/3546).\\n\\n### 3. Verification of Configuration\\n1. **Verify in a VM:**\\n   - Access the GitLab Rails console:\\n     ```bash\\n     gitlab-rails console\\n     ```\\n   - Check the connection to Redis:\\n     ```ruby\\n     Gitlab::Redis::FeatureFlag.with{|c| c.ping} # Replace with the appropriate RAILS_CLASS_NAME\\n     ```\\n   - Ensure it returns `\"PONG\"` indicating a successful connection.\\n\\n### 4. Update GitLab Rails `.Values.global.redis`\\n1. **Modify the Configuration:**\\n   - Update the `.Values.global.redis` section in your configuration file:\\n     - Add a new key to `.Values.global.redis.<RAILS_INSTANCE_NAME>` or `.Values.global.redis.redisYmlOverride.<RAILS_INSTANCE_NAME>`.\\n   - Refer to an example MR for guidance: [Example MR](https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/2753).\\n\\n# Key Commands\\n- **Update Vault Secrets:**\\n  ```bash\\n  vault kv patch chef/env/<ENV>/shared/gitlab-omnibus-secrets @data.json\\n  ```\\n- **Check Redis Connection:**\\n  ```ruby\\n  Gitlab::Redis::FeatureFlag.with{|c| c.ping}\\n  ```\\n\\n# Important Notes\\n- Ensure that the `RAILS_INSTANCE_NAME` matches the Redis configuration in `redis.yml` to avoid connection issues.\\n- Always verify the configuration in a VM after making changes to ensure that GitLab Rails can connect to the Redis instance correctly.\\n- Be cautious when rotating secrets; follow the controlled rollout procedures outlined in the documentation to prevent service disruptions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         RAILS_INSTANCE_NAME is the name that GitLab Rails would recognize, matching the naming convention used in redis.xxx.yml or the second top-level key in redis.yml. It is crucial for ensuring that the instance name aligns with the application's configuration requirements.         1.000             0.993              0.804           1.000\n",
            "                                                                                                          How chef help in setting up instances for Redis?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [Update `redis-cluster` gkms vault secrets to disable `debug` command for `rails` user, using these commands in the [`chef-repo`](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/tree/master):\\n```\\n./bin/gkms-vault-edit redis-cluster $ENV\\n```\\nUpdate the JSON payload to include `-debug` for `rails` user for the target Redis Cluster (There is a space between `-debug` and `<`):\\n```\\n...\\n\"rails on ~* &* +@all -debug ><RAILS_REDACTED>\"\\n...\\n```\\nComplete json object, with modification:\\n```\\n{\\n...,\\n\"redis-cluster-<INSTANCE_TYPE>\": {\\n\"redis_conf\": {\\n\"masteruser\": \"replica\",\\n\"masterauth\": \"<REPLICA_REDACTED>\",\\n\"user\": [\\n\"default off\",\\n\"replica on ~* &* +@all ><REPLICA_REDACTED>\",\\n\"console on ~* &* +@all ><CONSOLE_REDACTED>\",\\n\"redis_exporter on +client +ping +info +config|get +cluster|info +slowlog +latency +memory +select +get +scan +xinfo +type +pfcount +strlen +llen +scard +zcard +hlen +xlen +eval allkeys ><EXPORTER_REDACTED>\",\\n\"rails on ~* &* +@all -debug ><RAILS_REDACTED>\"\\n]\\n}\\n}\\n}\\n```\\nThis won’t change anything yet, on the existing Redis servers, as we would reconfigure the Redis processes as mentioned later in this guide.\\n### 2. Create Chef role for new shard\\n[Section titled “2. Create Chef role for new shard”](#2-create-chef-role-for-new-shard)\\nCreate chef role for the new Redis shard, based on the chef roles of existing Redis shards of the cluster. Also add `enable-debug-command` option to Redis configuration in the base role, if not added already.\\nAn example MR can be found [here](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/merge_requests/4789).\\n### 3. Provision VMs\\n[Section titled “3. Provision VMs”](#3-provision-vms)\\nProvision the VMs for new Redis shard by incrementing `count` parameter to generic-stor/google terraform module of target Redis Cluster in the [config-mgmt project in the ops environment](https://ops.gitlab.net/gitlab-com/gl-infra/config-mgmt/). An example MR can be found [here](https://ops.gitlab.net/gitlab-com/gl-infra/config-mgmt/-/merge_requests/8474).\\nAfter the MR is merged and applied, check the VM state via:\\n```\\ngcloud compute instances list --project=$PROJECT | grep \"$DEPLOYMENT-shard-$SHARD_NUMBER\"\\n```\\nYou need to wait for the initial chef-client run to complete.\\nOne way to check is to tail the serial port output to check when the initial run is completed. An example:\\n```\\ngcloud compute --project=$PROJECT instances tail-serial-port-output $DEPLOYMENT-shard-$SHARD_NUMBER-01-db-$ENV --zone us-east1-{c/b/d}\\n```\\nAlso ensure that instance was bootstrapped and configured successfully by checking if `startup-script` finished with exit status 0. If it is not zero then check the reason and fix it and restart the instance once to initialize journald properly.\\n```\\ngcloud compute --project=$PROJECT instances tail-serial-port-output $DEPLOYMENT-shard-$SHARD_NUMBER-01-db-$ENV --zone us-east1-{c/b/d} | grep 'startup-script exit status'\\n```\\n### 4. Add instances in new shard to the cluster\\n[Section titled “4. Add instances in new shard to the cluster”](#4-add-instances-in-new-shard-to-the-cluster)\\na. SSH into one of the previous instances:\\n```\\nssh $DEPLOYMENT-shard-01-01-db-$ENV.c.$PROJECT.internal\\n```\\nb. Run the following:\\nNote: here we are exporting these env vars in the shell within Redis server.\\n```\\nexport ENV=<ENV>\\nexport PROJECT=<GCP_PROJECT>\\nexport DEPLOYMENT=redis-cluster-<INSTANCE_TYPE>\\nexport SHARD_NUMBER=<SHARD_NUMBER>\\n```, \"console on ~* &* +@all ><CONSOLE_REDACTED>\",\\n\"redis_exporter on +client +ping +info +config|get +cluster|info +slowlog +latency +memory +select +get +scan +xinfo +type +pfcount +strlen +llen +scard +zcard +hlen +xlen +eval allkeys ><EXPORTER_REDACTED>\",\\n\"rails on ~* &* +@all -debug ><RAILS_REDACTED>\"\\n]\\n}\\n}\\n}\\n```\\nDo the same for\\n```\\n./bin/gkms-vault-edit redis-exporter <ENV>\\n```\\nModify the existing JSON\\n```\\n{\\n\"redis_exporter\": {\\n\"redis-cluster-<INSTANCE_TYPE>\": {\\n\"env\": {\\n\"REDIS_PASSWORD\": \"<EXPORTER_REDACTED>\"\\n}\\n}\\n}\\n}\\n```\\n### 2. Create Chef roles\\n[Section titled “2. Create Chef roles”](#2-create-chef-roles)\\nSet the new chef roles and add the new role to the list of gitlab-redis roles in -infra-prometheus-server role.\\nAn example MR can be found [here](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/merge_requests/3494).\\n### 3. Provision VMs\\n[Section titled “3. Provision VMs”](#3-provision-vms)\\nProvision the VMs via the generic-stor/google terraform module. This is done in the [config-mgmt project in the ops environment](https://ops.gitlab.net/gitlab-com/gl-infra/config-mgmt/). An example MR can be found [here](https://ops.gitlab.net/gitlab-com/gl-infra/config-mgmt/-/merge_requests/5811).\\nAfter the MR is merged and applied, check the VM state via:\\n```\\ngcloud compute instances list --project <GCP_PROJECT> | grep 'redis-cluster-<INSTANCE_TYPE>'\\n```\\nYou need to wait for the initial chef-client run to complete.\\nOne way to check is to tail the serial port output to check when the initial run is completed. An example:\\n```\\ngcloud compute --project=<GCP_PROJECT> instances tail-serial-port-output redis-cluster-<INSTANCE_TYPE>-shard-01-01-db-<ENV> --zone us-east1-{c/b/d}\\n```\\n### 4. Initialising the cluster\\n[Section titled “4. Initialising the cluster”](#4-initialising-the-cluster)\\na. SSH into one of the instance:\\n```\\nssh redis-cluster-<INSTANCE_TYPE>-shard-01-01-db-<ENV>.c.<GCP_PROJECT>.internal\\n```\\nb. Run the following:\\n```\\nexport ENV=<ENV>\\nexport PROJECT=<GCP_PROJECT>\\nexport DEPLOYMENT=redis-cluster-<INSTANCE_TYPE>\\n```\\nc. Use the following command to connect the master-nodes and initialise a working cluster. Add more shard FQDN where necessary. e.g. `$DEPLOYMENT-shard-<shard_number>-01-db-$ENV.c.$PROJECT.internal:6379`.\\n```\\nsudo gitlab-redis-cli --cluster create \\\\n$DEPLOYMENT-shard-01-01-db-$ENV.c.$PROJECT.internal:6379 \\\\n$DEPLOYMENT-shard-02-01-db-$ENV.c.$PROJECT.internal:6379 \\\\n$DEPLOYMENT-shard-03-01-db-$ENV.c.$PROJECT.internal:6379\\n```\\nUse the following command to connect the remaining nodes to the cluster. Update `{01, 02, 03, ... n}-{02,03,..m}` where `n` is the number of shards and `m` is the number of instances per shard.\\n```\\nfor i in {01,02,03}-{02,03}; do\\nsudo gitlab-redis-cli --cluster add-node \\\\n$DEPLOYMENT-shard-$i-db-$ENV.c.$PROJECT.internal:6379 \\\\n$DEPLOYMENT-shard-01-01-db-$ENV.c.$PROJECT.internal:6379\\nsleep 2\\ndone\\n```\\nUse the following command, to assign the replicas within each shard. Update `{01, 02, 03, ... n}-{02,03,..n}` where necessary depending on the cluster-size.\\n```\\nfor i in {01,02,03}; do\\nfor j in {02,03}; do, - [Setting up instances](#setting-up-instances)\\n- [1. Update Redis app user to deny execution of debug command](#1-update-redis-app-user-to-deny-execution-of-debug-command)\\n- [2. Create Chef role for new shard](#2-create-chef-role-for-new-shard)\\n- [3. Provision VMs](#3-provision-vms)\\n- [4. Add instances in new shard to the cluster](#4-add-instances-in-new-shard-to-the-cluster)\\n- [5. Validation](#5-validation)\\n- [Enable debug command on Redis Cluster](#enable-debug-command-on-redis-cluster)\\n- [Scale out cluster by resharding Redis keys](#scale-out-cluster-by-resharding-redis-keys)\\n- [1. Check the current keyslot assignments](#1-check-the-current-keyslot-assignments)\\n- [2. Calibrate the batch size for migration](#2-calibrate-the-batch-size-for-migration)\\n- [3. Migrate keys from existing shards to new shard](#3-migrate-keys-from-existing-shards-to-new-shard)\\n- [4. Troubleshooting](#4-troubleshooting)\\n- [Key metrics to observe](#key-metrics-to-observe)\\n# Scaling Redis Cluster\\nThis document outlines the steps for scaling an existing Redis Cluster. Previous scaling Change Requests (CR) shows the operation in practice:\\n## Previous CRs for scaling `redis-cluster-ratelimiting`\\n[Section titled “Previous CRs for scaling redis-cluster-ratelimiting”](#previous-crs-for-scaling-redis-cluster-ratelimiting)\\n- [Provision nodes for new shard in redis-cluster-ratelimiting](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/18042)\\n- [Enable debug command](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/18043) (If the cluster already doesn’t have `debug` command enabled)\\n- [Migrate the keyslots](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/18061)\\n## Setting up instances\\n[Section titled “Setting up instances”](#setting-up-instances)\\nThis guide defines a few variables in `<>`, which are used in the scripts in this guide:\\n- `ENV`: `pre`, `gstg`, `gprd`\\n- `GCP_PROJECT`: `gitlab-production` or `gitlab-staging-1`\\n- `INSTANCE_TYPE`: `feature-flag`\\n- `SHARD_NUMBER`: sequential number of shard to be added e.g. in an existing cluster with 3 shards, this would be `04`\\n**Note:** To avoid mistakes in manually copy-pasting the variables in `<>` above during a provisioning session, it is recommended to prepare this doc with all the variables replaced beforehand.\\nUse them as environment variables in your local shell:\\n```\\nexport ENV=<ENV>\\nexport PROJECT=<GCP_PROJECT>\\nexport DEPLOYMENT=redis-cluster-<INSTANCE_TYPE>\\nexport SHARD_NUMBER=<SHARD_NUMBER>\\n```\\n### 1. Update Redis app user to deny execution of `debug` command\\n[Section titled “1. Update Redis app user to deny execution of debug command”](#1-update-redis-app-user-to-deny-execution-of-debug-command)\\nRedis clusters provisioned before April 2024, didn’t have `debug` command enabled, so at first, we need to enable the `debug` command and add ACL for `rails` (or any other app user e.g. `registry`) user to deny `-debug` command for it.\\nUpdate `redis-cluster` gkms vault secrets to disable `debug` command for `rails` user, using these commands in the [`chef-repo`](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/tree/master):\\n```\\n./bin/gkms-vault-edit redis-cluster $ENV\\n```\\nUpdate the JSON payload to include `-debug` for `rails` user for the target Redis Cluster (There is a space between `-debug` and `<`):\\n```\\n...\\n\"rails on ~* &* +@all -debug ><RAILS_REDACTED>\"\\n...\\n```\\nComplete json object, with modification:\\n```\\n{\\n...,\\n\"redis-cluster-<INSTANCE_TYPE>\": {\\n\"redis_conf\": {\\n\"masteruser\": \"replica\",\\n\"masterauth\": \"<REPLICA_REDACTED>\",\\n\"user\": [\\n\"default off\",, Terminal window\\n```\\n./scripts/redis-reconfigure.sh $ENVIRONMENT $INSTANCE_NAME bootstrap\\n```\\n### K8s\\n[Section titled “K8s”](#k8s)\\n1. [Create a new nodepool in terraform](https://ops.gitlab.net/gitlab-com/gl-infra/config-mgmt/-/merge_requests/4413)\\n2. [Update the tanka clusters](https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/tanka-deployments/-/blob/master/environments/redis/values.jsonnet)\\n3. Add secret to vault\\n### Redis-cluster\\n[Section titled “Redis-cluster”](#redis-cluster)\\n[Not yet implemented.](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/823))\\n## Observability\\n[Section titled “Observability”](#observability)\\nThe new redis type will require a new set of dashboards created from the redis archetype. [Example MR.](https://gitlab.com/gitlab-com/runbooks/-/merge_requests/5386)\\n## Migration process\\n[Section titled “Migration process”](#migration-process)\\n1. Create MRs to configure gitlab-rails for the new instance\\nThis needs to be done in both chef and k8s.\\nExample MRs\\n[Chef](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/merge_requests/2892)\\n[K8s](https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/2558)\\n2. Update chef-repo vault with the correct key.\\nIn the chef-repo repository, run the following command, and then update the section with the correct information.\\n```\\n$ bin/gkms-vault-edit gitlab-omnibus-secrets gprd\\n\"gitlab-rails\": {\\n...\\n\"redis_yml_override\": {\\n\"db_load_balancing\": {\\n\"url\": \"redis://<IAMAPASSWORD>@gprd-redis-db-load-balancing\"\\n}\\n},\\n...\\n```\\n3. Use feature flags to turn on and off the dual writes\\nUse feature flags to transition to the new store. Between each step, check [error metrics](https://thanos.gitlab.net/graph?g0.expr=sum%20by%20(env%2C%20stage%2C%20instance_name)%20(rate(gitlab_redis_multi_store_pipelined_diff_error_total%5B1m%5D))&g0.tab=0&g0.stacked=0&g0.range_input=6h&g0.max_source_resolution=0s&g0.deduplicate=1&g0.partial_response=0&g0.store_matches=%5B%5D&g1.expr=rate(gitlab_redis_multi_store_method_missing_total%7Benv%3D%22gstg%22%7D%5B10m%5D)&g1.tab=1&g1.stacked=0&g1.range_input=6h&g1.max_source_resolution=0s&g1.deduplicate=1&g1.partial_response=0&g1.store_matches=%5B%5D) and [error logs](https://nonprod-log.gitlab.net/goto/781e9c40-ad59-11ed-9af2-6131f0ee4ce6).\\nYou should also let enough time elapse between feature toggles to “warm up” the new store. The amount of time required to warm up a new instance depends on the usage pattern. Often, looking at the info keyspace ttl (which is in milliseconds) and multiplying it times two will get you a pretty good guestimate. For some usage patterns, we do not have a TTL set, and those will require a different method of rollout. See [scalability #2193](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2193) for more information.\\nThe sequence of feature flag toggles you want to follow is:\\n- Turn on `use_primary_and_secondary_stores_for_<store_name>`\\n- Turn on `use_primary_store_as_default_for_<store_name>`\\n- Turn off `use_primary_and_secondary_stores_for_<store_name>`\\nAfter the first feature flag is toggled, you should begin to see activity on your new instance. Look for overall RPS, primary RPS and connected clients on the appropriate Grafana dashboard. Another good command to use is info keyspace on the new redis instance.\\nBefore dual writes:\\n```\\nsudo gitlab-redis-cli, - workhorse\\n- [Workhorse Service](/workhorse/)\\n- [Workhorse Apdex Degradation](/workhorse/workhorse-apdex-degradation/)\\n- workspaces\\n- [Remote Development Workspaces Service](/workspaces/)\\n- zoekt\\n- [Exact code search Service](/zoekt/)\\n(() => {\\nconst scroller = document.getElementById('starlight\\_\\_sidebar');\\nif (!window.\\_starlightScrollRestore || !scroller) return;\\nscroller.scrollTop = window.\\_starlightScrollRestore;\\ndelete window.\\_starlightScrollRestore;\\n})();\\n[GitLab](https://gitlab.com/gitlab-com/runbooks)[Code Context](https://code-context.runway.gitlab.net/runbooks-docs)\\nSelect theme   DarkLightAuto\\nStarlightThemeProvider.updatePickers();\\nconst a=document.getElementById(\"starlight\\_\\_sidebar\"),n=a?.querySelector(\"sl-sidebar-state-persist\"),o=\"sl-sidebar-state\",i=()=>{let t=[];const e=n?.dataset.hash||\"\";try{const s=sessionStorage.getItem(o),r=JSON.parse(s||\"{}\");Array.isArray(r.open)&&r.hash===e&&(t=r.open)}catch{}return{hash:e,open:t,scroll:a?.scrollTop||0}},c=t=>{try{sessionStorage.setItem(o,JSON.stringify(t))}catch{}},d=()=>c(i()),l=(t,e)=>{const s=i();s.open[e]=t,c(s)};n?.addEventListener(\"click\",t=>{if(!(t.target instanceof Element))return;const e=t.target.closest(\"summary\")?.closest(\"details\");if(!e)return;const s=e.querySelector(\"sl-sidebar-restore\"),r=parseInt(s?.dataset.index||\"\");isNaN(r)||l(!e.open,r)});addEventListener(\"visibilitychange\",()=>{document.visibilityState===\"hidden\"&&d()});addEventListener(\"pageHide\",d);\\nOn this page\\n- [Overview](#_top)\\n- [Setting up instances](#setting-up-instances)\\n- [1. Generate Redis passwords](#1-generate-redis-passwords)\\n- [2. Create Chef roles](#2-create-chef-roles)\\n- [3. Provision VMs](#3-provision-vms)\\n- [4. Initialising the cluster](#4-initialising-the-cluster)\\n- [5. Validation](#5-validation)\\n- [Configuring the applications](#configuring-the-applications)\\n- [1. Configure console instances](#1-configure-console-instances)\\n- [2. Verification of config in a VM](#2-verification-of-config-in-a-vm)\\n- [3. Configure Gitlab Rails](#3-configure-gitlab-rails)\\n- [4. Troubleshooting](#4-troubleshooting)\\n## On this page\\n- [Overview](#_top)\\n- [Setting up instances](#setting-up-instances)\\n- [1. Generate Redis passwords](#1-generate-redis-passwords)\\n- [2. Create Chef roles](#2-create-chef-roles)\\n- [3. Provision VMs](#3-provision-vms)\\n- [4. Initialising the cluster](#4-initialising-the-cluster)\\n- [5. Validation](#5-validation)\\n- [Configuring the applications](#configuring-the-applications)\\n- [1. Configure console instances](#1-configure-console-instances)\\n- [2. Verification of config in a VM](#2-verification-of-config-in-a-vm)\\n- [3. Configure Gitlab Rails](#3-configure-gitlab-rails)\\n- [4. Troubleshooting](#4-troubleshooting)\\n# Provisioning Redis Cluster\\nThis document outlines the steps for provisioning a Redis Cluster. Former attempts are documented here:\\n- [`redis-cluster-ratelimiting`](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2256)\\n- [`redis-cluster-chat-cache`](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2358)\\n## Setting up instances\\n[Section titled “Setting up instances”](#setting-up-instances)\\nFirst, this guide defines a few variables in `<>`, namely:\\n- `ENV`: `pre`, `gstg`, `gprd`\\n- `GCP_PROJECT`: `gitlab-production` or `gitlab-staging-1`\\n- `INSTANCE_TYPE`: `feature-flag`] # How Chef Helps in Setting Up Instances for Redis\\n\\n## 1. Direct Answer\\nChef automates the provisioning and configuration of Redis instances by managing the necessary configurations, roles, and environment settings. It ensures that the Redis instances are set up correctly and consistently across different environments.\\n\\n## 2. Step-by-Step Instructions\\n\\n### Step 1: Update Redis App User to Deny Execution of Debug Command\\n1. Navigate to the `chef-repo` directory.\\n2. Run the following command to edit the gkms vault secrets for the Redis cluster:\\n   ```bash\\n   ./bin/gkms-vault-edit redis-cluster $ENV\\n   ```\\n3. Update the JSON payload to include `-debug` for the `rails` user:\\n   ```json\\n   \"rails on ~* &* +@all -debug ><RAILS_REDACTED>\"\\n   ```\\n4. Ensure the complete JSON object reflects this change.\\n\\n### Step 2: Create Chef Role for New Shard\\n1. Create a new Chef role for the new Redis shard based on existing roles.\\n2. Add the `enable-debug-command` option to the Redis configuration in the base role if it is not already included.\\n3. Submit a merge request (MR) for the new role. An example MR can be found [here](https://gitlab.com/gitlab-com/gl-infra/chef-repo/-/merge_requests/4789).\\n\\n### Step 3: Provision VMs\\n1. In the `config-mgmt` project, increment the `count` parameter in the generic-stor/google terraform module for the target Redis Cluster.\\n2. Submit a merge request (MR) and apply it. An example MR can be found [here](https://ops.gitlab.net/gitlab-com/gl-infra/config-mgmt/-/merge_requests/8474).\\n3. Check the VM state using:\\n   ```bash\\n   gcloud compute instances list --project=$PROJECT | grep \"$DEPLOYMENT-shard-$SHARD_NUMBER\"\\n   ```\\n4. Wait for the initial chef-client run to complete. You can check the status by tailing the serial port output:\\n   ```bash\\n   gcloud compute --project=$PROJECT instances tail-serial-port-output $DEPLOYMENT-shard-$SHARD_NUMBER-01-db-$ENV --zone us-east1-{c/b/d}\\n   ```\\n5. Ensure the instance was bootstrapped successfully by checking the `startup-script` exit status:\\n   ```bash\\n   gcloud compute --project=$PROJECT instances tail-serial-port-output $DEPLOYMENT-shard-$SHARD_NUMBER-01-db-$ENV --zone us-east1-{c/b/d} | grep 'startup-script exit status'\\n   ```\\n\\n### Step 4: Add Instances in New Shard to the Cluster\\n1. SSH into one of the previous instances:\\n   ```bash\\n   ssh $DEPLOYMENT-shard-01-01-db-$ENV.c.$PROJECT.internal\\n   ```\\n2. Set the necessary environment variables:\\n   ```bash\\n   export ENV=<ENV>\\n   export PROJECT=<GCP_PROJECT>\\n   export DEPLOYMENT=redis-cluster-<INSTANCE_TYPE>\\n   export SHARD_NUMBER=<SHARD_NUMBER>\\n   ```\\n\\n### Step 5: Initialize the Cluster\\n1. SSH into one of the new instances:\\n   ```bash\\n   ssh redis-cluster-<INSTANCE_TYPE>-shard-01-01-db-<ENV>.c.<GCP_PROJECT>.internal\\n   ```\\n2. Run the following command to create the cluster:\\n   ```bash\\n   sudo gitlab-redis-cli --cluster create \\\\n   $DEPLOYMENT-shard-01-01-db-$ENV.c.$PROJECT.internal:6379 \\\\n   $DEPLOYMENT-shard-02-01-db-$ENV.c.$PROJECT.internal:6379 \\\\n   $DEPLOYMENT-shard-03-01-db-$ENV.c.$PROJECT.internal:6379\\n   ```\\n3. Add remaining nodes to the cluster:\\n   ```bash\\n   for i in {01,02,03}-{02,03}; do\\n   sudo gitlab-redis-cli --cluster add-node \\\\n   $DEPLOYMENT-shard-$i-db-$ENV.c.$PROJECT.internal:6379 \\\\n   $DEPLOYMENT-shard-01-01-db-$ENV.c.$PROJECT.internal:6379\\n   sleep 2\\n   done\\n   ```\\n\\n## 3. Key Commands\\n- **Update gkms vault secrets:**\\n  ```bash\\n  ./bin/gkms-vault-edit redis-cluster $ENV\\n  ```\\n- **Check VM state:**\\n  ```bash\\n  gcloud compute instances list --project=$PROJECT | grep \"$DEPLOYMENT-shard-$SHARD_NUMBER\"\\n  ```\\n- **Tail serial port output:**\\n  ```bash\\n  gcloud compute --project=$PROJECT instances tail-serial-port-output $DEPLOYMENT-shard-$SHARD_NUMBER-01-db-$ENV --zone us-east1-{c/b/d}\\n  ```\\n\\n## 4. Important Notes\\n- Ensure that you have the correct permissions to edit the gkms vault and provision VMs.\\n- The environment variables (`ENV`, `PROJECT`, `DEPLOYMENT`, `SHARD_NUMBER`) must be set correctly before running commands.\\n- Monitor the initial chef-client run to ensure that the configuration is applied successfully.\\n- Always check the exit status of the `startup-script` to troubleshoot any issues during instance initialization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Chef is used in the process of setting up instances for Redis by running `chef-client` to update the node and render the updated configuration files. This is crucial as the pipeline does not check the correctness of the config files, which may impact the deploy-node since GitLab Rails connects to Redis instances on start-up.           NaN             0.965              1.000           1.000\n",
            "                                                  What is the significance of database connection in cloud services, particularly in the context of Redis?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [Redis is in the usual 3-node single-primary/two-replica VM configuration, with sentinels on the same VMs as Redis (as for persistent and sidekiq clusters)\\nSingle threaded CPU is the normal constraint for Redis but due to the large data transfers networking *may* become a constraint before CPU does on this cluster, although there is a *lot* of headroom (2 orders of magnitude) at this writing (2021) and scaling up the instances is possible to obtain more (or maybe the limits will be higher by then)\\nThe usual persistent redis availability expectations for our 3-node clusters apply (see <https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis-survival-guide-for-sres.md#high-availability> )\\nData is expected to be largely persistent; some data loss may occur during an unplanned failover event, but in all normal operations all data in this instance is expected to be fully durable.\\nUses mostly the same metrics-catalog definitions as for the other Redis clusters, except for memory saturation which is deliberately set very low; the transient nature of the data here means any build up is abnormal and should be investigated early while it is easier to recover from.\\n- Implementation/migration epic: <https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/462>\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis-tracechunks/README.md)\\n[Previous\\nRedis Sidekiq Service](/redis-sidekiq/)   [Next\\nContainerRegistryDBLoadBalancerReplicaPoolSize](/registry/alerts/ContainerRegistryDBLoadBalancerReplicaPoolSize/), - [Network partition](#network-partition)\\n- [Persistence Guarantees](#persistence-guarantees)\\n- [What do we store?](#what-do-we-store)\\n- [Clients](#clients)\\n- [What about redis cluster?](#what-about-redis-cluster)\\n- [Maintanence](#maintanence)\\n- [Reconfigure](#reconfigure)\\n- [Debugging and Diagnosis](#debugging-and-diagnosis)\\n- [Identifying the primary](#identifying-the-primary)\\n- [Basic stats](#basic-stats)\\n- [Slowlogs](#slowlogs)\\n- [ElasticSearch analysis](#elasticsearch-analysis)\\n- [Monitor commands](#monitor-commands)\\n- [Flamegraphs](#flamegraphs)\\n- [Bigkeys](#bigkeys)\\n# A survival guide for SREs to working with Redis at GitLab\\nSee also <https://docs.gitlab.com/ee/development/redis.html> which covers some of the same\\nground, but with a developer orientation and the SRE-oriented [runbook](https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis.md)\\n## What is Redis\\n[Section titled “What is Redis”](#what-is-redis)\\nRedis is a fast in-memory key-value store. It offers a number of data types, from simple strings to sets, hashes,\\nor complex data types like HyperLogLog. It is fundamentally single-threaded in its core loop, which keeps the\\nimplementation simple and robust (e.g. tasks execute serially with no lock contention). However, that means it is\\nconstrained to the performance of a single CPU, and any slow task delays the start of other queued tasks. We have had\\nscaling problems in the past on GitLab.com where the single CPU became the bottleneck, and our architectural design\\nhas evolved to take that into account.\\n## Why do we use it?\\n[Section titled “Why do we use it?”](#why-do-we-use-it)\\nWe use it both as a cache (for performance savings) and to store various types of persistent data, either directly and\\nexplicitly (typically from Rails), or implicitly by using Sidekiq or `Rails.cache`\\nCaching for performance is primarily to reduce the load on other systems, particularly Postgres and Gitaly which are often\\nbottlenecks (or single points of failure) This can also save time in the web tier (e.g. rendering Markdown), but more\\nas a side-effect than the direct reason.\\n## Architecture\\n[Section titled “Architecture”](#architecture)\\nFor gitlab.com, as at September 2023, we have 11 sets of Redis instances, each handling a distinct use case:\\nRoleNodesClientsSentinel?Persistence?Cache for `Rails.cache`redis-cluster-cache-XX-YYPuma workers, Sidekiq workersNoNoneCache for Chat ([To be decommissioned](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/19221))redis-cluster-chat-cache-XX-YYPuma workersNoNoneCache for Feature flagredis-cluster-feature-flag-XX-YYPuma workersNoNoneCache for Repository dataredis-repository-cache-XXPuma workersYes (localhost)NoneSidekiq job queuesredis-sidekiq-XXPuma workers, Sidekiq workersYes (localhost)RDB dump every 900 secondsPersistent shared stateredis-XXPuma workers, Sidekiq workers, WorkhorseYes (localhost)RDB dump every 900 secondsCI build trace chunksredis-tracechunks-XXPuma workers (API), Sidekiq workersYes (localhost)RDB dump every 900 secondsRatelimiting (RackAttack/App)redis-cluster-ratelimiting-XX-YYPuma workersNoNoneSessionsredis-sessions-XXPuma workersYes (localhost)RDB dump every 900 seconds, but also an eviction policyLatest WAL pointerredis-db-load-balancing-XXPuma workers, Sidekiq workersYes (localhost)RDB dump every 900 seconds, but also an eviction policyPub/Subredis-pubsub-XXPuma workers, WorkhorseYes (localhost)No\\nThe split is *largely* a form of functional partitioning for scalability (see single threaded comments above), but also\\nbecause the application expects the non-cache instances to persist data across failures or restarts so those instances\\nmust write data to disk periodically (in addition to replication/failover capability). It’s not a full DBMS with\\nguaranteed write semantics (and performance implications to match), but it is sufficiently persistent that there wouldn’t, manage the data/expiry (no client activity required; old data just ages out)\\nIt is *not* a service to rate limit requests to Redis itself.\\nThis Redis instance was split off from `redis-cache` because as found in <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3034#note_460538394> RackAttack\\nalone accounted for somewhere in the order of 25% (absolute) CPU usage on the redis-cache instance in Nov 2020, and that will\\nonly have grown with traffic.\\nAs the cache cluster approached 95% saturation it was determined the best short-term approach was to split out the rate-limiting\\ndata storage to its own cluster before we go further with more horizontal scalability.\\nThe `redis-cluster-ratelimiting` instance is the horizontally scalable\\ncounterpart of `redis-ratelimiting` whichs runs in Sentinel mode. More\\ninformation on efforts to horizontally scale Redis instances can be found [here](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/823).\\n## Architecture\\n[Section titled “Architecture”](#architecture)\\nRedis is in a 3-node-per-shard (single-primary/two-replica per shard) VM configuration, in\\ncluster mode. The node count will increase as we scale horizontally.\\n## Scalability\\n[Section titled “Scalability”](#scalability)\\nSingle threaded CPU is the normal constraint for Redis, and will in particular be the case here. It is CPU heavy, and *not* data\\nheavy. Analysis suggests data volumes are in the order of MBs, not GBs, and small numbers at that.\\nRedis Cluster is horizontally scalable as decsribed in the\\n[specifications](https://redis.io/docs/reference/cluster-spec/).\\n## Availability\\n[Section titled “Availability”](#availability)\\nThe usual redis availability expectations for our 3-node-per-shard clusters apply (see <https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis-survival-guide-for-sres.md#high-availability>)\\nFor Redis instances in cluster mode, each shard is made of 1 master node and at\\nleast 2 replica nodes, similar to a Redis sentinel setup.\\nThe availability would depend on the configuration for the Redis instance:\\n- `cluster-require-full-coverage`: If this is set to yes, as it is by default,\\nthe cluster stops accepting writes if some percentage of the key space is not\\ncovered by any node.\\n- `cluster-allow-reads-when-down`: If this is set to no, as it is by default,\\na node in a Redis Cluster will stop serving all traffic when the cluster is\\nmarked as failed, either when a node can’t reach a quorum of masters or when\\nfull coverage is not met.\\nMore information can be found\\n[here](https://redis.io/docs/management/scaling/).\\n## Durability\\n[Section titled “Durability”](#durability)\\nUnimportant; data has a (almost exclusively) 1 minute useful period. If we lost all data in this cluster, within 1 minute (at most) all effects would be passed, and at worst during that 1 minute some users might be able to access the system at up to double the usual rate-limit (an absolute upper limit, depending on which part of the minute the failure happened and the distribution of their requests during that period).\\n## Monitoring/Alerting\\n[Section titled “Monitoring/Alerting”](#monitoringalerting)\\nUses the same metrics-catalog definitions as for the other Redis clusters.\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis-cluster-ratelimiting/README.md)\\n[Previous\\nRedis Cluster Queues Meta Service](/redis-cluster-queues-meta/)   [Next\\nRedis Cluster Registry Service](/redis-cluster-registry/), - workhorse\\n- [Workhorse Service](/workhorse/)\\n- [Workhorse Apdex Degradation](/workhorse/workhorse-apdex-degradation/)\\n- workspaces\\n- [Remote Development Workspaces Service](/workspaces/)\\n- zoekt\\n- [Exact code search Service](/zoekt/)\\n(() => {\\nconst scroller = document.getElementById('starlight\\_\\_sidebar');\\nif (!window.\\_starlightScrollRestore || !scroller) return;\\nscroller.scrollTop = window.\\_starlightScrollRestore;\\ndelete window.\\_starlightScrollRestore;\\n})();\\n[GitLab](https://gitlab.com/gitlab-com/runbooks)[Code Context](https://code-context.runway.gitlab.net/runbooks-docs)\\nSelect theme   DarkLightAuto\\nStarlightThemeProvider.updatePickers();\\nconst a=document.getElementById(\"starlight\\_\\_sidebar\"),n=a?.querySelector(\"sl-sidebar-state-persist\"),o=\"sl-sidebar-state\",i=()=>{let t=[];const e=n?.dataset.hash||\"\";try{const s=sessionStorage.getItem(o),r=JSON.parse(s||\"{}\");Array.isArray(r.open)&&r.hash===e&&(t=r.open)}catch{}return{hash:e,open:t,scroll:a?.scrollTop||0}},c=t=>{try{sessionStorage.setItem(o,JSON.stringify(t))}catch{}},d=()=>c(i()),l=(t,e)=>{const s=i();s.open[e]=t,c(s)};n?.addEventListener(\"click\",t=>{if(!(t.target instanceof Element))return;const e=t.target.closest(\"summary\")?.closest(\"details\");if(!e)return;const s=e.querySelector(\"sl-sidebar-restore\"),r=parseInt(s?.dataset.index||\"\");isNaN(r)||l(!e.open,r)});addEventListener(\"visibilitychange\",()=>{document.visibilityState===\"hidden\"&&d()});addEventListener(\"pageHide\",d);\\nOn this page\\n- [Overview](#_top)\\n- [Core concepts](#core-concepts)\\n- [Glossary](#glossary)\\n- [Observability](#observability)\\n- [Troubleshooting](#troubleshooting)\\n- [What is the state of the cluster?](#what-is-the-state-of-the-cluster)\\n- [How to initialize a new cluster](#how-to-initialize-a-new-cluster)\\n- [How to recover from all nodes going down](#how-to-recover-from-all-nodes-going-down)\\n- [How to force promotion of a replica in an emergency where Redis cannot heal itself](#how-to-force-promotion-of-a-replica-in-an-emergency-where-redis-cannot-heal-itself)\\n- [How to do online resharding (with warnings)?](#how-to-do-online-resharding-with-warnings)\\n- [How do we rotate passwords in the ACL list?](#how-do-we-rotate-passwords-in-the-acl-list)\\n## On this page\\n- [Overview](#_top)\\n- [Core concepts](#core-concepts)\\n- [Glossary](#glossary)\\n- [Observability](#observability)\\n- [Troubleshooting](#troubleshooting)\\n- [What is the state of the cluster?](#what-is-the-state-of-the-cluster)\\n- [How to initialize a new cluster](#how-to-initialize-a-new-cluster)\\n- [How to recover from all nodes going down](#how-to-recover-from-all-nodes-going-down)\\n- [How to force promotion of a replica in an emergency where Redis cannot heal itself](#how-to-force-promotion-of-a-replica-in-an-emergency-where-redis-cannot-heal-itself)\\n- [How to do online resharding (with warnings)?](#how-to-do-online-resharding-with-warnings)\\n- [How do we rotate passwords in the ACL list?](#how-do-we-rotate-passwords-in-the-acl-list)\\n# Redis Cluster\\n## Core concepts\\n[Section titled “Core concepts”](#core-concepts)\\nRedis Cluster is a mode of Redis to enable horizontal scaling. The dataset is divided among multiple nodes, allowing the computational load to be distributed.\\nHow does Redis Cluster differ from non-clustered Sentinel-based Redis instances?, The split is *largely* a form of functional partitioning for scalability (see single threaded comments above), but also\\nbecause the application expects the non-cache instances to persist data across failures or restarts so those instances\\nmust write data to disk periodically (in addition to replication/failover capability). It’s not a full DBMS with\\nguaranteed write semantics (and performance implications to match), but it is sufficiently persistent that there wouldn’t\\nbe enormous implications if we did have a substantial failure and had to go back to the RDB files on disk. However,\\nhaving this level of persistence for our large cache (currently ~60GB of cache) would be expensive in computation and I/O,\\nfor insufficient benefit. While we don’t want to lose our cache regularly, we can certainly survive an occasional loss\\nin unlikely circumstances (all 3 nodes die at once, which would probably mean all or much of the rest of our\\ninfrastructure is also down or badly affected).\\nAt the time (mid 2021) we chose to split CI build trace chunks into it’s own instance, CI trace chunks were responsible for roughly 60% of the\\ndata throughput into/out of the shared state redis, and 16% of Redis calls (see <https://gitlab.com/gitlab-org/gitlab/-/issues/327469#note_556531587>)\\nwhich was sufficient reason for the split, along with the distinctive usage profile (transient data on its way to permanent storage).\\nWhen we split out Ratelimiting (latter-half of 2021) this was for CPU saturation; the cache instance was peaking at a little over\\n90% CPU, and we knew from when we [enabled RackAttack in November 2020](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3034)\\nthat it is responsible for at least 25% (absolute) of the CPU utilization, so splitting this out gives the cache instance room\\nto breath. Note that the [data usage](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1246#sizing) is tiny.\\nWhen we split out Sessions (very late 2021), this was for CPU saturation (antipicated something like 20% absolute savings)\\nand also to separate a workload that may be problematic for Redis Cluster, unblocking perhaps moving the persistent Redis\\nto Cluster in the future.\\n### CPUs\\n[Section titled “CPUs”](#cpus)\\nRedis VMs were the first nodes we switched to from N1 to ‘[C2](https://cloud.google.com/compute/docs/machine-types#c2_machine_types)’\\nnode types for the best raw single-threaded CPU performance. This [halved](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/230#note_312403063)\\nthe CPU usage on our sidekiq cluster, and [almost the same](https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/9636)\\non the cache cluster. Just in case you were in any doubt as to how important the single-threaded CPU performance was\\nto redis.\\nRedis 6 has [multithreaded I/O](https://github.com/redis/redis/pull/6038/files) which helps by moving\\nsome network I/O work to non-core threads, but the core work must still occur on the main thread, so it is only a\\nmitigation.\\n## High Availability\\n[Section titled “High Availability”](#high-availability)\\nFor each cluster we run 3 nodes, using [Redis Sentinel](https://redis.io/topics/sentinel) to manage failover. All\\ntraffic goes through the currently active primary, and writes are replicated to the two replicas asynchronously. If the\\nprimary fails (as determined by Sentinel), one of the replicas is promoted to primary.\\nClients are configured to connect to the sentinels which will then direct them to the current primary; there is no load\\nbalancer involved in this path. Failover is automatic (handled by Sentinel) and results in clients being disconnected\\nfrom the primary and then reconnecting via the sentinels again. It requires no operator intervention under normal circumstances\\nThe configuration is subtly different across the clusters, for historical reasons; the persistent and sidekiq clusters\\nhave sentinel running on the VMs alongside redis, whereas the cache cluster uses a distinct set of sentinel VMs.\\n<https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/11389> records the desire to clean this up.\\n### Node failure\\n[Section titled “Node failure”](#node-failure)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       # Significance of Database Connection in Cloud Services for Redis\\n\\n## 1. Direct Answer\\nThe database connection in cloud services, particularly for Redis, is crucial for ensuring high availability, performance, and durability of data. Redis operates in a single-threaded manner, meaning that it can only handle one command at a time per instance. This makes efficient connection management essential to avoid bottlenecks and ensure that applications can quickly access and manipulate data. Properly configured connections also facilitate automatic failover and replication, which are vital for maintaining service continuity in a cloud environment.\\n\\n## 2. Step-by-Step Instructions\\n\\n### Step 1: Understand Redis Architecture\\n- Redis operates in a 3-node configuration (1 primary and 2 replicas) with Sentinel for high availability.\\n- Clients connect to Sentinels, which direct them to the current primary node for read/write operations.\\n\\n### Step 2: Configure Client Connections\\n- Ensure that your application is configured to connect to the Redis Sentinels rather than directly to the Redis nodes. This allows for automatic failover handling.\\n- Example configuration for a Redis client (e.g., in Ruby):\\n  ```ruby\\n  require 'redis'\\n  require 'redis-sentinel'\\n\\n  sentinel = Redis::Sentinel.new(['sentinel1:26379', 'sentinel2:26379', 'sentinel3:26379'])\\n  redis = sentinel.master(:my_master)\\n  ```\\n\\n### Step 3: Monitor Connection Health\\n- Regularly monitor the health of the Redis connections using metrics such as connection count, latency, and error rates.\\n- Use Redis commands to check the status:\\n  ```bash\\n  redis-cli info clients\\n  redis-cli ping\\n  ```\\n\\n### Step 4: Handle Failover Scenarios\\n- In case of a primary node failure, ensure that your application can gracefully handle reconnections to the new primary node.\\n- Implement retry logic in your application to reconnect to Redis after a failover event.\\n\\n### Step 5: Optimize Connection Pooling\\n- Use connection pooling in your application to manage Redis connections efficiently. This reduces the overhead of establishing new connections.\\n- Example configuration for connection pooling (in Ruby):\\n  ```ruby\\n  require 'connection_pool'\\n\\n  pool = ConnectionPool.new(size: 5, timeout: 5) { Redis.new }\\n  ```\\n\\n## 3. Key Commands\\n- **Check Redis Status:**\\n  ```bash\\n  redis-cli info\\n  ```\\n- **Ping Redis:**\\n  ```bash\\n  redis-cli ping\\n  ```\\n- **Monitor Clients:**\\n  ```bash\\n  redis-cli info clients\\n  ```\\n\\n## 4. Important Notes\\n- **Single-Threaded Limitation:** Redis is single-threaded, so ensure that your application does not overwhelm it with too many simultaneous requests.\\n- **Network Constraints:** Be aware that network performance can become a bottleneck before CPU usage in Redis, especially during large data transfers.\\n- **Data Persistence:** While Redis is designed for high availability, some data loss may occur during unplanned failover events. Ensure your application can tolerate this.\\n- **Monitoring and Alerts:** Set up monitoring and alerting based on Redis metrics to catch issues early, especially regarding connection saturation and latency.\\n\\nBy following these steps and considerations, you can effectively manage Redis connections in a cloud environment, ensuring high availability and performance for your applications.                                                                                                                                                                                                                                                                                                                                                                                       The significance of database connection in cloud services, particularly in the context of Redis, lies in its role in ensuring reliable data storage and caching. Redis operates in a cluster mode with a specific configuration of nodes, which facilitates efficient data access and management. This setup is crucial for maintaining the performance and reliability of applications that depend on Redis for their chat features and other functionalities.         0.459             0.982              0.589           0.667\n",
            "                               What are the key aspects of Patroni cluster management in relation to PostgreSQL, and how does it ensure high availability?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [- [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/), - [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/), - [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/), - [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/), - [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      # Direct Answer\\nPatroni is a tool for managing PostgreSQL clusters that ensures high availability through automated failover, leader election, and replication management. It leverages distributed consensus algorithms (like etcd or Consul) to maintain cluster state and manage leader nodes, ensuring that the database remains operational even in the event of node failures.\\n\\n# Step-by-Step Instructions\\n1. **Cluster Setup**:\\n   - Install Patroni on all nodes in the cluster.\\n   - Configure PostgreSQL on each node to work with Patroni by editing the `postgresql.conf` and `pg_hba.conf` files.\\n   - Create a Patroni configuration file (usually `patroni.yml`) that specifies cluster settings, including the DCS (Distributed Configuration Store) settings (e.g., etcd or Consul).\\n\\n2. **Start Patroni**:\\n   - Use the command to start Patroni on each node:\\n     ```bash\\n     patroni /path/to/patroni.yml\\n     ```\\n\\n3. **Cluster Initialization**:\\n   - Initialize the cluster by starting Patroni on the first node. This node will become the primary (leader) node.\\n   - Monitor the logs to ensure that the cluster is initialized correctly.\\n\\n4. **Adding Standby Nodes**:\\n   - Start Patroni on standby nodes with the same configuration file.\\n   - Patroni will automatically detect the primary node and replicate data.\\n\\n5. **Monitoring and Alerts**:\\n   - Set up monitoring for the Patroni cluster using tools like Prometheus and Grafana.\\n   - Configure alerts for critical events such as leader election, node failures, or replication lag.\\n\\n6. **Failover Management**:\\n   - In case of a primary node failure, Patroni will automatically promote a standby node to primary.\\n   - Monitor the logs to confirm the failover process.\\n\\n7. **Backup and Recovery**:\\n   - Implement a backup strategy using tools like WAL-G or pgBackRest.\\n   - Regularly test recovery procedures to ensure data integrity.\\n\\n# Key Commands\\n- **Start Patroni**:\\n  ```bash\\n  patroni /path/to/patroni.yml\\n  ```\\n\\n- **Check Cluster Status**:\\n  ```bash\\n  patronictl -c /path/to/patroni.yml list\\n  ```\\n\\n- **Trigger Manual Failover**:\\n  ```bash\\n  patronictl -c /path/to/patroni.yml failover\\n  ```\\n\\n- **View Logs**:\\n  ```bash\\n  journalctl -u patroni\\n  ```\\n\\n# Important Notes\\n- **Prerequisites**: Ensure that PostgreSQL is installed and configured on all nodes. Patroni requires a distributed configuration store (like etcd or Consul) to manage cluster state.\\n- **Network Configuration**: All nodes must be able to communicate with each other over the network. Proper firewall rules should be in place.\\n- **Data Consistency**: Regularly monitor replication lag and ensure that backups are taken frequently to prevent data loss.\\n- **Testing Failover**: Regularly test the failover process in a staging environment to ensure that the team is familiar with the procedures during an actual incident.                                                                                                                                                                                                                                                          Patroni cluster management is crucial for maintaining high availability in PostgreSQL environments. It automates the management of PostgreSQL clusters, ensuring that the database remains operational even in the event of node failures. Key aspects include the use of distributed consensus algorithms to manage leader election and failover processes, as well as the integration with tools like Pgbouncer for connection pooling. This setup allows for seamless scaling and recovery, thereby enhancing the reliability and performance of production systems utilizing PostgreSQL.         1.000             0.825              0.000           0.400\n",
            "                             What are the key aspects of Patroni Cluster Management and how does it relate to secret detection in PostgreSQL environments?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [- [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/), - [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/), - [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/), - [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/), - [Re-indexing a package](/packagecloud/reindex-package/)\\n- packaging\\n- [GPG Keys for Package Signing](/packaging/manage-package-signing-keys/)\\n- pages\\n- [Block specific pages domains through HAproxy](/pages/block-pages-domain/)\\n- [GitLab Pages returning 404](/pages/gitlab-pages/)\\n- [Pages Service](/pages/)\\n- [Determine The GitLab Project Associated with a Domain](/pages/pages-domain-lookup/)\\n- [Troubleshooting LetsEncrypt for Pages](/pages/pages-letsencrypt/)\\n- patroni\\n- alerts\\n- [PatroniGCSSnapshotDelayed](/patroni/alerts/PatroniGCSSnapshotDelayed/)\\n- [PatroniLongRunningTransactionDetected](/patroni/alerts/PatroniLongRunningTransactionDetected/)\\n- [PatroniScrapeFailures](/patroni/alerts/PatroniScrapeFailures/)\\n- [PostgresSplitBrain](/patroni/alerts/PostgresSplitBrain/)\\n- [walgBaseBackupDelayed, WALGBaseBackupFailed](/patroni/alerts/walgBaseBackup/)\\n- [Steps to create (or recreate) a Standby CLuster using a Snapshot from a Production cluster as Master cluster (instead of pg\\_basebackup)](/patroni/build_cluster_from_snapshot/)\\n- [Check the status of transaction wraparound Runbook](/patroni/check_wraparound/)\\n- [Custom PostgreSQL Package Build Process for Ubuntu Xenial 16.04](/patroni/custom_postgres_packages/)\\n- [database\\_peak\\_analysis](/patroni/database_peak_analysis/)\\n- [How and when to deprovision the db-benchmarking environment](/patroni/deprovision-db-benchmarking-env/)\\n- [Patroni GCS Snapshots](/patroni/gcs-snapshots/)\\n- [Geo Patroni Cluster Management](/patroni/geo-patroni-cluster/)\\n- [gitlab-com-wale-backups](/patroni/gitlab-com-wale-backups/)\\n- [gitlab-com-walg-backups](/patroni/gitlab-com-walg-backups/)\\n- [Postgres (Patroni) Service](/patroni/)\\n- [Log analysis on PostgreSQL, Pgbouncer, Patroni and consul Runbook](/patroni/log_analysis/)\\n- [Making a manual clone of the DB for the data team](/patroni/manual_data_team_clone/)\\n- [Mapping Postgres Statements, Slowlogs, Activity Monitoring and Traces](/patroni/mapping_statements/)\\n- [OS Upgrade Reference Architecture](/patroni/os_upgrade_reference_architecture/)\\n- [Patroni Cluster Management](/patroni/patroni-management/)\\n- [performance-degradation-troubleshooting](/patroni/performance-degradation-troubleshooting/)\\n- [pg\\_collect\\_query\\_data](/patroni/pg_collect_query_data/)\\n- [Postgresql minor upgrade](/patroni/pg_minor_upgrade/)\\n- [Pg\\_repack using gitlab-pgrepack](/patroni/pg_repack/)\\n- [`pg\\_xid\\_wraparound` Saturation Alert](/patroni/pg_xid_wraparound_alert/)\\n- [`pg\\_txid\\_xmin\\_age` Saturation Alert](/patroni/pg_xid_xmin_age_alert/)\\n- [pg-ext-manager](/patroni/pg-ext-manager/)\\n- [PostgreSQL HA](/patroni/pg-ha/)\\n- [pgbadger Runbook](/patroni/pgbadger_report/)\\n- [Postgresql troubleshooting](/patroni/postgres/)\\n- [postgres\\_exporter](/patroni/postgres_exporter/)\\n- [GitLab application-side reindexing](/patroni/postgres-automatic-reindexing/)\\n- [postgres-backups-verification-failures](/patroni/postgres-backups-verification-failures/)\\n- [postgres-checkup](/patroni/postgres-checkup/)\\n- [Dealing with Data Corruption in PostgreSQL](/patroni/postgres-data-corruption/)\\n- [Diagnosing long running transactions](/patroni/postgres-long-running-transaction/)\\n- [Postgres maintenance](/patroni/postgres-maintenance/)\\n- [Postgresql](/patroni/postgresql/)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       # Direct Answer\\nPatroni is a tool for managing PostgreSQL clusters, providing high availability and automated failover capabilities. It uses a distributed consensus algorithm to manage cluster state and leader election. In PostgreSQL environments, secret detection is crucial for ensuring that sensitive information (like passwords and API keys) is not exposed in logs or configuration files. Patroni can help manage these secrets by integrating with external secret management systems.\\n\\n# Step-by-Step Instructions\\n\\n### 1. Understanding Patroni Cluster Management\\n   - **Overview**: Patroni manages PostgreSQL clusters by using a distributed consensus mechanism (like etcd, Consul, or Zookeeper) to maintain cluster state and leader election.\\n   - **Key Features**:\\n     - Automated failover and recovery.\\n     - Configuration management.\\n     - Integration with external secret management systems for sensitive data.\\n\\n### 2. Setting Up Patroni\\n   - **Prerequisites**:\\n     - A running PostgreSQL instance.\\n     - A distributed key-value store (etcd, Consul, or Zookeeper).\\n     - Access to a secret management system (e.g., HashiCorp Vault).\\n\\n   - **Installation**:\\n     1. Install Patroni on your PostgreSQL nodes.\\n     2. Configure Patroni to connect to your key-value store.\\n     3. Set up PostgreSQL parameters in the Patroni configuration file.\\n\\n### 3. Configuring Secret Management\\n   - **Integrate with Secret Management**:\\n     1. Choose a secret management tool (e.g., HashiCorp Vault).\\n     2. Store sensitive information (like database passwords) in the secret management tool.\\n     3. Update the Patroni configuration to retrieve secrets from the secret management tool.\\n\\n### 4. Monitoring and Alerts\\n   - **Set Up Alerts**:\\n     - Use the alerts provided by Patroni to monitor the health of the cluster.\\n     - Configure alerts for conditions like `PatroniGCSSnapshotDelayed`, `PostgresSplitBrain`, and `PatroniScrapeFailures`.\\n\\n### 5. Regular Maintenance\\n   - **Perform Regular Backups**:\\n     - Use the backup strategies outlined in the runbook, such as WAL-G or pg_basebackup.\\n   - **Monitor Performance**:\\n     - Regularly check for long-running transactions and transaction wraparound issues.\\n\\n# Key Commands\\n- **Start Patroni**:\\n  ```bash\\n  patroni /path/to/patroni.yml\\n  ```\\n\\n- **Check Cluster Status**:\\n  ```bash\\n  patronictl -c /path/to/patroni.yml list\\n  ```\\n\\n- **Trigger Failover**:\\n  ```bash\\n  patronictl -c /path/to/patroni.yml failover\\n  ```\\n\\n- **Retrieve Secrets from Vault** (example):\\n  ```bash\\n  vault kv get -field=password secret/postgres\\n  ```\\n\\n# Important Notes\\n- **Security**: Ensure that your secret management system is properly secured and that access is restricted to authorized users only.\\n- **Backup Strategy**: Regularly test your backup and restore procedures to ensure data integrity and availability.\\n- **Monitoring**: Set up comprehensive monitoring for both Patroni and PostgreSQL to catch issues early and maintain high availability.\\n- **Documentation**: Keep your Patroni and PostgreSQL documentation up to date to facilitate troubleshooting and maintenance. \\n\\nBy following these steps, you can effectively manage a PostgreSQL cluster using Patroni while ensuring that sensitive information is handled securely.                                                                                                                                             Patroni Cluster Management is essential for managing PostgreSQL clusters, ensuring high availability and automated failover. It includes various services such as the Postgres (Patroni) Service and tools for log analysis, performance troubleshooting, and database maintenance. Secret detection, on the other hand, is crucial for identifying and mitigating risks associated with sensitive information leaks in the database. The integration of secret detection within the Patroni management framework helps maintain security by ensuring that sensitive credentials are not exposed in logs or configurations, thereby enhancing the overall security posture of the PostgreSQL environment.         1.000             0.769              0.000           0.500\n",
            "                                 How do you delete cache keys on GitLab.com using Redis, and what steps are involved in obtaining and deleting those keys?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [do not want to conflate this with data deletion, which should ideally be as\\nsimple as possible.\\n(In the future we may move to having [version-based cache invalidation](https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/9815), where\\nthis will be simpler. One example is the way it’s possible to [invalidate the\\nMarkdown cache](https://docs.gitlab.com/ee/administration/invalidate_markdown_cache.html) - which is in Postgres, not Redis - via the API.)\\n## Step 1: obtain the keys to be deleted\\n[Section titled “Step 1: obtain the keys to be deleted”](#step-1-obtain-the-keys-to-be-deleted)\\n**On a secondary**, we can run a Ruby script like the below. `REDIS_PASSWORD` is\\nprovided as an environment variable. In this example, we’re looking for keys of\\nthe form `cache:gitlab:license_key*`, but the general approach will be the same\\nno matter the pattern.\\n```\\nrequire 'redis'\\npattern = \"cache:gitlab:license_key*\"\\noutput = File.open('/tmp/keys-to-delete.txt', 'w')\\nredis = Redis.new(:url => \"redis://#{ENV['REDIS_PASSWORD']}@127.0.0.1\")\\ncursor = '0'\\nloop do\\ncursor, keys = redis.scan(cursor, match: pattern, count: 100000)\\nif keys.count > 0\\nputs \"Writing #{keys.count}\"\\nkeys.each { |key| output.write(key + \"\\n\") }\\nend\\nputs cursor\\nbreak if cursor == '0'\\nend\\n```\\n## Step 2: delete the keys\\n[Section titled “Step 2: delete the keys”](#step-2-delete-the-keys)\\nThis can be run on the console node as `Gitlab::Redis::Cache` will connect to\\nthe primary by default. Make sure to copy `/tmp/keys-to-delete.txt` to the\\nconsole node before starting. It sleeps for a second after every 10 000\\ndeletions. In most cases there should not be many tens of thousands of deletions\\nanyway.\\n```\\nlines = File.readlines('/tmp/keys-to-delete.txt')\\ncount = 0\\nGitlab::Redis::Cache.with do |redis|\\nlines.each do |line|\\nline.rstrip!\\n# Gitlab::Redis::Cache automatically adds the cache:gitlab: namespace, so\\n# we have to remove it. Otherwise we will try to delete keys of the form\\n# cache:gitlab:cache:gitlab:...\\nline.gsub!(/^cache:gitlab:/, '')\\nredis.expire(line, 0)\\ncount += 1\\nif (count % 10000 == 0)\\nputs \"count is #{count}, sleeping...\"\\nsleep 1\\nend\\nend\\nend\\n```\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis-cluster-cache/remove-cache-entries.md)\\n[Previous\\nRedis Cluster Cache Service](/redis-cluster-cache/)   [Next\\nRedis Cluster Chat Cache Service](/redis-cluster-chat-cache/), - workhorse\\n- [Workhorse Service](/workhorse/)\\n- [Workhorse Apdex Degradation](/workhorse/workhorse-apdex-degradation/)\\n- workspaces\\n- [Remote Development Workspaces Service](/workspaces/)\\n- zoekt\\n- [Exact code search Service](/zoekt/)\\n(() => {\\nconst scroller = document.getElementById('starlight\\_\\_sidebar');\\nif (!window.\\_starlightScrollRestore || !scroller) return;\\nscroller.scrollTop = window.\\_starlightScrollRestore;\\ndelete window.\\_starlightScrollRestore;\\n})();\\n[GitLab](https://gitlab.com/gitlab-com/runbooks)[Code Context](https://code-context.runway.gitlab.net/runbooks-docs)\\nSelect theme   DarkLightAuto\\nStarlightThemeProvider.updatePickers();\\nconst a=document.getElementById(\"starlight\\_\\_sidebar\"),n=a?.querySelector(\"sl-sidebar-state-persist\"),o=\"sl-sidebar-state\",i=()=>{let t=[];const e=n?.dataset.hash||\"\";try{const s=sessionStorage.getItem(o),r=JSON.parse(s||\"{}\");Array.isArray(r.open)&&r.hash===e&&(t=r.open)}catch{}return{hash:e,open:t,scroll:a?.scrollTop||0}},c=t=>{try{sessionStorage.setItem(o,JSON.stringify(t))}catch{}},d=()=>c(i()),l=(t,e)=>{const s=i();s.open[e]=t,c(s)};n?.addEventListener(\"click\",t=>{if(!(t.target instanceof Element))return;const e=t.target.closest(\"summary\")?.closest(\"details\");if(!e)return;const s=e.querySelector(\"sl-sidebar-restore\"),r=parseInt(s?.dataset.index||\"\");isNaN(r)||l(!e.open,r)});addEventListener(\"visibilitychange\",()=>{document.visibilityState===\"hidden\"&&d()});addEventListener(\"pageHide\",d);\\nOn this page\\n- [Overview](#_top)\\n- [Step 1: obtain the keys to be deleted](#step-1-obtain-the-keys-to-be-deleted)\\n- [Step 2: delete the keys](#step-2-delete-the-keys)\\n## On this page\\n- [Overview](#_top)\\n- [Step 1: obtain the keys to be deleted](#step-1-obtain-the-keys-to-be-deleted)\\n- [Step 2: delete the keys](#step-2-delete-the-keys)\\n# Removing cache entries from Redis\\nCache invalidation is one of the [hard things](https://martinfowler.com/bliki/TwoHardThings.html). Sometimes we have incidents like\\n[#5478](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/5478) where we end up with invalid data in the cache, and no convenient way to\\nclear it. This document describes an approach for deleting a specified set of\\ncache keys on GitLab.com without consuming excessive Redis resources.\\nThis approach is split into two phases:\\n1. Obtain the keys to be deleted (against a secondary)\\n2. Delete the specified keys (against the primary)\\nBy doing this, we gain a few benefits:\\n1. We perform as few operations on the primary as necessary. Secondaries do not\\nreceive application traffic, and so have more headroom and can tolerate being\\nblocked briefly.\\n2. We can retain a list of deleted keys for later inspection - for instance, if\\na user reports an issue, we can check if their cache key was in the list.\\n3. Obtaining the keys to be deleted may be an iterative, exploratory process. We\\ndo not want to conflate this with data deletion, which should ideally be as\\nsimple as possible.\\n(In the future we may move to having [version-based cache invalidation](https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/9815), where\\nthis will be simpler. One example is the way it’s possible to [invalidate the\\nMarkdown cache](https://docs.gitlab.com/ee/administration/invalidate_markdown_cache.html) - which is in Postgres, not Redis - via the API.)\\n## Step 1: obtain the keys to be deleted\\n[Section titled “Step 1: obtain the keys to be deleted”](#step-1-obtain-the-keys-to-be-deleted)\\n**On a secondary**, we can run a Ruby script like the below. `REDIS_PASSWORD` is\\nprovided as an environment variable. In this example, we’re looking for keys of, You’ll need to convert any non-bytes number into bytes to apply it on the console (i.e. 4gb = 4*1024*1024\\*1024 = 4294967296)\\nThus for a line in the config like this\\n```\\nclient-output-buffer-limit slave 4gb 4gb 0\\n```\\nYou need to execute this:\\n```\\nREDISCLI_AUTH=$(sudo grep ^masterauth /var/opt/gitlab/redis/redis.conf|cut -d\\\" -f2)\\n/opt/gitlab/embedded/bin/redis-cli config set client-output-buffer-limit \"slave 4294967296 4294967296 0\"\\n```\\n### Redis is down\\n[Section titled “Redis is down”](#redis-is-down)\\n#### Start Redis\\n[Section titled “Start Redis”](#start-redis)\\n`gitlab-ctl start redis`\\n### Failed to collect Redis metrics\\n[Section titled “Failed to collect Redis metrics”](#failed-to-collect-redis-metrics)\\n#### Symptoms\\n[Section titled “Symptoms”](#symptoms)\\n- You see alerts like `FailedToCollectRedisMetrics`.\\n- Redis metrics are unavailable\\n#### Possible checks\\n[Section titled “Possible checks”](#possible-checks-1)\\n#### Solution\\n[Section titled “Solution”](#solution)\\nIf everything looks ok, it might be that the instance made a full resync from\\nmaster. During that time the redis\\_exporter fails to collect metrics from\\nredis. Check `/var/log/gitlab/redis/current` for `MASTER <-> SLAVE sync`\\nevents during the time of the alert.\\nIf either of the `redis` or `sentinel` services is down, restart it with\\n`gitlab-ctl restart redis`\\nor\\n`gitlab-ctl restart sentinel`.\\nElse check for possible issues in `/var/log/gitlab/redis/current` (e.g. resync\\nfrom master) and see [redis\\_replication.md].\\n## Miscellaneous\\n[Section titled “Miscellaneous”](#miscellaneous)\\n### BigKeys analysis\\n[Section titled “BigKeys analysis”](#bigkeys-analysis)\\nPer <https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/360> there may be a script that runs periodically (hourly by default) on a redis replica, to collect ‘bigkeys’ output and store it for later analysis.\\nThe bigkeys are stored in a GCS bucket named `gitlab-gprd-redis-analysis` under the `gitlab-production` project.\\nThe frequency can be controlled with the chef attribute `redis_analysis.bigkeys.timer_on_calendar`, being a systemd time spec. You probably do not want to run it more than once an hour (it’s intended for broad-brush data collection, not fine-grained), although other than considering how long it takes to run and avoiding overlap there’s not actual constraint on that.\\nIf it needs to be stopped for some reason (it is running badly, is causing undue load, or other unexpected effects) it can be\\n1. Stopped if currently running, with `sudo systemctl stop redis-bigkeys-extract.service’\\n2. Prevented from running again (until chef next runs) with `sudo systemctl stop redis-bigkeys-extract.timer`\\n3. Turned off by chef by setting the attribute ‘redis\\_analysis.bigkeys.timer\\_enabled` to false, e.g. in a role\\n## References\\n[Section titled “References”](#references)\\n- <https://blog.octo.com/en/what-redis-deployment-do-you-need/>\\n- <https://lzone.de/cheat-sheet/Redis>\\n- <https://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/>\\n- <https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/80>\\n- <https://gitlab.com/gitlab-com/gl-infra/scalability/issues/49>\\n- <https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/7199>\\n- <https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/9414>\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis/redis.md)\\n[Previous\\nProvisioning Redis Cluster](/redis/provisioning-redis-cluster/)   [Next\\nRedis Cluster](/redis/redis-cluster/), 65642 command: get cache:gitlab:flipper/v1/feature/enforced_sso_requires_session\\n(...)\\n```\\n##### keyspace analysis\\n[Section titled “keyspace analysis”](#keyspace-analysis)\\nThe redis trace script parses out flows into a timeline of commands, one line per key. The fields are: timestamp, second offset, command, src host, key pattern, key.\\nIt has some pre-canned key pattern extractions that can be enabled via `GITLAB_REDIS_CLUSTER`. Supported values are: `persistent`, `cache`.\\nThe script can be tweaked or its output further processed with `awk` and friends.\\nTerminal window\\n```\\nfind redis-analysis -name '*.06379.findx' | GITLAB_REDIS_CLUSTER=cache parallel -j0 -n100 ruby runbooks/scripts/redis_trace_cmd.rb | sed '/^$/d' > trace.txt\\ngsort --parallel=8 trace.txt -o trace.txt\\n```\\nFor example, count per key pattern:\\nTerminal window\\n```\\ncat trace.txt | awk '{ print $5 }' | sort -n | uniq -c | sort -nr\\n```\\nIt is also possible to output in JSON format for processing via `jq`:\\nTerminal window\\n```\\nfind redis-analysis -name '*.06379.findx' | GITLAB_REDIS_CLUSTER=cache OUTPUT_FORMAT=json parallel -j0 -n100 ruby runbooks/scripts/redis_trace_cmd.rb | sed '/^$/d' > trace.json\\n```\\nThis allows for a similar count per command and key pattern:\\nTerminal window\\n```\\ncat trace.json | jq -c '[.cmd, .patterns]' | sort | uniq -c | sort -rn | head\\n```\\n##### key size estimation\\n[Section titled “key size estimation”](#key-size-estimation)\\nThe following commands are meant to be run on a replica instance, for example `redis-cache-01-db-gprd`.\\nIn this example, we’re filtering the dump output for `Class:merge_requests`, replace this with your keyname.\\nTerminal window\\n```\\n$ sudo gitlab-redis-cli bgsave\\n# Monitor the file on disk, once it stops increasing in size, it's ready to be used!\\n$ sudo ls -lta /var/opt/gitlab/redis/dump.rdb\\n# Once the file is ready, move it to a safe-er location, for example\\n$ sudo mv /var/opt/gitlab/redis/dump.rdb /var/log/redis-data/\\n$ RDB_FILE_PATH=/var/log/redis-data\\n# build the `dump` binary in your local machine (https://github.com/igorwwwwwwwwwwwwwwwwwwww/rdb/tree/version-9)\\n###  $ git clone https://github.com/igorwwwwwwwwwwwwwwwwwwww/rdb\\n###  $ cd rdb\\n###  $ git checkout version-9\\n###  $ GOOS=linux GOARCH=amd64 go build ./cmd/dump\\n###  $ scp dump redis-cache-01-db-gprd:\\n# Now we'll use the `dump` binary to analyze `dump.rdb`\\n$ sudo ./dump $RDB_FILE_PATH/dump.rdb | awk -F'\\t' '$1 ~ /Class:merge_requests/ { sum1 += $3; sum2 += $4 } END { print sum1, sum2 }'\\n# The two values you get from this represent estimates in bytes used for values and keys+values respectively.\\n6039116565 6549803309\\n# Convert to GiB\\n$ echo $((6039116565.0/(1024.0**3.0)))\\n5.6243655877187848\\n```\\nThe values presented are an optimistic estimate, as redis will require some more memory for its datastructures. Generally, the key size will be on that order of magnitude.\\nThe current `maxmemory` in Redis-cache is set to `60 GiB`. Depending on the numbers you get, the ratio of each compared to the `maxmemory` can give you an idea of how significant of an impact your change might introduce.\\n**Please remember to delete the RDB file once you’re done!**\\nTerminal window\\n```\\nrm $RDB_FILE_PATH/dump.rdb\\n```\\n#### Please remember to delete the `pcap` file immediately after performing the analysis\\n[Section titled “Please remember to delete the pcap file immediately after performing the analysis”](#please-remember-to-delete-the-pcap-file-immediately-after-performing-the-analysis)\\n### CPU profiling\\n[Section titled “CPU profiling”](#cpu-profiling), Note that when rotating secrets (eg having v1 in `gstg` and v2 in `gprd`), follow a safe and controlled rollout as described [here](https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/vault/usage.md?ref_type=heads#rotating-kubernetes-secrets).\\nb. Update Gitlab Rails `.Values.global.redis` accordingly.\\nEither add a new key to `.Values.global.redis.<RAILS_INSTANCE_NAME>` or `.Values.global.redis.redisYmlOverride.<RAILS_INSTANCE_NAME>`. An example MR can be found [here](https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/2753).\\n### 4. Troubleshooting\\n[Section titled “4. Troubleshooting”](#4-troubleshooting)\\n#### No metrics on dashboard\\n[Section titled “No metrics on dashboard”](#no-metrics-on-dashboard)\\nThis was encountered when provisioning the [production instance of redis-cluster-chat-cache](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2358#note_1406105979). To resolve this, run chef-repo on Prometheus with:\\n```\\nknife ssh roles:gprd-infra-prometheus-server \"sudo chef-client\"\\n```\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis/provisioning-redis-cluster.md)\\n[Previous\\nMemory space analysis with cupcake-rdb](/redis/memory-space-analysis-cupcake-rdb/)   [Next\\nTroubleshooting](/redis/redis/)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          # Direct Answer\\nTo delete cache keys on GitLab.com using Redis, you need to follow a two-step process: first, obtain the keys you want to delete, and then delete those keys. This process involves running Ruby scripts on a secondary Redis instance to gather the keys and then executing deletion commands on the primary Redis instance.\\n\\n# Step-by-Step Instructions\\n\\n## Step 1: Obtain the Keys to be Deleted\\n1. **Access a Secondary Redis Instance**: Ensure you have access to a secondary Redis instance where you can run the Ruby script.\\n   \\n2. **Set the Environment Variable**: Make sure the `REDIS_PASSWORD` environment variable is set with the password for your Redis instance.\\n\\n3. **Run the Ruby Script**: Use the following Ruby script to find and list the keys you want to delete. In this example, we are looking for keys matching the pattern `cache:gitlab:license_key*`.\\n\\n   ```ruby\\n   require 'redis'\\n   pattern = \"cache:gitlab:license_key*\"\\n   output = File.open('/tmp/keys-to-delete.txt', 'w')\\n   redis = Redis.new(:url => \"redis://#{ENV['REDIS_PASSWORD']}@127.0.0.1\")\\n   cursor = '0'\\n   loop do\\n     cursor, keys = redis.scan(cursor, match: pattern, count: 100000)\\n     if keys.count > 0\\n       puts \"Writing #{keys.count}\"\\n       keys.each { |key| output.write(key + \"\\n\") }\\n     end\\n     puts cursor\\n     break if cursor == '0'\\n   end\\n   ```\\n\\n4. **Check the Output**: After running the script, verify that the file `/tmp/keys-to-delete.txt` contains the keys you want to delete.\\n\\n## Step 2: Delete the Keys\\n1. **Copy the Key List**: Ensure that the `/tmp/keys-to-delete.txt` file is copied to the console node where you will execute the deletion.\\n\\n2. **Run the Deletion Script**: Execute the following Ruby script on the console node to delete the keys listed in the file. This script connects to the primary Redis instance.\\n\\n   ```ruby\\n   lines = File.readlines('/tmp/keys-to-delete.txt')\\n   count = 0\\n   Gitlab::Redis::Cache.with do |redis|\\n     lines.each do |line|\\n       line.rstrip!\\n       # Remove the namespace prefix\\n       line.gsub!(/^cache:gitlab:/, '')\\n       redis.expire(line, 0)\\n       count += 1\\n       if (count % 10000 == 0)\\n         puts \"count is #{count}, sleeping...\"\\n         sleep 1\\n       end\\n     end\\n   end\\n   ```\\n\\n3. **Monitor the Deletion Process**: The script will output the count of deleted keys and will pause for a second after every 10,000 deletions to avoid overwhelming the Redis instance.\\n\\n# Key Commands\\n- **Ruby Script to Obtain Keys**: \\n   ```ruby\\n   require 'redis'\\n   # (Script as shown in Step 1)\\n   ```\\n\\n- **Ruby Script to Delete Keys**:\\n   ```ruby\\n   lines = File.readlines('/tmp/keys-to-delete.txt')\\n   # (Script as shown in Step 2)\\n   ```\\n\\n# Important Notes\\n- **Environment Setup**: Ensure that the `REDIS_PASSWORD` environment variable is correctly set before running the scripts.\\n- **Key Patterns**: Modify the `pattern` variable in the first script to match the specific keys you want to delete.\\n- **Resource Management**: The deletion script is designed to manage Redis resources effectively by pausing after a set number of deletions.\\n- **Backup**: Consider backing up any important data before performing deletions, as this process is irreversible.\\n- **Testing**: If possible, test the scripts in a staging environment before running them in production to ensure they work as expected. To delete cache keys on GitLab.com using Redis, the process is divided into two main steps. First, you need to obtain the keys to be deleted, which can be done on a secondary Redis instance. You can run a Ruby script that scans for keys matching a specific pattern, such as 'cache:gitlab:license_key*', and writes them to a temporary file. The script uses the Redis 'scan' command to iteratively find keys without overwhelming the server. Second, once you have the list of keys, you can delete them from the primary Redis instance. This is done by reading the keys from the temporary file and using the 'expire' command to remove them. It's important to ensure that the keys are correctly formatted, as the Gitlab::Redis::Cache automatically adds the 'cache:gitlab:' namespace, which needs to be stripped before deletion.         0.986             0.963              0.917           1.000\n",
            "How does the RDB format affect data storage in Redis, especially in relation to the 15-minute write interval and potential data loss during node failures? [- *Large requests:* Some Redis operations can potentially be much more expensive than most (e.g. adding many elements\\nto a large set via a single atomic `SADD` request). Activity at higher layers in the application stack can\\nsometimes increase the frequency of these 10+ millisecond operations, implicitly causing response time spikes\\nfor other clients that have to wait in the queue.\\n### Network partition\\n[Section titled “Network partition”](#network-partition)\\nFor the clusters with sentinel on the VM alongside redis, a simple network partition results in 2 nodes thinking the 3rd\\nis down, and the 3rd node thinking the other 2 are down.\\nIf the primary is in the 2-node side, no failover will occur on Redis; when the partition resolves, the replica that was\\nalone will resynchronize replication.\\nIf the primary is on the 1-node side, then Sentinel on the two-node side will retain quorum and initiate a failover\\nto one of the nodes on the 2-node side. The old primary will *continue* to be a primary for any clients that can talk\\nto it, but when the network partition heals it will be demoted to a replica and any writes to it during the partition\\nwill be lost. See <https://redis.io/topics/sentinel#example-2-basic-setup-with-three-boxes>, in particular\\n`min-replicas-to-write` for more depth on this and the related tradeoffs\\nA 3-way partition will result in no changes; no sentinel will have quorum to force a failover, one primary will remain\\nand any clients that cannot get to the primary will be inoperable.\\nFor the cache cluster it depends on the nature of the network partition. At core it is based on what nodes the\\nsentinels can see are up, both between themselves for determining a quorum to make a failover decision, and which redis\\nnodes can be contacted from the sentinels for selecting a primary. In the most “likely” case, where connectivity between AZs goes down, it is\\nequivalent to the behavior of the persistent clusters. A partition between only the sentinels will have no effect.\\nA partition between only the redis nodes will break replication to the separated replica(s), but otherwise have no effect on\\nthe operability of the cluster. A partition between sentinels and redis nodes will cause a failover if the loss of\\ncommunication is between 2 sentinels (that can thus maintain authoritative quorum) and the current primary, as long as\\nthe sentinels can still talk to a replica. Consideration of more complicated failure scenarios is left as an exercise\\nfor the reader.\\n### Persistence Guarantees\\n[Section titled “Persistence Guarantees”](#persistence-guarantees)\\nRedis replication is asynchronous (barring the explicit use of the [wait](https://redis.io/commands/wait) command by the\\napplication, which we do not currently use). This means any uncontrolled failover can result in the loss of some writes\\naccepted by the lost primary. This is of very little concern for the cache cluster, but could be lightly problematic for\\nthe others. For sidekiq it likely means we’ll not run jobs that should be run, or that some jobs will run twice. For\\nthe persistent cluster, results may vary (it will be deeply dependent on the specific key and how the application behaves).\\nFor tracechunks we may lose the output of some CI jobs.\\nOn the non-cache clusters, the data is saved to disk (RDB format) every 900 seconds (15 minutes) as long as at least\\n1 key has changed. In the event that all 3 nodes fail at once and the in-memory contents is lost, we may lose up to\\n15 minutes of writes.\\nAs noted elsewhere in this document, the cache clusters do not regularly write to disk (only indirectly as part of a\\nreplication resynchronization). If all 3 nodes fail at once, the entire cache will be wiped; this is not ideal but\\nacceptable as it will be refilled on demand. For ratelimiting, the data has only short term usefulness anyway (generally\\n1 minute, with some up to 3 minutes) so complete loss is not significant.\\n## What do we store?\\n[Section titled “What do we store?”](#what-do-we-store)\\n<https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/419> contains some summary analysis of the keyspace across\\nthe persistent and cache instances, as at July 2020. This will change over time as the code base evolves, but the link\\nprovides some indication what we’re storing (it’s not as wide a range of things as you might expect in the, Redis is in the usual 3-node single-primary/two-replica VM configuration, with sentinels on the same VMs as Redis (as for persistent and sidekiq clusters)\\nSingle threaded CPU is the normal constraint for Redis but due to the large data transfers networking *may* become a constraint before CPU does on this cluster, although there is a *lot* of headroom (2 orders of magnitude) at this writing (2021) and scaling up the instances is possible to obtain more (or maybe the limits will be higher by then)\\nThe usual persistent redis availability expectations for our 3-node clusters apply (see <https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis-survival-guide-for-sres.md#high-availability> )\\nData is expected to be largely persistent; some data loss may occur during an unplanned failover event, but in all normal operations all data in this instance is expected to be fully durable.\\nUses mostly the same metrics-catalog definitions as for the other Redis clusters, except for memory saturation which is deliberately set very low; the transient nature of the data here means any build up is abnormal and should be investigated early while it is easier to recover from.\\n- Implementation/migration epic: <https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/462>\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis-tracechunks/README.md)\\n[Previous\\nRedis Sidekiq Service](/redis-sidekiq/)   [Next\\nContainerRegistryDBLoadBalancerReplicaPoolSize](/registry/alerts/ContainerRegistryDBLoadBalancerReplicaPoolSize/), The split is *largely* a form of functional partitioning for scalability (see single threaded comments above), but also\\nbecause the application expects the non-cache instances to persist data across failures or restarts so those instances\\nmust write data to disk periodically (in addition to replication/failover capability). It’s not a full DBMS with\\nguaranteed write semantics (and performance implications to match), but it is sufficiently persistent that there wouldn’t\\nbe enormous implications if we did have a substantial failure and had to go back to the RDB files on disk. However,\\nhaving this level of persistence for our large cache (currently ~60GB of cache) would be expensive in computation and I/O,\\nfor insufficient benefit. While we don’t want to lose our cache regularly, we can certainly survive an occasional loss\\nin unlikely circumstances (all 3 nodes die at once, which would probably mean all or much of the rest of our\\ninfrastructure is also down or badly affected).\\nAt the time (mid 2021) we chose to split CI build trace chunks into it’s own instance, CI trace chunks were responsible for roughly 60% of the\\ndata throughput into/out of the shared state redis, and 16% of Redis calls (see <https://gitlab.com/gitlab-org/gitlab/-/issues/327469#note_556531587>)\\nwhich was sufficient reason for the split, along with the distinctive usage profile (transient data on its way to permanent storage).\\nWhen we split out Ratelimiting (latter-half of 2021) this was for CPU saturation; the cache instance was peaking at a little over\\n90% CPU, and we knew from when we [enabled RackAttack in November 2020](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3034)\\nthat it is responsible for at least 25% (absolute) of the CPU utilization, so splitting this out gives the cache instance room\\nto breath. Note that the [data usage](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1246#sizing) is tiny.\\nWhen we split out Sessions (very late 2021), this was for CPU saturation (antipicated something like 20% absolute savings)\\nand also to separate a workload that may be problematic for Redis Cluster, unblocking perhaps moving the persistent Redis\\nto Cluster in the future.\\n### CPUs\\n[Section titled “CPUs”](#cpus)\\nRedis VMs were the first nodes we switched to from N1 to ‘[C2](https://cloud.google.com/compute/docs/machine-types#c2_machine_types)’\\nnode types for the best raw single-threaded CPU performance. This [halved](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/230#note_312403063)\\nthe CPU usage on our sidekiq cluster, and [almost the same](https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/9636)\\non the cache cluster. Just in case you were in any doubt as to how important the single-threaded CPU performance was\\nto redis.\\nRedis 6 has [multithreaded I/O](https://github.com/redis/redis/pull/6038/files) which helps by moving\\nsome network I/O work to non-core threads, but the core work must still occur on the main thread, so it is only a\\nmitigation.\\n## High Availability\\n[Section titled “High Availability”](#high-availability)\\nFor each cluster we run 3 nodes, using [Redis Sentinel](https://redis.io/topics/sentinel) to manage failover. All\\ntraffic goes through the currently active primary, and writes are replicated to the two replicas asynchronously. If the\\nprimary fails (as determined by Sentinel), one of the replicas is promoted to primary.\\nClients are configured to connect to the sentinels which will then direct them to the current primary; there is no load\\nbalancer involved in this path. Failover is automatic (handled by Sentinel) and results in clients being disconnected\\nfrom the primary and then reconnecting via the sentinels again. It requires no operator intervention under normal circumstances\\nThe configuration is subtly different across the clusters, for historical reasons; the persistent and sidekiq clusters\\nhave sentinel running on the VMs alongside redis, whereas the cache cluster uses a distinct set of sentinel VMs.\\n<https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/11389> records the desire to clean this up.\\n### Node failure\\n[Section titled “Node failure”](#node-failure), - [Network partition](#network-partition)\\n- [Persistence Guarantees](#persistence-guarantees)\\n- [What do we store?](#what-do-we-store)\\n- [Clients](#clients)\\n- [What about redis cluster?](#what-about-redis-cluster)\\n- [Maintanence](#maintanence)\\n- [Reconfigure](#reconfigure)\\n- [Debugging and Diagnosis](#debugging-and-diagnosis)\\n- [Identifying the primary](#identifying-the-primary)\\n- [Basic stats](#basic-stats)\\n- [Slowlogs](#slowlogs)\\n- [ElasticSearch analysis](#elasticsearch-analysis)\\n- [Monitor commands](#monitor-commands)\\n- [Flamegraphs](#flamegraphs)\\n- [Bigkeys](#bigkeys)\\n# A survival guide for SREs to working with Redis at GitLab\\nSee also <https://docs.gitlab.com/ee/development/redis.html> which covers some of the same\\nground, but with a developer orientation and the SRE-oriented [runbook](https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis.md)\\n## What is Redis\\n[Section titled “What is Redis”](#what-is-redis)\\nRedis is a fast in-memory key-value store. It offers a number of data types, from simple strings to sets, hashes,\\nor complex data types like HyperLogLog. It is fundamentally single-threaded in its core loop, which keeps the\\nimplementation simple and robust (e.g. tasks execute serially with no lock contention). However, that means it is\\nconstrained to the performance of a single CPU, and any slow task delays the start of other queued tasks. We have had\\nscaling problems in the past on GitLab.com where the single CPU became the bottleneck, and our architectural design\\nhas evolved to take that into account.\\n## Why do we use it?\\n[Section titled “Why do we use it?”](#why-do-we-use-it)\\nWe use it both as a cache (for performance savings) and to store various types of persistent data, either directly and\\nexplicitly (typically from Rails), or implicitly by using Sidekiq or `Rails.cache`\\nCaching for performance is primarily to reduce the load on other systems, particularly Postgres and Gitaly which are often\\nbottlenecks (or single points of failure) This can also save time in the web tier (e.g. rendering Markdown), but more\\nas a side-effect than the direct reason.\\n## Architecture\\n[Section titled “Architecture”](#architecture)\\nFor gitlab.com, as at September 2023, we have 11 sets of Redis instances, each handling a distinct use case:\\nRoleNodesClientsSentinel?Persistence?Cache for `Rails.cache`redis-cluster-cache-XX-YYPuma workers, Sidekiq workersNoNoneCache for Chat ([To be decommissioned](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/19221))redis-cluster-chat-cache-XX-YYPuma workersNoNoneCache for Feature flagredis-cluster-feature-flag-XX-YYPuma workersNoNoneCache for Repository dataredis-repository-cache-XXPuma workersYes (localhost)NoneSidekiq job queuesredis-sidekiq-XXPuma workers, Sidekiq workersYes (localhost)RDB dump every 900 secondsPersistent shared stateredis-XXPuma workers, Sidekiq workers, WorkhorseYes (localhost)RDB dump every 900 secondsCI build trace chunksredis-tracechunks-XXPuma workers (API), Sidekiq workersYes (localhost)RDB dump every 900 secondsRatelimiting (RackAttack/App)redis-cluster-ratelimiting-XX-YYPuma workersNoNoneSessionsredis-sessions-XXPuma workersYes (localhost)RDB dump every 900 seconds, but also an eviction policyLatest WAL pointerredis-db-load-balancing-XXPuma workers, Sidekiq workersYes (localhost)RDB dump every 900 seconds, but also an eviction policyPub/Subredis-pubsub-XXPuma workers, WorkhorseYes (localhost)No\\nThe split is *largely* a form of functional partitioning for scalability (see single threaded comments above), but also\\nbecause the application expects the non-cache instances to persist data across failures or restarts so those instances\\nmust write data to disk periodically (in addition to replication/failover capability). It’s not a full DBMS with\\nguaranteed write semantics (and performance implications to match), but it is sufficiently persistent that there wouldn’t, manage the data/expiry (no client activity required; old data just ages out)\\nIt is *not* a service to rate limit requests to Redis itself.\\nThis Redis instance was split off from `redis-cache` because as found in <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3034#note_460538394> RackAttack\\nalone accounted for somewhere in the order of 25% (absolute) CPU usage on the redis-cache instance in Nov 2020, and that will\\nonly have grown with traffic.\\nAs the cache cluster approached 95% saturation it was determined the best short-term approach was to split out the rate-limiting\\ndata storage to its own cluster before we go further with more horizontal scalability.\\nThe `redis-cluster-ratelimiting` instance is the horizontally scalable\\ncounterpart of `redis-ratelimiting` whichs runs in Sentinel mode. More\\ninformation on efforts to horizontally scale Redis instances can be found [here](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/823).\\n## Architecture\\n[Section titled “Architecture”](#architecture)\\nRedis is in a 3-node-per-shard (single-primary/two-replica per shard) VM configuration, in\\ncluster mode. The node count will increase as we scale horizontally.\\n## Scalability\\n[Section titled “Scalability”](#scalability)\\nSingle threaded CPU is the normal constraint for Redis, and will in particular be the case here. It is CPU heavy, and *not* data\\nheavy. Analysis suggests data volumes are in the order of MBs, not GBs, and small numbers at that.\\nRedis Cluster is horizontally scalable as decsribed in the\\n[specifications](https://redis.io/docs/reference/cluster-spec/).\\n## Availability\\n[Section titled “Availability”](#availability)\\nThe usual redis availability expectations for our 3-node-per-shard clusters apply (see <https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis-survival-guide-for-sres.md#high-availability>)\\nFor Redis instances in cluster mode, each shard is made of 1 master node and at\\nleast 2 replica nodes, similar to a Redis sentinel setup.\\nThe availability would depend on the configuration for the Redis instance:\\n- `cluster-require-full-coverage`: If this is set to yes, as it is by default,\\nthe cluster stops accepting writes if some percentage of the key space is not\\ncovered by any node.\\n- `cluster-allow-reads-when-down`: If this is set to no, as it is by default,\\na node in a Redis Cluster will stop serving all traffic when the cluster is\\nmarked as failed, either when a node can’t reach a quorum of masters or when\\nfull coverage is not met.\\nMore information can be found\\n[here](https://redis.io/docs/management/scaling/).\\n## Durability\\n[Section titled “Durability”](#durability)\\nUnimportant; data has a (almost exclusively) 1 minute useful period. If we lost all data in this cluster, within 1 minute (at most) all effects would be passed, and at worst during that 1 minute some users might be able to access the system at up to double the usual rate-limit (an absolute upper limit, depending on which part of the minute the failure happened and the distribution of their requests during that period).\\n## Monitoring/Alerting\\n[Section titled “Monitoring/Alerting”](#monitoringalerting)\\nUses the same metrics-catalog definitions as for the other Redis clusters.\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis-cluster-ratelimiting/README.md)\\n[Previous\\nRedis Cluster Queues Meta Service](/redis-cluster-queues-meta/)   [Next\\nRedis Cluster Registry Service](/redis-cluster-registry/)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           # Redis RDB Format and Data Storage\\n\\n## 1. Direct Answer\\nThe RDB (Redis Database) format affects data storage in Redis by providing a snapshot of the dataset at specified intervals, typically every 15 minutes (900 seconds), as long as at least one key has changed. This means that in the event of a node failure, any data written in the last 15 minutes may be lost if the primary node fails before the next RDB snapshot is created. Therefore, while Redis offers persistence through RDB, it does not guarantee zero data loss during unexpected failures.\\n\\n## 2. Step-by-Step Instructions\\nTo understand how the RDB format impacts data storage and to mitigate potential data loss, follow these steps:\\n\\n### Step 1: Understand RDB Persistence\\n- Redis saves data to disk in RDB format every 900 seconds if at least one key has changed.\\n- If all nodes fail before the next snapshot, you may lose up to 15 minutes of writes.\\n\\n### Step 2: Monitor Write Operations\\n- Regularly monitor the write operations to ensure that data is being modified frequently enough to trigger RDB snapshots.\\n- Use Redis commands like `INFO persistence` to check the last save time and the number of keys saved.\\n\\n### Step 3: Configure Sentinel for High Availability\\n- Ensure that Redis Sentinel is configured to manage failover. This allows replicas to take over if the primary fails.\\n- Check the configuration for `min-replicas-to-write` to ensure that writes are only accepted when a certain number of replicas are available.\\n\\n### Step 4: Implement Regular Backups\\n- Schedule regular backups of the RDB files to a secure location to prevent data loss in case of catastrophic failures.\\n- Use a cron job or similar scheduling tool to automate the backup process.\\n\\n### Step 5: Test Failover Scenarios\\n- Simulate node failures to test the failover process and ensure that data integrity is maintained.\\n- Use commands like `SENTINEL failover <master-name>` to manually trigger a failover and observe the behavior.\\n\\n## 3. Key Commands\\n- **Check RDB Persistence Status:**\\n  ```bash\\n  INFO persistence\\n  ```\\n- **Trigger Manual Failover:**\\n  ```bash\\n  SENTINEL failover <master-name>\\n  ```\\n- **Backup RDB File:**\\n  ```bash\\n  cp /var/lib/redis/dump.rdb /path/to/backup/location/\\n  ```\\n\\n## 4. Important Notes\\n- **Data Loss Risk:** Be aware that if the primary node fails and no RDB snapshot has been taken in the last 15 minutes, any writes during that period will be lost.\\n- **Replication Lag:** Since replication is asynchronous, there may be a delay in data being written to replicas, which can also lead to data loss during failover.\\n- **Monitoring Tools:** Utilize monitoring tools to keep track of Redis performance and persistence metrics to proactively manage potential issues.\\n- **Configuration Review:** Regularly review and update your Redis and Sentinel configurations to align with best practices for high availability and data durability.\\n\\nBy following these guidelines, you can effectively manage the implications of the RDB format on data storage in Redis and minimize the risk of data loss during node failures.                                                                                                                                                                                                                                                                                                      The RDB format in Redis affects data storage by saving data to disk every 900 seconds (15 minutes) as long as at least one key has changed. This means that if all three nodes fail at once, there is a risk of losing up to 15 minutes of writes. In contrast, the cache clusters do not regularly write to disk, which means that if all nodes fail simultaneously, the entire cache will be wiped. This is considered acceptable as the cache will be refilled on demand, but it highlights the transient nature of the data stored in Redis.         0.571             0.839              1.000           1.000\n",
            "                           How does the use of Sidekiq in GitLab's architecture impact the persistence guarantees and high availability of Redis clusters?                                                                                                                       [- [Network partition](#network-partition)\\n- [Persistence Guarantees](#persistence-guarantees)\\n- [What do we store?](#what-do-we-store)\\n- [Clients](#clients)\\n- [What about redis cluster?](#what-about-redis-cluster)\\n- [Maintanence](#maintanence)\\n- [Reconfigure](#reconfigure)\\n- [Debugging and Diagnosis](#debugging-and-diagnosis)\\n- [Identifying the primary](#identifying-the-primary)\\n- [Basic stats](#basic-stats)\\n- [Slowlogs](#slowlogs)\\n- [ElasticSearch analysis](#elasticsearch-analysis)\\n- [Monitor commands](#monitor-commands)\\n- [Flamegraphs](#flamegraphs)\\n- [Bigkeys](#bigkeys)\\n# A survival guide for SREs to working with Redis at GitLab\\nSee also <https://docs.gitlab.com/ee/development/redis.html> which covers some of the same\\nground, but with a developer orientation and the SRE-oriented [runbook](https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis.md)\\n## What is Redis\\n[Section titled “What is Redis”](#what-is-redis)\\nRedis is a fast in-memory key-value store. It offers a number of data types, from simple strings to sets, hashes,\\nor complex data types like HyperLogLog. It is fundamentally single-threaded in its core loop, which keeps the\\nimplementation simple and robust (e.g. tasks execute serially with no lock contention). However, that means it is\\nconstrained to the performance of a single CPU, and any slow task delays the start of other queued tasks. We have had\\nscaling problems in the past on GitLab.com where the single CPU became the bottleneck, and our architectural design\\nhas evolved to take that into account.\\n## Why do we use it?\\n[Section titled “Why do we use it?”](#why-do-we-use-it)\\nWe use it both as a cache (for performance savings) and to store various types of persistent data, either directly and\\nexplicitly (typically from Rails), or implicitly by using Sidekiq or `Rails.cache`\\nCaching for performance is primarily to reduce the load on other systems, particularly Postgres and Gitaly which are often\\nbottlenecks (or single points of failure) This can also save time in the web tier (e.g. rendering Markdown), but more\\nas a side-effect than the direct reason.\\n## Architecture\\n[Section titled “Architecture”](#architecture)\\nFor gitlab.com, as at September 2023, we have 11 sets of Redis instances, each handling a distinct use case:\\nRoleNodesClientsSentinel?Persistence?Cache for `Rails.cache`redis-cluster-cache-XX-YYPuma workers, Sidekiq workersNoNoneCache for Chat ([To be decommissioned](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/19221))redis-cluster-chat-cache-XX-YYPuma workersNoNoneCache for Feature flagredis-cluster-feature-flag-XX-YYPuma workersNoNoneCache for Repository dataredis-repository-cache-XXPuma workersYes (localhost)NoneSidekiq job queuesredis-sidekiq-XXPuma workers, Sidekiq workersYes (localhost)RDB dump every 900 secondsPersistent shared stateredis-XXPuma workers, Sidekiq workers, WorkhorseYes (localhost)RDB dump every 900 secondsCI build trace chunksredis-tracechunks-XXPuma workers (API), Sidekiq workersYes (localhost)RDB dump every 900 secondsRatelimiting (RackAttack/App)redis-cluster-ratelimiting-XX-YYPuma workersNoNoneSessionsredis-sessions-XXPuma workersYes (localhost)RDB dump every 900 seconds, but also an eviction policyLatest WAL pointerredis-db-load-balancing-XXPuma workers, Sidekiq workersYes (localhost)RDB dump every 900 seconds, but also an eviction policyPub/Subredis-pubsub-XXPuma workers, WorkhorseYes (localhost)No\\nThe split is *largely* a form of functional partitioning for scalability (see single threaded comments above), but also\\nbecause the application expects the non-cache instances to persist data across failures or restarts so those instances\\nmust write data to disk periodically (in addition to replication/failover capability). It’s not a full DBMS with\\nguaranteed write semantics (and performance implications to match), but it is sufficiently persistent that there wouldn’t, Redis is in the usual 3-node single-primary/two-replica VM configuration, with sentinels on the same VMs as Redis (as for persistent and sidekiq clusters)\\nSingle threaded CPU is the normal constraint for Redis but due to the large data transfers networking *may* become a constraint before CPU does on this cluster, although there is a *lot* of headroom (2 orders of magnitude) at this writing (2021) and scaling up the instances is possible to obtain more (or maybe the limits will be higher by then)\\nThe usual persistent redis availability expectations for our 3-node clusters apply (see <https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis-survival-guide-for-sres.md#high-availability> )\\nData is expected to be largely persistent; some data loss may occur during an unplanned failover event, but in all normal operations all data in this instance is expected to be fully durable.\\nUses mostly the same metrics-catalog definitions as for the other Redis clusters, except for memory saturation which is deliberately set very low; the transient nature of the data here means any build up is abnormal and should be investigated early while it is easier to recover from.\\n- Implementation/migration epic: <https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/462>\\n[Edit page](https://gitlab.com/gitlab-com/runbooks/-/edit/master/docs/redis-tracechunks/README.md)\\n[Previous\\nRedis Sidekiq Service](/redis-sidekiq/)   [Next\\nContainerRegistryDBLoadBalancerReplicaPoolSize](/registry/alerts/ContainerRegistryDBLoadBalancerReplicaPoolSize/), replication resynchronization). If all 3 nodes fail at once, the entire cache will be wiped; this is not ideal but\\nacceptable as it will be refilled on demand. For ratelimiting, the data has only short term usefulness anyway (generally\\n1 minute, with some up to 3 minutes) so complete loss is not significant.\\n## What do we store?\\n[Section titled “What do we store?”](#what-do-we-store)\\n<https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/419> contains some summary analysis of the keyspace across\\nthe persistent and cache instances, as at July 2020. This will change over time as the code base evolves, but the link\\nprovides some indication what we’re storing (it’s not as wide a range of things as you might expect in the\\npersistent instance).\\nThe data stored in the sidekiq instances is not really under our control (it’s whatever sidekiq needs to do its job).\\nSome details on that are available in [Sidekiq Survival Guide for SREs](../../sidekiq/sidekiq-survival-guide-for-sres).\\nThe data in the tracechunks instance is exclusively the output of CI jobs on its way into Object Storage (received by\\nand API request, written into Redis, and a Sidekiq job immediately scheduled to asynchronously write it to Object Storage);\\nwhile important, it is also extremely transient, and under normal circumstances there should be no significant build up\\nof data in this instance. Any such build up implies a problem (Sidekiq not processing, perhaps?) and so we have\\nparticularly low alert thresholds for memory saturation on this instance. At the time we chose to split this instance\\nfrom the primary persistent (shared state) instance (mid 2021), CI trace chunks were responsible for roughly 60% of the\\ndata throughput into/out of the shared state redis, and 16% of Redis calls.\\nThe data in the ratelimiting instance is a set of keys identifying actors (typically users, IP addresses, or in some\\ncases projects) and activities, with the value being the count of usage in the current period (the period is also encoded\\ninto the key name). TTLs are used to expire these automatically so there’s no manual cleanup by clients, it is automatic\\nand internal to Redis.\\nThe data in the sessions instance is exclusively that related to web sessions.\\n## Clients\\n[Section titled “Clients”](#clients)\\nMost of the clients are Ruby, specifically Rails code on the web, api, git, and sidekiq nodes. Workhorse (in Go) also\\n[uses](https://gitlab.com/gitlab-org/gitlab-workhorse/blob/master/README.md#redis) Redis to do long polling for CI build requests.\\nRails uses the generic [connection\\_pool](https://github.com/mperham/connection_pool) to maintain long running connections\\nwhich are used by threads/workers as necessary, which is particularly important as redis is typically expected to be\\nvery fast and low latency.\\nSee [High Availability](#high-availability) for some comments regarding connections via sentinels.\\n## What about redis cluster?\\n[Section titled “What about redis cluster?”](#what-about-redis-cluster)\\n[Redis Cluster](https://redis.io/topics/cluster-spec) is a Redis feature that lets you horizontally shard data across\\nmultiple machines. If any one of our Redis clusters grows too large to fit in memory on a single machine or such that\\nthe single-threaded CPU becomes a hard limit, then Redis Cluster is a possible mitigation. Manually sharding the data\\nis an alternative approach, e.g. having more than one cache cluster and dividing the types of keys up in some way using\\napplication controlled logic, or splitting the sidekiq processing onto multiple clusters. Neither option is low effort.\\nSee the [Redis Cluster documentation for more details](../../redis/redis-cluster).\\nSee:\\n1. <https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/9788>\\n2. <https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/305>\\n## Maintanence\\n[Section titled “Maintanence”](#maintanence)\\n### Reconfigure\\n[Section titled “Reconfigure”](#reconfigure)\\nIn order to apply config changes, we need to run a `sudo gitlab-ctl reconfigure`. However, this operation requires coordination, since it will usually restart the redis process., The split is *largely* a form of functional partitioning for scalability (see single threaded comments above), but also\\nbecause the application expects the non-cache instances to persist data across failures or restarts so those instances\\nmust write data to disk periodically (in addition to replication/failover capability). It’s not a full DBMS with\\nguaranteed write semantics (and performance implications to match), but it is sufficiently persistent that there wouldn’t\\nbe enormous implications if we did have a substantial failure and had to go back to the RDB files on disk. However,\\nhaving this level of persistence for our large cache (currently ~60GB of cache) would be expensive in computation and I/O,\\nfor insufficient benefit. While we don’t want to lose our cache regularly, we can certainly survive an occasional loss\\nin unlikely circumstances (all 3 nodes die at once, which would probably mean all or much of the rest of our\\ninfrastructure is also down or badly affected).\\nAt the time (mid 2021) we chose to split CI build trace chunks into it’s own instance, CI trace chunks were responsible for roughly 60% of the\\ndata throughput into/out of the shared state redis, and 16% of Redis calls (see <https://gitlab.com/gitlab-org/gitlab/-/issues/327469#note_556531587>)\\nwhich was sufficient reason for the split, along with the distinctive usage profile (transient data on its way to permanent storage).\\nWhen we split out Ratelimiting (latter-half of 2021) this was for CPU saturation; the cache instance was peaking at a little over\\n90% CPU, and we knew from when we [enabled RackAttack in November 2020](https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3034)\\nthat it is responsible for at least 25% (absolute) of the CPU utilization, so splitting this out gives the cache instance room\\nto breath. Note that the [data usage](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1246#sizing) is tiny.\\nWhen we split out Sessions (very late 2021), this was for CPU saturation (antipicated something like 20% absolute savings)\\nand also to separate a workload that may be problematic for Redis Cluster, unblocking perhaps moving the persistent Redis\\nto Cluster in the future.\\n### CPUs\\n[Section titled “CPUs”](#cpus)\\nRedis VMs were the first nodes we switched to from N1 to ‘[C2](https://cloud.google.com/compute/docs/machine-types#c2_machine_types)’\\nnode types for the best raw single-threaded CPU performance. This [halved](https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/230#note_312403063)\\nthe CPU usage on our sidekiq cluster, and [almost the same](https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/9636)\\non the cache cluster. Just in case you were in any doubt as to how important the single-threaded CPU performance was\\nto redis.\\nRedis 6 has [multithreaded I/O](https://github.com/redis/redis/pull/6038/files) which helps by moving\\nsome network I/O work to non-core threads, but the core work must still occur on the main thread, so it is only a\\nmitigation.\\n## High Availability\\n[Section titled “High Availability”](#high-availability)\\nFor each cluster we run 3 nodes, using [Redis Sentinel](https://redis.io/topics/sentinel) to manage failover. All\\ntraffic goes through the currently active primary, and writes are replicated to the two replicas asynchronously. If the\\nprimary fails (as determined by Sentinel), one of the replicas is promoted to primary.\\nClients are configured to connect to the sentinels which will then direct them to the current primary; there is no load\\nbalancer involved in this path. Failover is automatic (handled by Sentinel) and results in clients being disconnected\\nfrom the primary and then reconnecting via the sentinels again. It requires no operator intervention under normal circumstances\\nThe configuration is subtly different across the clusters, for historical reasons; the persistent and sidekiq clusters\\nhave sentinel running on the VMs alongside redis, whereas the cache cluster uses a distinct set of sentinel VMs.\\n<https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/11389> records the desire to clean this up.\\n### Node failure\\n[Section titled “Node failure”](#node-failure), - workhorse\\n- [Workhorse Service](/workhorse/)\\n- [Workhorse Apdex Degradation](/workhorse/workhorse-apdex-degradation/)\\n- workspaces\\n- [Remote Development Workspaces Service](/workspaces/)\\n- zoekt\\n- [Exact code search Service](/zoekt/)\\n(() => {\\nconst scroller = document.getElementById('starlight\\_\\_sidebar');\\nif (!window.\\_starlightScrollRestore || !scroller) return;\\nscroller.scrollTop = window.\\_starlightScrollRestore;\\ndelete window.\\_starlightScrollRestore;\\n})();\\n[GitLab](https://gitlab.com/gitlab-com/runbooks)[Code Context](https://code-context.runway.gitlab.net/runbooks-docs)\\nSelect theme   DarkLightAuto\\nStarlightThemeProvider.updatePickers();\\nconst a=document.getElementById(\"starlight\\_\\_sidebar\"),n=a?.querySelector(\"sl-sidebar-state-persist\"),o=\"sl-sidebar-state\",i=()=>{let t=[];const e=n?.dataset.hash||\"\";try{const s=sessionStorage.getItem(o),r=JSON.parse(s||\"{}\");Array.isArray(r.open)&&r.hash===e&&(t=r.open)}catch{}return{hash:e,open:t,scroll:a?.scrollTop||0}},c=t=>{try{sessionStorage.setItem(o,JSON.stringify(t))}catch{}},d=()=>c(i()),l=(t,e)=>{const s=i();s.open[e]=t,c(s)};n?.addEventListener(\"click\",t=>{if(!(t.target instanceof Element))return;const e=t.target.closest(\"summary\")?.closest(\"details\");if(!e)return;const s=e.querySelector(\"sl-sidebar-restore\"),r=parseInt(s?.dataset.index||\"\");isNaN(r)||l(!e.open,r)});addEventListener(\"visibilitychange\",()=>{document.visibilityState===\"hidden\"&&d()});addEventListener(\"pageHide\",d);\\nOn this page\\n- [Overview](#_top)\\n- [Logging](#logging)\\n## On this page\\n- [Overview](#_top)\\n- [Logging](#logging)\\n# Redis TraceChunks Service\\n- [Service Overview](https://dashboards.gitlab.net/d/redis-tracechunks-main/redis-tracechunks-overview)\\n- **Alerts**: <https://alerts.gitlab.net/#/alerts?filter=%7Btype%3D%22redis-tracechunks%22%2C%20tier%3D%22db%22%7D>\\n- **Label**: gitlab-com/gl-infra/production~“Service::RedisTraceChunks”\\n## Logging\\n[Section titled “Logging”](#logging)\\n- [system](https://log.gprd.gitlab.net/goto/a10c2cd2b710f9eb65b13b9a2a328d51)\\nThe TraceChunks redis instances is a distinct redis cluster for storing CI build trace chunks immediately when received by the API from runners, before they are asynchronously (Ci::BuildTraceChunkFlushWorker) moved from there to ObjectStorage (as configured in .com; the database is another target option, but we don’t do that anymore). The data is important and persistent but transient (seconds to 10s of seconds normally, probably not minutes, definitely not hours, unless something has gone horribly wrong).\\nThis was split because as found in <https://gitlab.com/gitlab-org/gitlab/-/issues/327469#note_556531587> these trace chunks were (at the time) responsible for about 60% of the network throughput by bytes on the persistent redis, and 16% of the Redis calls, and it is a distinctive enough workload/usage pattern that splitting it out was deemed a good thing.\\nRedis is in the usual 3-node single-primary/two-replica VM configuration, with sentinels on the same VMs as Redis (as for persistent and sidekiq clusters)\\nSingle threaded CPU is the normal constraint for Redis but due to the large data transfers networking *may* become a constraint before CPU does on this cluster, although there is a *lot* of headroom (2 orders of magnitude) at this writing (2021) and scaling up the instances is possible to obtain more (or maybe the limits will be higher by then)\\nThe usual persistent redis availability expectations for our 3-node clusters apply (see <https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/redis/redis-survival-guide-for-sres.md#high-availability> )]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             # Impact of Sidekiq on Redis Persistence and High Availability in GitLab\\n\\n## 1. Direct Answer\\nThe use of Sidekiq in GitLab's architecture impacts the persistence guarantees and high availability of Redis clusters by introducing a reliance on Redis for job queue management. Sidekiq jobs are stored in Redis, which is configured for persistence through RDB dumps every 900 seconds. However, while the data in Sidekiq is expected to be durable under normal operations, some data loss may occur during unplanned failover events. The architecture is designed to ensure high availability through a 3-node Redis cluster setup with Sentinel for automatic failover, but complete data loss is acceptable for transient data, such as Sidekiq job queues.\\n\\n## 2. Step-by-Step Instructions\\n### Step 1: Understand Redis Configuration for Sidekiq\\n- Redis instances used for Sidekiq are configured to persist data with RDB dumps every 900 seconds.\\n- Ensure that the Redis cluster for Sidekiq is set up with 3 nodes (1 primary and 2 replicas) for high availability.\\n\\n### Step 2: Monitor Redis and Sidekiq\\n- Regularly check the health of the Redis cluster and Sidekiq jobs.\\n- Use the following commands to monitor Redis:\\n  ```bash\\n  redis-cli -h <redis_host> -p <redis_port> info\\n  redis-cli -h <redis_host> -p <redis_port> slowlog get\\n  ```\\n\\n### Step 3: Handle Failover Events\\n- In the event of a primary node failure, Redis Sentinel will automatically promote one of the replicas to primary.\\n- Ensure that clients are configured to connect to the sentinels for automatic failover handling.\\n\\n### Step 4: Review Persistence Guarantees\\n- Understand that while Redis provides some level of persistence, it is not a full DBMS. Data loss may occur during unplanned failovers.\\n- Regularly back up Redis data and monitor the RDB files to ensure data integrity.\\n\\n### Step 5: Optimize Sidekiq Performance\\n- Monitor Sidekiq performance and ensure that job processing is efficient to prevent data buildup in Redis.\\n- Use the following command to check Sidekiq stats:\\n  ```bash\\n  bundle exec sidekiqctl stats <pid>\\n  ```\\n\\n## 3. Key Commands\\n- **Check Redis Info:**\\n  ```bash\\n  redis-cli -h <redis_host> -p <redis_port> info\\n  ```\\n- **Get Slow Logs:**\\n  ```bash\\n  redis-cli -h <redis_host> -p <redis_port> slowlog get\\n  ```\\n- **Check Sidekiq Stats:**\\n  ```bash\\n  bundle exec sidekiqctl stats <pid>\\n  ```\\n\\n## 4. Important Notes\\n- **Data Loss Tolerance:** Be aware that while Redis is configured for persistence, some data loss is acceptable for transient data like Sidekiq job queues during failover events.\\n- **High Availability Setup:** Ensure that Redis is set up with Sentinel for automatic failover, and that clients are properly configured to connect through Sentinels.\\n- **Monitoring:** Regularly monitor both Redis and Sidekiq to ensure that they are functioning correctly and that there are no bottlenecks or failures in job processing.\\n- **Backup Strategy:** Implement a backup strategy for Redis data to mitigate risks associated with data loss during unplanned outages.                                                                                                                                    In GitLab's architecture, Sidekiq is utilized for job processing, which means that jobs may not run if there are issues with the Redis instances. The persistence guarantees for Sidekiq imply that some jobs might run twice or not at all if a primary Redis instance fails. High availability is maintained through the use of Redis Sentinel, which manages failover processes. If the primary Redis fails, Sentinel promotes a replica to primary, ensuring that Sidekiq can continue processing jobs without significant downtime. However, the asynchronous nature of Redis replication means that some writes accepted by the lost primary could be lost, which is a concern for Sidekiq's job processing.         0.562             0.949              1.000           1.000\n",
            "\n",
            "📈 PERFORMANCE INTERPRETATION:\n",
            "--------------------------------------------------\n",
            "Faithfulness: 0.700 - 🟡 Good\n",
            "Answer Relevancy: 0.912 - 🟢 Excellent\n",
            "Context Precision: 0.590 - 🟠 Fair\n",
            "Context Recall: 0.841 - 🟢 Excellent\n",
            "\n",
            "💾 Full results saved to: results_df\n",
            "💾 Summary statistics saved to: summary_df\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run RAGAS Evaluation\n",
        "if cached_questions and 'vector_store' in locals() and 'get_model_factory' in locals():\n",
        "    print(\"Creating evaluation dataset...\")\n",
        "    evaluation_data = create_evaluation_dataset(cached_questions, vector_store, get_model_factory())\n",
        "    \n",
        "    # Convert to HuggingFace Dataset format for RAGAS\n",
        "    print(\"Converting to RAGAS evaluation format...\")\n",
        "    eval_dataset = Dataset.from_list(evaluation_data)\n",
        "    \n",
        "    print(f\"Evaluation dataset created with {len(eval_dataset)} samples\")\n",
        "    print(f\"Sample structure: {eval_dataset[0].keys()}\")\n",
        "    \n",
        "    # Run RAGAS evaluation\n",
        "    print(\"\\nRunning RAGAS evaluation metrics...\")\n",
        "    print(\"This may take a few minutes...\")\n",
        "    \n",
        "    try:\n",
        "        # Define metrics\n",
        "        metrics = [\n",
        "            faithfulness,           # Measures factual consistency of generated answer\n",
        "            answer_relevancy,      # Measures how relevant the answer is to the question  \n",
        "            context_precision,     # Measures precision of retrieved contexts\n",
        "            context_recall         # Measures recall of retrieved contexts\n",
        "        ]\n",
        "        \n",
        "        # Run evaluation\n",
        "        result = evaluate(\n",
        "            eval_dataset,\n",
        "            metrics=metrics,\n",
        "            llm=get_model_factory().get_llm(),\n",
        "            embeddings=get_model_factory().get_embeddings()\n",
        "        )\n",
        "        \n",
        "        print(\"✅ RAGAS evaluation completed successfully!\")\n",
        "        \n",
        "        # Convert results to DataFrame for better display\n",
        "        results_df = result.to_pandas()\n",
        "        \n",
        "        # Calculate summary statistics\n",
        "        summary_stats = {\n",
        "            'Metric': ['Faithfulness', 'Answer Relevancy', 'Context Precision', 'Context Recall'],\n",
        "            'Mean Score': [\n",
        "                results_df['faithfulness'].mean(),\n",
        "                results_df['answer_relevancy'].mean(), \n",
        "                results_df['context_precision'].mean(),\n",
        "                results_df['context_recall'].mean()\n",
        "            ],\n",
        "            'Std Dev': [\n",
        "                results_df['faithfulness'].std(),\n",
        "                results_df['answer_relevancy'].std(),\n",
        "                results_df['context_precision'].std(), \n",
        "                results_df['context_recall'].std()\n",
        "            ],\n",
        "            'Min Score': [\n",
        "                results_df['faithfulness'].min(),\n",
        "                results_df['answer_relevancy'].min(),\n",
        "                results_df['context_precision'].min(),\n",
        "                results_df['context_recall'].min()\n",
        "            ],\n",
        "            'Max Score': [\n",
        "                results_df['faithfulness'].max(),\n",
        "                results_df['answer_relevancy'].max(),\n",
        "                results_df['context_precision'].max(),\n",
        "                results_df['context_recall'].max()\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        summary_df = pd.DataFrame(summary_stats)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RAGAS EVALUATION RESULTS - REDIS RUNBOOK ASSISTANT\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Display summary table\n",
        "        print(\"\\n📊 SUMMARY METRICS TABLE:\")\n",
        "        print(summary_df.round(3).to_string(index=False))\n",
        "        \n",
        "        # Display detailed results\n",
        "        print(f\"\\n📋 DETAILED RESULTS ({len(results_df)} samples):\")\n",
        "        print(results_df.round(3).to_string(index=False))\n",
        "        \n",
        "        # Performance interpretation\n",
        "        print(\"\\n📈 PERFORMANCE INTERPRETATION:\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        for metric in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:\n",
        "            mean_score = results_df[metric].mean()\n",
        "            if mean_score >= 0.8:\n",
        "                performance = \"🟢 Excellent\"\n",
        "            elif mean_score >= 0.6:\n",
        "                performance = \"🟡 Good\" \n",
        "            elif mean_score >= 0.4:\n",
        "                performance = \"🟠 Fair\"\n",
        "            else:\n",
        "                performance = \"🔴 Needs Improvement\"\n",
        "                \n",
        "            print(f\"{metric.replace('_', ' ').title()}: {mean_score:.3f} - {performance}\")\n",
        "        \n",
        "        print(f\"\\n💾 Full results saved to: results_df\")\n",
        "        print(f\"💾 Summary statistics saved to: summary_df\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during RAGAS evaluation: {str(e)}\")\n",
        "        print(\"This might be due to API rate limits or configuration issues.\")\n",
        "        \n",
        "else:\n",
        "    print(\"❌ Cannot run evaluation - missing required components:\")\n",
        "    if not cached_questions:\n",
        "        print(\"  - No cached test questions found\")\n",
        "    if 'vector_store' not in locals():\n",
        "        print(\"  - Vector store not available\")\n",
        "    if 'get_model_factory' not in locals():\n",
        "        print(\"  - Model factory not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Retrieval Comparison\n",
        "\n",
        "This section implements and compares advanced retrieval methods:\n",
        "\n",
        "1. **BM25 Retrieval** - Keyword-based retrieval using BM25 algorithm\n",
        "2. **Performance Comparison** - Compare naive vs BM25 retrieval using RAGAS metrics\n",
        "3. **Results Analysis** - Identify which method performs better for Redis runbook queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Retrieval: BM25 Implementation\n",
        "from rank_bm25 import BM25Okapi\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def preprocess_text_for_bm25(text: str) -> List[str]:\n",
        "    \"\"\"Preprocess text for BM25 tokenization\"\"\"\n",
        "    # Convert to lowercase and split into words\n",
        "    text = text.lower()\n",
        "    # Remove special characters and split\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def create_bm25_index(documents: List[str]):\n",
        "    \"\"\"Create BM25 index from documents\"\"\"\n",
        "    # Tokenize documents\n",
        "    tokenized_docs = [preprocess_text_for_bm25(doc) for doc in documents]\n",
        "    \n",
        "    # Create BM25 index\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "    return bm25, tokenized_docs\n",
        "\n",
        "def bm25_retrieval(query: str, bm25_index, tokenized_docs, documents, k=5):\n",
        "    \"\"\"Retrieve documents using BM25\"\"\"\n",
        "    # Tokenize query\n",
        "    query_tokens = preprocess_text_for_bm25(query)\n",
        "    \n",
        "    # Get BM25 scores\n",
        "    scores = bm25_index.get_scores(query_tokens)\n",
        "    \n",
        "    # Get top k documents\n",
        "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    \n",
        "    # Return documents in order of relevance\n",
        "    retrieved_docs = []\n",
        "    for idx in top_indices:\n",
        "        retrieved_docs.append(documents[idx])\n",
        "    \n",
        "    return retrieved_docs, top_indices\n",
        "\n",
        "def answer_runbook_question_bm25(question, bm25_index, tokenized_docs, documents, model_factory, k=5):\n",
        "    \"\"\"Answer questions using BM25 retrieval - same prompt as naive retrieval\"\"\"\n",
        "    \n",
        "    # Retrieve relevant documents using BM25\n",
        "    retrieved_docs, indices = bm25_retrieval(question, bm25_index, tokenized_docs, documents, k=k)\n",
        "    \n",
        "    # Create Document objects to match naive retrieval format\n",
        "    from langchain_core.documents import Document\n",
        "    bm25_docs = []\n",
        "    for i, doc_content in enumerate(retrieved_docs):\n",
        "        # Create minimal metadata to match format\n",
        "        doc = Document(\n",
        "            page_content=doc_content,\n",
        "            metadata={\"source\": f\"bm25_retrieved_{i}\", \"title\": f\"BM25 Result {i+1}\"}\n",
        "        )\n",
        "        bm25_docs.append(doc)\n",
        "    \n",
        "    # Use the EXACT same function as naive retrieval\n",
        "    return answer_runbook_question(question, None, model_factory, custom_docs=bm25_docs)\n",
        "\n",
        "# Create BM25 index from all document chunks\n",
        "if 'chunks' in locals():\n",
        "    print(\"Creating BM25 index from document chunks...\")\n",
        "    documents = [chunk.page_content for chunk in chunks]\n",
        "    bm25_index, tokenized_docs = create_bm25_index(documents)\n",
        "    print(f\"BM25 index created with {len(documents)} documents\")\n",
        "    \n",
        "    # Test BM25 retrieval\n",
        "    test_query = \"How to monitor Redis memory usage?\"\n",
        "    print(f\"\\nTesting BM25 retrieval with: '{test_query}'\")\n",
        "    \n",
        "    retrieved_docs, indices = bm25_retrieval(test_query, bm25_index, tokenized_docs, documents, k=3)\n",
        "    print(f\"Retrieved {len(retrieved_docs)} documents using BM25\")\n",
        "    \n",
        "    for i, doc in enumerate(retrieved_docs[:2], 1):\n",
        "        preview = doc[:200] + \"...\" if len(doc) > 200 else doc\n",
        "        print(f\"\\nBM25 Result {i}: {preview}\")\n",
        "        \n",
        "else:\n",
        "    print(\"❌ Cannot create BM25 index - chunks not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Naive vs BM25 Retrieval Performance\n",
        "if (cached_questions and 'vector_store' in locals() and 'bm25_index' in locals() \n",
        "    and 'get_model_factory' in locals() and 'documents' in locals() and 'tokenized_docs' in locals()):\n",
        "    \n",
        "    print(\"🔄 COMPARING NAIVE vs BM25 RETRIEVAL PERFORMANCE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Create evaluation datasets for both methods\n",
        "    print(\"Creating evaluation dataset for Naive Retrieval...\")\n",
        "    naive_eval_data = create_evaluation_dataset(cached_questions, vector_store, get_model_factory())\n",
        "    \n",
        "    print(\"Creating evaluation dataset for BM25 Retrieval...\")\n",
        "    bm25_eval_data = []\n",
        "    for i, test_item in enumerate(cached_questions, 1):\n",
        "        print(f\"Processing BM25 question {i}/{len(cached_questions)}: {test_item['question'][:50]}...\")\n",
        "        \n",
        "        question = test_item['question']\n",
        "        ground_truths = test_item['ground_truths']\n",
        "        \n",
        "        # Get answer and contexts from BM25 RAG pipeline\n",
        "        answer, retrieved_docs = answer_runbook_question_bm25(\n",
        "            question, bm25_index, tokenized_docs, documents, get_model_factory(), k=5\n",
        "        )\n",
        "        contexts = [doc.page_content for doc in retrieved_docs]  # Extract content from Document objects\n",
        "        \n",
        "        bm25_eval_data.append({\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'contexts': contexts,\n",
        "            'ground_truths': ground_truths,\n",
        "            'reference': ground_truths[0] if ground_truths else ''  # RAGAS expects 'reference' column\n",
        "        })\n",
        "    \n",
        "    # Convert to RAGAS format\n",
        "    naive_dataset = Dataset.from_list(naive_eval_data)\n",
        "    bm25_dataset = Dataset.from_list(bm25_eval_data)\n",
        "    \n",
        "    print(f\"\\nRunning RAGAS evaluation for both methods...\")\n",
        "    print(\"This may take several minutes...\")\n",
        "    \n",
        "    try:\n",
        "        # Define metrics\n",
        "        metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
        "        \n",
        "        # Evaluate Naive Retrieval\n",
        "        print(\"\\n📊 Evaluating Naive Retrieval...\")\n",
        "        naive_result = evaluate(naive_dataset, metrics=metrics, \n",
        "                               llm=get_model_factory().get_llm(), \n",
        "                               embeddings=get_model_factory().get_embeddings())\n",
        "        \n",
        "        # Evaluate BM25 Retrieval  \n",
        "        print(\"\\n📊 Evaluating BM25 Retrieval...\")\n",
        "        bm25_result = evaluate(bm25_dataset, metrics=metrics,\n",
        "                              llm=get_model_factory().get_llm(),\n",
        "                              embeddings=get_model_factory().get_embeddings())\n",
        "        \n",
        "        # Convert to DataFrames\n",
        "        naive_df = naive_result.to_pandas()\n",
        "        bm25_df = bm25_result.to_pandas()\n",
        "        \n",
        "        # Create comparison table\n",
        "        comparison_data = {\n",
        "            'Metric': ['Faithfulness', 'Answer Relevancy', 'Context Precision', 'Context Recall'],\n",
        "            'Naive Retrieval': [\n",
        "                naive_df['faithfulness'].mean(),\n",
        "                naive_df['answer_relevancy'].mean(),\n",
        "                naive_df['context_precision'].mean(),\n",
        "                naive_df['context_recall'].mean()\n",
        "            ],\n",
        "            'BM25 Retrieval': [\n",
        "                bm25_df['faithfulness'].mean(),\n",
        "                bm25_df['answer_relevancy'].mean(), \n",
        "                bm25_df['context_precision'].mean(),\n",
        "                bm25_df['context_recall'].mean()\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        \n",
        "        # Calculate differences\n",
        "        comparison_df['Difference'] = comparison_df['BM25 Retrieval'] - comparison_df['Naive Retrieval']\n",
        "        comparison_df['Better Method'] = comparison_df.apply(\n",
        "            lambda row: 'BM25' if row['Difference'] > 0 else 'Naive', axis=1\n",
        "        )\n",
        "        \n",
        "        # Round to 3 decimal places\n",
        "        comparison_df = comparison_df.round(3)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RAGAS EVALUATION COMPARISON: NAIVE vs BM25 RETRIEVAL\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        print(\"\\n📊 PERFORMANCE COMPARISON TABLE:\")\n",
        "        print(comparison_df.to_string(index=False))\n",
        "        \n",
        "        # Summary analysis\n",
        "        print(\"\\n📈 PERFORMANCE ANALYSIS:\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        naive_wins = (comparison_df['Difference'] < 0).sum()\n",
        "        bm25_wins = (comparison_df['Difference'] > 0).sum()\n",
        "        \n",
        "        print(f\"🏆 Naive Retrieval wins: {naive_wins}/4 metrics\")\n",
        "        print(f\"🏆 BM25 Retrieval wins: {bm25_wins}/4 metrics\")\n",
        "        \n",
        "        if bm25_wins > naive_wins:\n",
        "            print(\"\\n🎯 CONCLUSION: BM25 Retrieval performs better overall\")\n",
        "        elif naive_wins > bm25_wins:\n",
        "            print(\"\\n🎯 CONCLUSION: Naive Retrieval performs better overall\") \n",
        "        else:\n",
        "            print(\"\\n🎯 CONCLUSION: Both methods perform similarly\")\n",
        "            \n",
        "        # Detailed metric analysis\n",
        "        print(\"\\n📋 METRIC-BY-METRIC ANALYSIS:\")\n",
        "        for _, row in comparison_df.iterrows():\n",
        "            metric = row['Metric']\n",
        "            naive_score = row['Naive Retrieval']\n",
        "            bm25_score = row['BM25 Retrieval']\n",
        "            diff = row['Difference']\n",
        "            \n",
        "            if abs(diff) < 0.05:\n",
        "                result = \"≈ Similar performance\"\n",
        "            elif diff > 0:\n",
        "                result = f\"📈 BM25 better by {diff:.3f}\"\n",
        "            else:\n",
        "                result = f\"📉 Naive better by {abs(diff):.3f}\"\n",
        "                \n",
        "            print(f\"  {metric}: {result}\")\n",
        "        \n",
        "        print(f\"\\n💾 Results saved to:\")\n",
        "        print(f\"  - naive_df: Naive retrieval detailed results\")\n",
        "        print(f\"  - bm25_df: BM25 retrieval detailed results\") \n",
        "        print(f\"  - comparison_df: Performance comparison table\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during comparison evaluation: {str(e)}\")\n",
        "        print(\"This might be due to API rate limits or configuration issues.\")\n",
        "        \n",
        "else:\n",
        "    print(\"❌ Cannot run comparison - missing required components:\")\n",
        "    missing = []\n",
        "    if not cached_questions:\n",
        "        missing.append(\"cached test questions\")\n",
        "    if 'vector_store' not in locals():\n",
        "        missing.append(\"vector store\")\n",
        "    if 'bm25_index' not in locals():\n",
        "        missing.append(\"BM25 index\")\n",
        "    if 'get_model_factory' not in locals():\n",
        "        missing.append(\"model factory\")\n",
        "    if 'documents' not in locals():\n",
        "        missing.append(\"document chunks\")\n",
        "        \n",
        "    print(f\"  Missing: {', '.join(missing)}\")\n",
        "    print(\"Please ensure all previous cells have been executed successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
