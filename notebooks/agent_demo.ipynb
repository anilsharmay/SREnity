{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SREnity Agent Demo - LangGraph ReAct Implementation\n",
        "\n",
        "This notebook demonstrates the agentic RAG system using LangGraph with a 2-node ReAct pattern:\n",
        "- **Assistant Node**: Agent reasoning and tool selection\n",
        "- **Tool Node**: Execute search_runbooks and search_web tools\n",
        "\n",
        "## Features:\n",
        "- Intelligent tool selection based on query analysis\n",
        "- Guardrails to refuse off-topic queries\n",
        "- Fallback from runbooks to web search when needed\n",
        "- Clear agent reasoning visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langgraph in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph) (0.3.79)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph) (2.1.2)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph) (1.0.0)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph) (2.12.2)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (0.4.37)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.11.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: certifi in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/anilyanamandra/Develop/aie8/SREnity/.venv/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install langgraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Set up minimal logging\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# Tavily search\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Local imports\n",
        "from src.utils.config import get_config, get_model_factory\n",
        "from src.rag.advanced_retrieval import create_bm25_reranker_chain\n",
        "from src.utils.document_loader import load_saved_documents\n",
        "\n",
        "print(\"✅ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Graph State\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ GraphState defined\n"
          ]
        }
      ],
      "source": [
        "class GraphState(TypedDict):\n",
        "    \"\"\"State for the agent graph\"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "print(\"✅ GraphState defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Existing RAG Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved runbooks...\n",
            "Loaded 696 total documents\n",
            "Filtered to 33 Redis documents\n"
          ]
        }
      ],
      "source": [
        "# Smart GitLab Runbook Loading with Service Filtering\n",
        "from src.utils.document_loader import download_gitlab_runbooks, save_documents, load_saved_documents\n",
        "from pathlib import Path\n",
        "\n",
        "def filter_by_service(documents, services=['redis']):\n",
        "    \"\"\"Filter documents by service type\"\"\"\n",
        "    filtered = []\n",
        "    for doc in documents:\n",
        "        source = doc.metadata.get('source', '').lower()\n",
        "        if any(service in source for service in services):\n",
        "            filtered.append(doc)\n",
        "    return filtered\n",
        "\n",
        "# Check if runbooks file exists\n",
        "runbooks_file = Path(\"../data/runbooks/gitlab_runbooks.json\")\n",
        "\n",
        "if runbooks_file.exists():\n",
        "    print(\"Loading saved runbooks...\")\n",
        "    documents = load_saved_documents()\n",
        "    print(f\"Loaded {len(documents)} total documents\")\n",
        "else:\n",
        "    print(\"Downloading fresh runbooks...\")\n",
        "    documents = download_gitlab_runbooks()\n",
        "    print(f\"Downloaded {len(documents)} documents\")\n",
        "    \n",
        "    print(\"Saving documents...\")\n",
        "    filepath = save_documents(documents)\n",
        "    print(f\"Saved to {filepath}\")\n",
        "\n",
        "# Filter to Redis services only\n",
        "documents = filter_by_service(documents, ['redis'])\n",
        "print(f\"Filtered to {len(documents)} Redis documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing HTML documents to markdown...\n",
            "HTML to Markdown conversion results:\n",
            "  Original: 290,437 - 575,312 chars\n",
            "  Markdown: 52,226 - 96,814 chars\n",
            "  Reduction: 81.5%\n",
            "Chunking preprocessed documents...\n",
            "Created 685 chunks (631,830 tokens, avg 922 tokens/chunk)\n",
            "Creating BM25 + Reranker chain...\n",
            "Creating BM25 + Reranker chain...\n",
            "Creating BM25 retriever from 685 documents...\n",
            "BM25 retriever created (k=12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anilyanamandra/Develop/aie8/SREnity/notebooks/../src/rag/advanced_retrieval.py:36: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
            "  compressor = CohereRerank(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BM25 + Reranker chain created (BM25 k=12, Rerank k=5)\n",
            "✅ BM25 + Reranker chain ready\n"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "config = get_config()\n",
        "model_factory = get_model_factory()\n",
        "\n",
        "# Preprocess and chunk documents (same as rag_evaluation)\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import tiktoken\n",
        "from src.utils.document_loader import preprocess_html_documents\n",
        "\n",
        "def chunk_documents_with_tiktoken(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"Split documents using tiktoken for accurate token counting\"\"\"\n",
        "    \n",
        "    # Get tiktoken encoding for the configured model\n",
        "    encoding = tiktoken.encoding_for_model(config.openai_model)\n",
        "    \n",
        "    # Create text splitter with tiktoken length function\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=lambda text: len(encoding.encode(text)),\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    # Split documents\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_tokens = sum(len(encoding.encode(chunk.page_content)) for chunk in chunks)\n",
        "    avg_tokens = total_tokens / len(chunks) if chunks else 0\n",
        "    \n",
        "    print(f\"Created {len(chunks)} chunks ({total_tokens:,} tokens, avg {avg_tokens:.0f} tokens/chunk)\")\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Preprocess HTML documents to markdown\n",
        "print(\"Preprocessing HTML documents to markdown...\")\n",
        "processed_documents = preprocess_html_documents(documents)\n",
        "\n",
        "# Chunk the preprocessed documents\n",
        "print(\"Chunking preprocessed documents...\")\n",
        "chunked_docs = chunk_documents_with_tiktoken(processed_documents, chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# Create BM25 + Reranker chain for runbook search\n",
        "print(\"Creating BM25 + Reranker chain...\")\n",
        "bm25_reranker_chain = create_bm25_reranker_chain(chunked_docs, model_factory, bm25_k=12, rerank_k=5)\n",
        "print(\"✅ BM25 + Reranker chain ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created 2 tools:\n",
            "  - search_runbooks: Search GitLab SRE runbooks for troubleshooting procedures, commands, and best practices.\n",
            "\n",
            "Use this tool for:\n",
            "- Standard SRE procedures\n",
            "- Troubleshooting steps\n",
            "- Command syntax and usage\n",
            "- Infrastructure best practices\n",
            "\n",
            "Args:\n",
            "    query: The SRE question or issue to search for\n",
            "\n",
            "Returns:\n",
            "    Formatted response with runbook guidance\n",
            "  - search_web: Search the web for latest updates, CVEs, version-specific issues, and recent changes.\n",
            "\n",
            "Use this tool for:\n",
            "- Recent vulnerabilities or security updates\n",
            "- Version-specific issues not in runbooks\n",
            "- Latest best practices or changes\n",
            "- Breaking changes in tools or services\n",
            "\n",
            "Args:\n",
            "    query: The technical question to search for on the web\n",
            "\n",
            "Returns:\n",
            "    Recent web information and updates\n"
          ]
        }
      ],
      "source": [
        "@tool\n",
        "def search_runbooks(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Search GitLab SRE runbooks for troubleshooting procedures, commands, and best practices.\n",
        "    \n",
        "    Use this tool for:\n",
        "    - Standard SRE procedures\n",
        "    - Troubleshooting steps\n",
        "    - Command syntax and usage\n",
        "    - Infrastructure best practices\n",
        "    \n",
        "    Args:\n",
        "        query: The SRE question or issue to search for\n",
        "    \n",
        "    Returns:\n",
        "        Formatted response with runbook guidance\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = bm25_reranker_chain.invoke({\"question\": query})\n",
        "        return result[\"response\"]\n",
        "    except Exception as e:\n",
        "        return f\"Error searching runbooks: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Search the web for latest updates, CVEs, version-specific issues, and recent changes.\n",
        "    \n",
        "    Use this tool for:\n",
        "    - Recent vulnerabilities or security updates\n",
        "    - Version-specific issues not in runbooks\n",
        "    - Latest best practices or changes\n",
        "    - Breaking changes in tools or services\n",
        "    \n",
        "    Args:\n",
        "        query: The technical question to search for on the web\n",
        "    \n",
        "    Returns:\n",
        "        Recent web information and updates\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize Tavily search\n",
        "        tavily_tool = TavilySearchResults(\n",
        "            max_results=3,\n",
        "            search_depth=\"advanced\"\n",
        "        )\n",
        "        \n",
        "        # Search with SRE context\n",
        "        search_query = f\"SRE DevOps {query} troubleshooting production incident\"\n",
        "        results = tavily_tool.invoke(search_query)\n",
        "        \n",
        "        # Format results\n",
        "        if results:\n",
        "            formatted_results = \"\\n\\n\".join([\n",
        "                f\"**Source:** {result.get('title', 'Unknown')}\\n\"\n",
        "                f\"**URL:** {result.get('url', 'N/A')}\\n\"\n",
        "                f\"**Content:** {result.get('content', 'No content available')}\"\n",
        "                for result in results\n",
        "            ])\n",
        "            return f\"Recent web information:\\n\\n{formatted_results}\"\n",
        "        else:\n",
        "            return \"No recent web information found for this query.\"\n",
        "            \n",
        "    except Exception as e:\n",
        "        return f\"Error searching web: {str(e)}\"\n",
        "\n",
        "# Create tools list\n",
        "tools = [search_runbooks, search_web]\n",
        "print(f\"✅ Created {len(tools)} tools:\")\n",
        "for tool in tools:\n",
        "    print(f\"  - {tool.name}: {tool.description}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create LLM with Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LLM configured with tools\n"
          ]
        }
      ],
      "source": [
        "# Create LLM with tools\n",
        "llm = model_factory.get_llm()\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"✅ LLM configured with tools\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Define Agent Nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Agent nodes defined:\n",
            "  - assistant: Agent reasoning and tool selection\n",
            "  - should_continue: Conditional edge logic\n",
            "  - tool_node: Tool execution\n"
          ]
        }
      ],
      "source": [
        "def assistant(state: GraphState):\n",
        "    \"\"\"\n",
        "    Assistant node: Agent reasoning and tool selection\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # Add system message if this is the first message\n",
        "    if len(messages) == 1 and isinstance(messages[0], HumanMessage):\n",
        "        system_message = \"\"\"\n",
        "You are SREnity, an expert SRE (Site Reliability Engineer) assistant specialized in production incident response.\n",
        "\n",
        "Your expertise includes:\n",
        "- Infrastructure troubleshooting (Redis, PostgreSQL, Elastic, etc.)\n",
        "- GitLab runbook procedures\n",
        "- Production incident resolution\n",
        "- DevOps best practices\n",
        "\n",
        "TOOL USAGE RULES:\n",
        "1. ALWAYS start with search_runbooks for SRE procedures and troubleshooting\n",
        "2. Use search_web ONLY when:\n",
        "   - Runbooks don't have the specific information needed\n",
        "   - You need latest updates, CVEs, or version-specific issues\n",
        "   - The query involves recent changes or breaking updates\n",
        "3. REFUSE non-SRE queries politely but firmly\n",
        "\n",
        "GUARDRAILS:\n",
        "- If the query is clearly off-topic (weather, cooking, general knowledge, personal advice), respond:\n",
        "  \"I'm specialized in SRE incident response and can only help with infrastructure troubleshooting, runbook procedures, and production issues. Please ask about system operations or technical problems.\"\n",
        "- Do NOT use tools for off-topic queries\n",
        "\n",
        "Always provide clear, actionable guidance based on the information you find.\n",
        "\"\"\"\n",
        "        messages = [AIMessage(content=system_message)] + messages\n",
        "    \n",
        "    # Get response from LLM\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    \n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def should_continue(state: GraphState):\n",
        "    \"\"\"\n",
        "    Conditional edge: Determine if tools need to be called\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    \n",
        "    # If the last message has tool calls, go to tools\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    \n",
        "    # Otherwise, we're done\n",
        "    return END\n",
        "\n",
        "# Create tool node\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "print(\"✅ Agent nodes defined:\")\n",
        "print(\"  - assistant: Agent reasoning and tool selection\")\n",
        "print(\"  - should_continue: Conditional edge logic\")\n",
        "print(\"  - tool_node: Tool execution\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build and Compile Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ReAct graph compiled successfully!\n",
            "\n",
            "Graph structure:\n",
            "START → assistant → [tools or END]\n",
            "              ↑          ↓\n",
            "              └──────────┘\n"
          ]
        }
      ],
      "source": [
        "# Build the graph\n",
        "builder = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Set entry point\n",
        "builder.add_edge(START, \"assistant\")\n",
        "\n",
        "# Add conditional edge\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    should_continue,\n",
        "    {\"tools\": \"tools\", END: END}\n",
        ")\n",
        "\n",
        "# Add edge from tools back to assistant\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "# Compile the graph\n",
        "react_graph = builder.compile()\n",
        "\n",
        "print(\"✅ ReAct graph compiled successfully!\")\n",
        "print(\"\\nGraph structure:\")\n",
        "print(\"START → assistant → [tools or END]\")\n",
        "print(\"              ↑          ↓\")\n",
        "print(\"              └──────────┘\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Agent with Sample Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test function ready\n"
          ]
        }
      ],
      "source": [
        "def test_agent(query: str, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Test the agent with a query and show the reasoning process\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"QUERY: {query}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Create initial state\n",
        "    initial_state = {\"messages\": [HumanMessage(content=query)]}\n",
        "    \n",
        "    # Run the graph\n",
        "    if verbose:\n",
        "        print(\"\\n🤖 Agent reasoning process:\")\n",
        "        \n",
        "    result = react_graph.invoke(initial_state, config={\"recursion_limit\": 10})\n",
        "    \n",
        "    # Extract final response\n",
        "    final_message = result[\"messages\"][-1]\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"\\n📝 Final Response:\")\n",
        "        print(\"-\" * 40)\n",
        "    \n",
        "    print(final_message.content)\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"✅ Test function ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Demo Scenarios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "QUERY: How to monitor Redis memory usage?\n",
            "============================================================\n",
            "\n",
            "🤖 Agent reasoning process:\n",
            "\n",
            "📝 Final Response:\n",
            "----------------------------------------\n",
            "To monitor Redis memory usage effectively, you can follow these procedures:\n",
            "\n",
            "1. Use `redis-cli` to gather memory metrics:\n",
            "   - Connect to Redis: `redis-cli -h <redis_host> -p <port>`\n",
            "   - Run `INFO memory` to get overall memory stats.\n",
            "   - Run `MEMORY_STATS` for detailed memory allocator info.\n",
            "\n",
            "2. Enable and use latency monitoring:\n",
            "   - Set threshold: `CONFIG SET latency-monitor-threshold 100`\n",
            "   - Run `LATENCY DOCTOR` for insights into latency spikes related to memory.\n",
            "\n",
            "3. Monitor slowlog for memory-intensive commands:\n",
            "   - Check slowlog entries: `SLOWLOG GET 10`\n",
            "   - Review slowlog configuration: `CONFIG GET slowlog-log-slower-than` and `CONFIG GET slowlog-max-len`\n",
            "\n",
            "4. Use Prometheus/Grafana dashboards for real-time and historical trends.\n",
            "\n",
            "5. For advanced analysis, analyze memory dumps or keyspace patterns with specialized tools.\n",
            "\n",
            "Key commands include `INFO memory`, `MEMORY_STATS`, `LATENCY DOCTOR`, and slowlog commands. Regular monitoring of `used_memory`, `mem_fragmentation_ratio`, and evicted keys helps maintain Redis health.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='How to monitor Redis memory usage?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9z2ZNOFgqE2n2gLERX2EVT9C', 'function': {'arguments': '{\"query\":\"monitor Redis memory usage\"}', 'name': 'search_runbooks'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 438, 'total_tokens': 456, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp79Gw8tquLTo8YzQFBhRGFTf7gC', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--23670134-e8ff-472f-9b35-7e74d334aef3-0', tool_calls=[{'name': 'search_runbooks', 'args': {'query': 'monitor Redis memory usage'}, 'id': 'call_9z2ZNOFgqE2n2gLERX2EVT9C', 'type': 'tool_call'}], usage_metadata={'input_tokens': 438, 'output_tokens': 18, 'total_tokens': 456, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='# Monitoring Redis Memory Usage\\n\\nTo effectively monitor Redis memory usage, follow these step-by-step procedures based on the runbook documentation:\\n\\n---\\n\\n## 1. Use `redis-cli` to Gather Memory Metrics\\n\\n### Step 1: Connect to Redis\\n```bash\\nredis-cli -h <redis_host> -p <port>\\n```\\nReplace `<redis_host>` and `<port>` with your Redis server details.\\n\\n### Step 2: Run the `INFO` Command\\n```bash\\nINFO memory\\n```\\n- This provides detailed memory metrics, including total used memory, peak memory, fragmentation, and more.\\n\\n### Step 3: Run the `MEMORY_STATS` Command\\n```bash\\nMEMORY_STATS\\n```\\n- Offers granular memory statistics, such as memory allocator info, fragmentation ratio, and more.\\n\\n---\\n\\n## 2. Enable and Use Latency Monitoring (Optional but Recommended)\\n\\n### Step 1: Enable Latency Monitoring\\n```bash\\nCONFIG SET latency-monitor-threshold 100\\n```\\n- Sets the latency threshold to 100 microseconds for monitoring.\\n\\n### Step 2: Run Latency Doctor\\n```bash\\nLATENCY DOCTOR\\n```\\n- Provides insights into latency spikes that could be related to memory issues.\\n\\n**Note:** Remember to disable if not needed:\\n```bash\\nCONFIG SET latency-monitor-threshold 0\\n```\\n\\n---\\n\\n## 3. Monitor Slowlog for Memory-Intensive Commands\\n\\n### Step 1: Check Slowlog Entries\\n```bash\\nSLOWLOG GET 10\\n```\\n- Review the top 10 slow commands, which may include memory-heavy operations.\\n\\n### Step 2: Review Slowlog Configuration\\n```bash\\nCONFIG GET slowlog-log-slower-than\\nCONFIG GET slowlog-max-len\\n```\\n- Adjust thresholds if necessary to capture relevant slow operations.\\n\\n---\\n\\n## 4. Use Prometheus/Grafana for Historical and Real-Time Metrics\\n\\n- Explore dashboards such as:\\n  - [Redis Cache Overview](https://dashboards.gitlab.net/d/redis-cache-main/redis-cache-overview)\\n  - [Redis Slowlog Entries](https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview)\\n\\n- These tools provide visualizations of memory usage trends over time, helping identify memory leaks or abnormal growth.\\n\\n---\\n\\n## 5. Additional Memory Analysis (Advanced)\\n\\n### Step 1: Analyze Memory Dump\\n```bash\\nrdb -c memory dump.rdb | ruby redis-analysis-tool.rb\\n```\\n- Use this to analyze the memory dump for key patterns and memory consumption.\\n\\n### Step 2: Keyspace Pattern Analysis\\n- Use the [Keyspace pattern analysis tool](https://gitlab.com/gitlab-com/gl-infra/redis-keyspace-analyzer) for offline analysis of key patterns and their memory footprint.\\n\\n---\\n\\n## Important Notes:\\n- **Avoid using `KEYS` command** in production as it can overload Redis.\\n- Regularly monitor `used_memory`, `mem_fragmentation_ratio`, and `evicted_keys_total`.\\n- Be cautious when enabling latency monitoring; it minimally impacts memory but should be used judiciously.\\n- Use Grafana dashboards for ongoing, visual monitoring of memory trends.\\n\\n---\\n\\n## Summary of Key Commands:\\n| Action | Command |\\n|---------|---------|\\n| Check memory info | `INFO memory` |\\n| Get detailed memory stats | `MEMORY_STATS` |\\n| Enable latency monitor | `CONFIG SET latency-monitor-threshold 100` |\\n| Run latency doctor | `LATENCY DOCTOR` |\\n| Check slowlog | `SLOWLOG GET 10` |\\n| Get slowlog config | `CONFIG GET slowlog-log-slower-than` |\\n| Get slowlog max length | `CONFIG GET slowlog-max-len` |\\n\\n---\\n\\nBy following these steps, SREs can effectively monitor Redis memory usage, identify potential issues early, and maintain optimal Redis performance.', name='search_runbooks', tool_call_id='call_9z2ZNOFgqE2n2gLERX2EVT9C'),\n",
              "  AIMessage(content='To monitor Redis memory usage effectively, you can follow these procedures:\\n\\n1. Use `redis-cli` to gather memory metrics:\\n   - Connect to Redis: `redis-cli -h <redis_host> -p <port>`\\n   - Run `INFO memory` to get overall memory stats.\\n   - Run `MEMORY_STATS` for detailed memory allocator info.\\n\\n2. Enable and use latency monitoring:\\n   - Set threshold: `CONFIG SET latency-monitor-threshold 100`\\n   - Run `LATENCY DOCTOR` for insights into latency spikes related to memory.\\n\\n3. Monitor slowlog for memory-intensive commands:\\n   - Check slowlog entries: `SLOWLOG GET 10`\\n   - Review slowlog configuration: `CONFIG GET slowlog-log-slower-than` and `CONFIG GET slowlog-max-len`\\n\\n4. Use Prometheus/Grafana dashboards for real-time and historical trends.\\n\\n5. For advanced analysis, analyze memory dumps or keyspace patterns with specialized tools.\\n\\nKey commands include `INFO memory`, `MEMORY_STATS`, `LATENCY DOCTOR`, and slowlog commands. Regular monitoring of `used_memory`, `mem_fragmentation_ratio`, and evicted keys helps maintain Redis health.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 244, 'prompt_tokens': 1034, 'total_tokens': 1278, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp7HzVRv2LrpYm2lXXyNPNqKxcyZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--7883b9e8-919a-49e2-92bb-d101f100fa17-0', usage_metadata={'input_tokens': 1034, 'output_tokens': 244, 'total_tokens': 1278, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test 1: Standard SRE query (should use runbooks only)\n",
        "test_agent(\"How to monitor Redis memory usage?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "QUERY: Redis 7.2 memory leak issues and fixes\n",
            "============================================================\n",
            "\n",
            "🤖 Agent reasoning process:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/cz/01ttpj991sx782zbyzgb4bdc0000gn/T/ipykernel_26293/2586420026.py:43: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  tavily_tool = TavilySearchResults(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📝 Final Response:\n",
            "----------------------------------------\n",
            "Based on the recent web information and known issues, here are the key points regarding Redis 7.2 memory leak issues and fixes:\n",
            "\n",
            "1. Redis 7.2 has known memory leak issues that can cause unbounded memory growth. It is crucial to ensure you are running the latest stable version of Redis 7.2 with all patches applied.\n",
            "\n",
            "2. To troubleshoot and fix memory leaks:\n",
            "   - Verify your Redis version:\n",
            "     ```\n",
            "     redis-server --version\n",
            "     ```\n",
            "   - Update Redis to the latest stable release if necessary.\n",
            "   - Monitor memory usage using:\n",
            "     ```\n",
            "     redis-cli info memory\n",
            "     ```\n",
            "   - Identify large keys or memory leaks:\n",
            "     ```\n",
            "     redis-cli --bigkeys\n",
            "     ```\n",
            "   - Use profiling tools like redis-memory-analyzer for detailed analysis.\n",
            "   - Clean up large or problematic keys:\n",
            "     ```\n",
            "     redis-cli DEL <key>\n",
            "     ```\n",
            "   - Adjust memory policies:\n",
            "     ```\n",
            "     redis-cli CONFIG SET maxmemory <bytes>\n",
            "     redis-cli CONFIG SET maxmemory-policy allkeys-lru\n",
            "     ```\n",
            "   - Restart Redis after cleanup:\n",
            "     ```\n",
            "     sudo systemctl restart redis\n",
            "     ```\n",
            "\n",
            "3. Recent updates indicate that setting `maxmemory` too low can cause errors like \"OOM command not allowed when used memory > 'maxmemory'\". Properly tuning `maxmemory` and eviction policies is essential.\n",
            "\n",
            "4. If persistent issues occur, consider increasing the `maxmemory` limit based on your environment's peak memory usage, following best practices for configuration.\n",
            "\n",
            "5. Always backup data before performing mass deletions or flushes, and monitor memory usage continuously after applying fixes.\n",
            "\n",
            "6. For detailed troubleshooting, consult the Redis changelog and breaking changes documentation to ensure compatibility and proper configuration.\n",
            "\n",
            "Would you like detailed commands for your specific environment or assistance with a particular issue?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Redis 7.2 memory leak issues and fixes', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_uBzzavbld9O6JAOvHSwiEmBR', 'function': {'arguments': '{\"query\":\"Redis 7.2 memory leak issues and fixes\"}', 'name': 'search_runbooks'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 441, 'total_tokens': 465, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp7Qn0pvmqDwGUxBRx6olo3Y1A1J', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--36da1b79-f2b4-45bb-9fa4-47ee4f45ecc5-0', tool_calls=[{'name': 'search_runbooks', 'args': {'query': 'Redis 7.2 memory leak issues and fixes'}, 'id': 'call_uBzzavbld9O6JAOvHSwiEmBR', 'type': 'tool_call'}], usage_metadata={'input_tokens': 441, 'output_tokens': 24, 'total_tokens': 465, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content=\"# Redis 7.2 Memory Leak Issues and Fixes\\n\\n## 1. Direct Answer\\nRedis 7.2 has known memory leak issues that can cause unbounded memory growth. To address this, ensure you are running the latest stable version of Redis 7.2 with all patches applied. If memory leaks persist, investigate specific leak sources (e.g., certain modules, large keys, or misconfigurations) and apply recommended fixes such as updating Redis, tuning memory policies, or cleaning up large keys. In critical cases, restart Redis after cleaning up memory-consuming keys or reconfigure Redis to prevent leaks.\\n\\n---\\n\\n## 2. Step-by-Step Instructions\\n\\n### A. Verify Redis Version and Update\\n1. Check current Redis version:\\n   ```\\n   redis-server --version\\n   ```\\n2. If not on the latest Redis 7.2 stable release, update Redis:\\n   - Follow your environment's update procedures (e.g., package manager, build from source).\\n   - Ensure you apply all patches related to memory leak fixes.\\n\\n### B. Monitor Memory Usage and Identify Leaks\\n3. Enable Redis memory diagnostics:\\n   ```\\n   redis-cli info memory\\n   ```\\n4. Use Redis slowlog and profiling tools to identify abnormal commands or patterns:\\n   ```\\n   redis-cli slowlog get\\n   ```\\n5. Check for large keys or unexpected memory growth:\\n   ```\\n   redis-cli --bigkeys\\n   ```\\n6. Use `redis-memory-analyzer` or cupcake-rdb to analyze memory footprint:\\n   ```\\n   redis-memory-analyzer\\n   ```\\n\\n### C. Investigate and Clean Up Large Keys\\n7. Identify large keys:\\n   ```\\n   redis-cli --bigkeys\\n   ```\\n8. Delete or truncate problematic keys:\\n   ```\\n   redis-cli DEL <key>\\n   ```\\n9. For persistent issues, consider flushing entire database (careful!):\\n   ```\\n   redis-cli FLUSHDB\\n   ```\\n\\n### D. Tune Redis Configuration to Prevent Leaks\\n10. Adjust memory policies if necessary:\\n    - Set maxmemory:\\n      ```\\n      redis-cli CONFIG SET maxmemory <bytes>\\n      ```\\n    - Set eviction policy:\\n      ```\\n      redis-cli CONFIG SET maxmemory-policy allkeys-lru\\n      ```\\n11. Disable or adjust modules/plugins that may cause leaks.\\n\\n### E. Restart Redis if Necessary\\n12. After cleanup, restart Redis to clear residual memory:\\n    ```\\n    sudo systemctl restart redis\\n    ```\\n    or\\n    ```\\n    gitlab-ctl restart redis\\n    ```\\n\\n---\\n\\n## 3. Key Commands\\n- Check version:\\n  ```\\n  redis-server --version\\n  ```\\n- Get memory info:\\n  ```\\n  redis-cli info memory\\n  ```\\n- List big keys:\\n  ```\\n  redis-cli --bigkeys\\n  ```\\n- Delete specific key:\\n  ```\\n  redis-cli DEL <key>\\n  ```\\n- Flush database:\\n  ```\\n  redis-cli FLUSHDB\\n  ```\\n- Set maxmemory:\\n  ```\\n  redis-cli CONFIG SET maxmemory <bytes>\\n  ```\\n- Set eviction policy:\\n  ```\\n  redis-cli CONFIG SET maxmemory-policy allkeys-lru\\n  ```\\n- Restart Redis:\\n  ```\\n  sudo systemctl restart redis\\n  ```\\n  or\\n  ```\\n  gitlab-ctl restart redis\\n  ```\\n\\n---\\n\\n## 4. Important Notes\\n- Always backup Redis data before mass deletions or flushes.\\n- Confirm that your Redis version is the latest stable release of 7.2, as patches may fix memory leaks.\\n- Monitor memory usage continuously after applying fixes.\\n- Investigate specific modules or custom code that may cause leaks.\\n- If memory leaks persist despite updates, consider opening a support ticket with Redis or your vendor.\\n\\n---\\n\\n**Summary:**  \\nEnsure Redis 7.2 is up-to-date, monitor memory usage, identify large or leaking keys, clean up as needed, tune memory policies, and restart Redis to mitigate memory leak issues effectively.\", name='search_runbooks', tool_call_id='call_uBzzavbld9O6JAOvHSwiEmBR'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cGkb0KFJCFVRiTFPBCyVH0bk', 'function': {'arguments': '{\"query\":\"Redis 7.2 memory leak issues and fixes\"}', 'name': 'search_web'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 1047, 'total_tokens': 1070, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp7aW4zJacu5deJo1JPUq6XVXcIo', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ef8af067-6816-4084-9676-ad2eba8f565e-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'Redis 7.2 memory leak issues and fixes'}, 'id': 'call_cGkb0KFJCFVRiTFPBCyVH0bk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1047, 'output_tokens': 23, 'total_tokens': 1070, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='Recent web information:\\n\\n**Source:** Redis 7.2 release notes and breaking changes | Docs\\n**URL:** https://redis.io/docs/latest/operate/rc/changelog/version-release-notes/7-2/\\n**Content:** ```\\n    1: config set maxmemory 1\\n    2: +OK\\n    3: eval \"return redis.call(\\'set\\',\\'x\\',\\'y\\')\" 0\\n  - 4: -ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: @user_script: 1: -OOM command not allowed when used memory > \\'maxmemory\\'.\\n  + 4: -ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: OOM command not allowed when used memory > \\'maxmemory\\'.\\n    5: eval \"return redis.pcall(\\'set\\',\\'x\\',\\'y\\')\" 0 [...] - 6: -@user_script: 1: -OOM command not allowed when used memory > \\'maxmemory\\'.\\n  + 6: -OOM command not allowed when used memory > \\'maxmemory\\'.\\n    7: eval \"return redis.call(\\'select\\',99)\" 0\\n    8: -ERR Error running script (call to 4ad5abfc50bbccb484223905f9a16f09cd043ba8): @user_script:1: ERR DB index is out of range\\n    9: eval \"return redis.pcall(\\'select\\',99)\" 0\\n   10: -ERR DB index is out of range\\n   11: eval_ro \"return redis.call(\\'set\\',\\'x\\',\\'y\\')\" 0 [...] ```\\n    cmdstat_acl|list:calls=1,usec=4994,usec_per_call=4994.00,rejected_calls=0,failed_calls=0\\n    cmdstat_acl|setuser:calls=2,usec=16409,usec_per_call=8204.50,rejected_calls=0,failed_calls=0\\n    cmdstat_acl|deluser:calls=1,usec=774,usec_per_call=774.00,rejected_calls=0,failed_calls=0\\n    cmdstat_acl|getuser:calls=1,usec=6044,usec_per_call=6044.00,rejected_calls=0,failed_calls=0\\n\\n    ```\\n `CONFIG REWRITE`, `CONFIG RESETSTAT`)\\n\\n### Known limitations\\n\\n**Source:** Redis 7.2 Breaking Changes\\n**URL:** https://redis.io/docs/latest/embeds/r7.2-breaking-changes/\\n**Content:** ```\\n    1: config set maxmemory 1\\n    2: +OK\\n    3: eval \"return redis.call(\\'set\\',\\'x\\',\\'y\\')\" 0\\n  - 4: -ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: @user_script: 1: -OOM command not allowed when used memory > \\'maxmemory\\'.\\n  + 4: -ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: OOM command not allowed when used memory > \\'maxmemory\\'.\\n    5: eval \"return redis.pcall(\\'set\\',\\'x\\',\\'y\\')\" 0 [...] - 6: -@user_script: 1: -OOM command not allowed when used memory > \\'maxmemory\\'.\\n  + 6: -OOM command not allowed when used memory > \\'maxmemory\\'.\\n    7: eval \"return redis.call(\\'select\\',99)\" 0\\n    8: -ERR Error running script (call to 4ad5abfc50bbccb484223905f9a16f09cd043ba8): @user_script:1: ERR DB index is out of range\\n    9: eval \"return redis.pcall(\\'select\\',99)\" 0\\n   10: -ERR DB index is out of range\\n   11: eval_ro \"return redis.call(\\'set\\',\\'x\\',\\'y\\')\" 0 [...] ```\\n    cmdstat_acl|list:calls=1,usec=4994,usec_per_call=4994.00,rejected_calls=0,failed_calls=0\\n    cmdstat_acl|setuser:calls=2,usec=16409,usec_per_call=8204.50,rejected_calls=0,failed_calls=0\\n    cmdstat_acl|deluser:calls=1,usec=774,usec_per_call=774.00,rejected_calls=0,failed_calls=0\\n    cmdstat_acl|getuser:calls=1,usec=6044,usec_per_call=6044.00,rejected_calls=0,failed_calls=0\\n\\n    ```\\n `CONFIG REWRITE`, `CONFIG RESETSTAT`)\\n\\n**Source:** How to troubleshoot Redis memory issues - the Fortinet Community!\\n**URL:** https://community.fortinet.com/t5/FortiSIEM/Technical-Tip-How-to-troubleshoot-Redis-memory-issues/ta-p/305144\\n**Content:** following the section.    Increase Redis memory.    To identify the new memory limit to set, choose one of the next values where it might take used\\\\_memory\\\\_peak value + 1 or 2GB to make sure data will fit inside:     + 4294967296   + 6442450944   + 8589934592  Then, from super and all the worker nodes, run the next command from CLI as root:  2. new\\\\_mem\\\\_val=6442450944 <--Replace by the value you previously identified.  cp /opt/phoenix/redis/conf/6666.conf [...] /opt/phoenix/redis/conf/6666.conf\\\\_bak  sed -i \"s/maxmemory [0-9]\\\\/maxmemory $new\\\\_mem\\\\_val/\" /opt/phoenix/redis/conf/6666.conf  sed -i \"s/maxmemory [0-9]\\\\/maxmemory $new\\\\_mem\\\\_val/\" /opt/phoenix/redis/conf/conf.tmpl  /opt/phoenix/redis/bin/redis\\\\_ops.sh stop /opt/phoenix/redis/bin/redis\\\\_ops.sh start  kill -9 $(cat /opt/glassfish/domains/domain1/config/pid) <-- To run only on super node.   Connect to the GUI and check if the issue is fixed. Be aware that this parameter may be reset during [...] | Solution | Next troubleshooting steps can be followed when one of the following behaviors is met:      In Analytics, a query filtering on a group does not show the result of the item in the group.  Incidents with a rule filter with the group do not trigger for elements in the group.  Error while navigating through the GUI: \\'IllegalArgumentException: CMDB report: invalid field src\\\\_name\\'.     Check for Out-of-memory errors in the application server logs.  As the Redis cache is fed by the', name='search_web', tool_call_id='call_cGkb0KFJCFVRiTFPBCyVH0bk'),\n",
              "  AIMessage(content='Based on the recent web information and known issues, here are the key points regarding Redis 7.2 memory leak issues and fixes:\\n\\n1. Redis 7.2 has known memory leak issues that can cause unbounded memory growth. It is crucial to ensure you are running the latest stable version of Redis 7.2 with all patches applied.\\n\\n2. To troubleshoot and fix memory leaks:\\n   - Verify your Redis version:\\n     ```\\n     redis-server --version\\n     ```\\n   - Update Redis to the latest stable release if necessary.\\n   - Monitor memory usage using:\\n     ```\\n     redis-cli info memory\\n     ```\\n   - Identify large keys or memory leaks:\\n     ```\\n     redis-cli --bigkeys\\n     ```\\n   - Use profiling tools like redis-memory-analyzer for detailed analysis.\\n   - Clean up large or problematic keys:\\n     ```\\n     redis-cli DEL <key>\\n     ```\\n   - Adjust memory policies:\\n     ```\\n     redis-cli CONFIG SET maxmemory <bytes>\\n     redis-cli CONFIG SET maxmemory-policy allkeys-lru\\n     ```\\n   - Restart Redis after cleanup:\\n     ```\\n     sudo systemctl restart redis\\n     ```\\n\\n3. Recent updates indicate that setting `maxmemory` too low can cause errors like \"OOM command not allowed when used memory > \\'maxmemory\\'\". Properly tuning `maxmemory` and eviction policies is essential.\\n\\n4. If persistent issues occur, consider increasing the `maxmemory` limit based on your environment\\'s peak memory usage, following best practices for configuration.\\n\\n5. Always backup data before performing mass deletions or flushes, and monitor memory usage continuously after applying fixes.\\n\\n6. For detailed troubleshooting, consult the Redis changelog and breaking changes documentation to ensure compatibility and proper configuration.\\n\\nWould you like detailed commands for your specific environment or assistance with a particular issue?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 367, 'prompt_tokens': 2612, 'total_tokens': 2979, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp7dfK6e6qsi6ArLjWhQG3bFo0WR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--77374187-b05a-4511-9017-120c5cfd1367-0', usage_metadata={'input_tokens': 2612, 'output_tokens': 367, 'total_tokens': 2979, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test 2: Version-specific query (should use both tools)\n",
        "test_agent(\"Redis 7.2 memory leak issues and fixes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "QUERY: What's the weather like today?\n",
            "============================================================\n",
            "\n",
            "🤖 Agent reasoning process:\n",
            "\n",
            "📝 Final Response:\n",
            "----------------------------------------\n",
            "I'm specialized in SRE incident response and can only help with infrastructure troubleshooting, runbook procedures, and production issues. Please ask about system operations or technical problems.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"What's the weather like today?\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"I'm specialized in SRE incident response and can only help with infrastructure troubleshooting, runbook procedures, and production issues. Please ask about system operations or technical problems.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 437, 'total_tokens': 470, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp7wiK6sSA9bTgHqW2CDctX5wKEx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b6887b48-8dbb-4220-b03b-9fc9fca1f08b-0', usage_metadata={'input_tokens': 437, 'output_tokens': 33, 'total_tokens': 470, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test 3: Off-topic query (should refuse)\n",
        "test_agent(\"What's the weather like today?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "QUERY: PostgreSQL connection pool exhaustion in production - how to diagnose and fix?\n",
            "============================================================\n",
            "\n",
            "🤖 Agent reasoning process:\n",
            "\n",
            "📝 Final Response:\n",
            "----------------------------------------\n",
            "To diagnose and fix PostgreSQL connection pool exhaustion in production, follow these steps:\n",
            "\n",
            "1. Confirm exhaustion by checking current connection usage via your pooler dashboard or commands.\n",
            "2. Diagnose the issue:\n",
            "   - Check active connections with `SELECT * FROM pg_stat_activity;`.\n",
            "   - Identify long-running transactions with `SELECT pid, age(current_timestamp, xact_start), query FROM pg_stat_activity WHERE ...;`.\n",
            "   - Review logs for errors or slow queries.\n",
            "3. Identify root causes:\n",
            "   - Long transactions or idle connections.\n",
            "   - High application load.\n",
            "   - Misconfigured pool size.\n",
            "4. Implement immediate fixes:\n",
            "   - Terminate long-running or idle connections if necessary:\n",
            "     ```sql\n",
            "     SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE ...;\n",
            "     ```\n",
            "   - Temporarily increase pool size by editing your pooler config (e.g., `pgbouncer.ini`) and reloading the service.\n",
            "5. Optimize:\n",
            "   - Improve slow queries.\n",
            "   - Adjust pool size parameters (`default_pool_size`, `max_client_conn`).\n",
            "   - Ensure proper connection handling in your application.\n",
            "   \n",
            "Monitoring and regular maintenance are key to preventing recurrence. Would you like detailed commands or configuration examples?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='PostgreSQL connection pool exhaustion in production - how to diagnose and fix?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_6DpZfwXl65dg2o4xbUSZVNTE', 'function': {'arguments': '{\"query\":\"PostgreSQL connection pool exhaustion diagnosis and fix\"}', 'name': 'search_runbooks'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 446, 'total_tokens': 469, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp84MRhMscvn2IR7c8bN1WsODGeG', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b596d1cc-28b7-407e-94e8-a2dedeac1d61-0', tool_calls=[{'name': 'search_runbooks', 'args': {'query': 'PostgreSQL connection pool exhaustion diagnosis and fix'}, 'id': 'call_6DpZfwXl65dg2o4xbUSZVNTE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 446, 'output_tokens': 23, 'total_tokens': 469, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='# PostgreSQL Connection Pool Exhaustion Diagnosis and Fix\\n\\n---\\n\\n## 1. **Direct Answer**\\nConnection pool exhaustion occurs when all available connections in the pool are in use, preventing new clients from connecting. To diagnose and fix this, identify the root cause (e.g., long-running transactions, high query load, misconfigured pool size) and then adjust the pool size or optimize queries to reduce connection usage.\\n\\n---\\n\\n## 2. **Step-by-Step Instructions**\\n\\n### **Step 1: Confirm Connection Pool Exhaustion**\\n- Check the current pool status via your pgbouncer or connection pooler dashboard.\\n- Look for high connection utilization or errors indicating no available connections.\\n\\n### **Step 2: Diagnose PostgreSQL and Pooler Load**\\n- Review active connections:\\n  ```bash\\n  psql -c \"SELECT * FROM pg_stat_activity WHERE state = \\'active\\';\"\\n  ```\\n- Check for long-running transactions:\\n  ```bash\\n  psql -c \"SELECT pid, age(current_timestamp, xact_start) AS xact_age, query FROM pg_stat_activity WHERE state = \\'active\\' AND (current_timestamp - xact_start) > interval \\'5 minutes\\';\"\\n  ```\\n- Review logs for errors or slow queries:\\n  - Use [pgbadger Runbook](/patroni/pgbadger_report/) for detailed analysis.\\n\\n### **Step 3: Identify Root Causes**\\n- Long transactions or idle connections holding resources.\\n- Excessive connection requests due to application load.\\n- Misconfigured pool size (e.g., `max_client_conn`, `default_pool_size`).\\n\\n### **Step 4: Implement Immediate Fixes**\\n- Terminate long-running or idle connections if necessary:\\n  ```bash\\n  SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE <conditions>;\\n  ```\\n- Increase pool size temporarily:\\n  - Edit your pgbouncer configuration (see **Key Commands** below).\\n\\n### **Step 5: Optimize and Prevent Future Exhaustion**\\n- Review and optimize slow or long-running queries.\\n- Adjust pool size parameters:\\n  - For pgbouncer: `default_pool_size`, `max_client_conn`.\\n- Implement application-side connection pooling best practices.\\n- Schedule regular VACUUM and analyze to maintain database health.\\n\\n---\\n\\n## 3. **Key Commands**\\n\\n### Check active connections:\\n```bash\\npsql -c \"SELECT * FROM pg_stat_activity WHERE state = \\'active\\';\"\\n```\\n\\n### Terminate long-running transactions:\\n```sql\\nSELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE age(current_timestamp, xact_start) > interval \\'5 minutes\\' AND pid <> pg_backend_pid();\\n```\\n\\n### Edit pgbouncer configuration (e.g., `/etc/pgbouncer/pgbouncer.ini`):\\n```ini\\n[databases]\\nyourdb = host=localhost port=5432 dbname=yourdb\\n\\n[pgbouncer]\\n# Increase pool size\\ndefault_pool_size = 20\\nmax_client_conn = 100\\n```\\n\\n### Reload pgbouncer after config change:\\n```bash\\nsudo systemctl reload pgbouncer\\n```\\n\\n---\\n\\n## 4. **Important Notes**\\n- **Prerequisites:** Ensure you have appropriate permissions to run `pg_stat_activity` queries and modify pgbouncer configs.\\n- **Caution:** Terminating active transactions may cause application errors; do so only if necessary and after informing relevant teams.\\n- **Monitoring:** Continuously monitor connection metrics via dashboards or logs to catch exhaustion early.\\n- **Long-term fix:** Adjust application connection handling, optimize queries, and tune pool parameters based on workload patterns.\\n\\n---\\n\\n**Summary:**  \\nDiagnose by checking active connections and long transactions, terminate unnecessary ones if needed, increase pool size temporarily, and then optimize queries and pool configurations for sustained stability.', name='search_runbooks', tool_call_id='call_6DpZfwXl65dg2o4xbUSZVNTE'),\n",
              "  AIMessage(content='To diagnose and fix PostgreSQL connection pool exhaustion in production, follow these steps:\\n\\n1. Confirm exhaustion by checking current connection usage via your pooler dashboard or commands.\\n2. Diagnose the issue:\\n   - Check active connections with `SELECT * FROM pg_stat_activity;`.\\n   - Identify long-running transactions with `SELECT pid, age(current_timestamp, xact_start), query FROM pg_stat_activity WHERE ...;`.\\n   - Review logs for errors or slow queries.\\n3. Identify root causes:\\n   - Long transactions or idle connections.\\n   - High application load.\\n   - Misconfigured pool size.\\n4. Implement immediate fixes:\\n   - Terminate long-running or idle connections if necessary:\\n     ```sql\\n     SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE ...;\\n     ```\\n   - Temporarily increase pool size by editing your pooler config (e.g., `pgbouncer.ini`) and reloading the service.\\n5. Optimize:\\n   - Improve slow queries.\\n   - Adjust pool size parameters (`default_pool_size`, `max_client_conn`).\\n   - Ensure proper connection handling in your application.\\n   \\nMonitoring and regular maintenance are key to preventing recurrence. Would you like detailed commands or configuration examples?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 245, 'prompt_tokens': 1023, 'total_tokens': 1268, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp8DIQKkePONLqRrD2fOItlgqXS4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0ae7589a-e8d5-414f-9082-44f0dfe8751c-0', usage_metadata={'input_tokens': 1023, 'output_tokens': 245, 'total_tokens': 1268, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test 4: Complex SRE query (should use both tools)\n",
        "test_agent(\"PostgreSQL connection pool exhaustion in production - how to diagnose and fix?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "QUERY: Show me the exact syntax for Redis MEMORY STATS command\n",
            "============================================================\n",
            "\n",
            "🤖 Agent reasoning process:\n",
            "\n",
            "📝 Final Response:\n",
            "----------------------------------------\n",
            "The syntax for the Redis MEMORY STATS command is:\n",
            "\n",
            "```\n",
            "MEMORY STATS\n",
            "```\n",
            "\n",
            "You can run this command after connecting to your Redis instance using redis-cli and authenticating if necessary. It provides detailed memory usage metrics.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Show me the exact syntax for Redis MEMORY STATS command', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jK7HJmFN2BRsscwu2d73Wr2V', 'function': {'arguments': '{\"query\":\"Redis MEMORY STATS command syntax\"}', 'name': 'search_runbooks'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 442, 'total_tokens': 462, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp8qrVG53zHzCjkAGy6HmX5V4bLM', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f31a64f1-a8a7-40c7-b611-9c341399f3ab-0', tool_calls=[{'name': 'search_runbooks', 'args': {'query': 'Redis MEMORY STATS command syntax'}, 'id': 'call_jK7HJmFN2BRsscwu2d73Wr2V', 'type': 'tool_call'}], usage_metadata={'input_tokens': 442, 'output_tokens': 20, 'total_tokens': 462, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='# Redis MEMORY STATS Command Syntax\\n\\n## 1. Direct Answer\\nThe syntax for the Redis `MEMORY STATS` command is:\\n```\\nMEMORY STATS\\n```\\n\\n## 2. Step-by-Step Instructions\\n1. Connect to your Redis instance using `redis-cli`. For example:\\n   ```\\n   /opt/gitlab/embedded/bin/redis-cli -h <redis-host> -p <port>\\n   ```\\n2. Authenticate if necessary:\\n   ```\\n   auth <password>\\n   ```\\n3. Run the `MEMORY STATS` command:\\n   ```\\n   MEMORY STATS\\n   ```\\n4. Review the output, which will be a list of key-value pairs detailing various memory metrics.\\n\\n## 3. Key Commands\\n- To connect:\\n  ```\\n  redis-cli -h <host> -p <port>\\n  ```\\n- To authenticate:\\n  ```\\n  auth <password>\\n  ```\\n- To get memory statistics:\\n  ```\\n  MEMORY STATS\\n  ```\\n\\n## 4. Important Notes\\n- The `MEMORY STATS` command provides detailed memory usage metrics, including total allocated memory, fragmentation, and other internal statistics.\\n- Do **not** use commands like `MONITOR` or `KEYS` during production troubleshooting, as they can overload Redis.\\n- Ensure you have proper permissions and are connected to the correct Redis node (master or replica as needed).\\n- For cluster environments, run the command on the appropriate node.\\n\\nThis command is useful for diagnosing memory-related issues, understanding memory consumption patterns, and planning capacity.\\n\\n---\\n\\n**Summary:**  \\nThe syntax is simply `MEMORY STATS`. Connect via `redis-cli`, authenticate if needed, then execute the command to retrieve detailed memory metrics.', name='search_runbooks', tool_call_id='call_jK7HJmFN2BRsscwu2d73Wr2V'),\n",
              "  AIMessage(content='The syntax for the Redis MEMORY STATS command is:\\n\\n```\\nMEMORY STATS\\n```\\n\\nYou can run this command after connecting to your Redis instance using redis-cli and authenticating if necessary. It provides detailed memory usage metrics.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 596, 'total_tokens': 644, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CRp8ukIJLZQJ069m94wYvJpZQHabK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d3d86c39-e080-43c2-8c41-afd2d28910a4-0', usage_metadata={'input_tokens': 596, 'output_tokens': 48, 'total_tokens': 644, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test 5: Command-specific query (should use runbooks)\n",
        "test_agent(\"Show me the exact syntax for Redis MEMORY STATS command\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Interactive Demo\n",
        "\n",
        "Run this cell to interact with the agent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive demo\n",
        "def interactive_demo():\n",
        "    \"\"\"\n",
        "    Interactive demo of the SREnity agent\n",
        "    \"\"\"\n",
        "    print(\"🤖 SREnity Agent Demo\")\n",
        "    print(\"Ask me about SRE procedures, troubleshooting, or production issues!\")\n",
        "    print(\"Type 'quit' to exit.\\n\")\n",
        "    \n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        \n",
        "        if query.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye! 👋\")\n",
        "            break\n",
        "            \n",
        "        if query.strip():\n",
        "            test_agent(query, verbose=False)\n",
        "            print()\n",
        "\n",
        "# Uncomment to run interactive demo\n",
        "# interactive_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Graph Visualization\n",
        "\n",
        "Visualize the agent graph structure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Agent Graph Structure:\n",
            "\n",
            "Mermaid representation:\n",
            "---\n",
            "config:\n",
            "  flowchart:\n",
            "    curve: linear\n",
            "---\n",
            "graph TD;\n",
            "\t__start__([<p>__start__</p>]):::first\n",
            "\tassistant(assistant)\n",
            "\ttools(tools)\n",
            "\t__end__([<p>__end__</p>]):::last\n",
            "\t__start__ --> assistant;\n",
            "\tassistant -.-> __end__;\n",
            "\tassistant -.-> tools;\n",
            "\ttools --> assistant;\n",
            "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
            "\tclassDef first fill-opacity:0\n",
            "\tclassDef last fill:#bfb6fc\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the graph structure\n",
        "try:\n",
        "    from IPython.display import Image, display\n",
        "    \n",
        "    # Generate graph image\n",
        "    graph_image = react_graph.get_graph().draw_mermaid()\n",
        "    \n",
        "    print(\"📊 Agent Graph Structure:\")\n",
        "    print(\"\\nMermaid representation:\")\n",
        "    print(graph_image)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Could not generate graph visualization: {e}\")\n",
        "    print(\"\\nGraph structure:\")\n",
        "    print(\"START → assistant → [tools or END]\")\n",
        "    print(\"              ↑          ↓\")\n",
        "    print(\"              └──────────┘\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
