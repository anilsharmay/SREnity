"""
SDG (Synthetic Data Generation) utility functions for RAGAS evaluation.

This module provides functions for caching and loading synthetic test datasets
generated by RAGAS, as well as creating evaluation datasets from RAG chains.
"""

import json
from pathlib import Path
from typing import List, Dict, Any, Optional


def save_test_dataset(test_dataset, filename: str = "redis_sdg_questions.json") -> Path:
    """
    Save test dataset to file for reuse with correct RAGAS field mapping.
    
    Args:
        test_dataset: RAGAS Testset object containing synthetic questions
        filename: Name of the file to save the dataset to
        
    Returns:
        Path to the saved file
    """
    data_dir = Path("../data/sdg")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    # Convert test dataset to serializable format
    test_data = []
    samples = list(test_dataset)  # Convert EvaluationDataset to list
    
    for sample in samples:
        if hasattr(sample, 'eval_sample'):
            eval_sample = sample.eval_sample
            if hasattr(eval_sample, 'model_dump'):
                sample_dict = eval_sample.model_dump()
            else:
                sample_dict = eval_sample.__dict__
        else:
            # Fallback for direct access
            if hasattr(sample, 'model_dump'):
                sample_dict = sample.model_dump()
            else:
                sample_dict = sample.__dict__
        
        test_data.append({
            'user_input': sample_dict.get('user_input', 'N/A'),
            'reference': sample_dict.get('reference', 'N/A'),
            'reference_contexts': sample_dict.get('reference_contexts', ['N/A']),
            'retrieved_contexts': [],  # Will be filled during evaluation
            'response': ''  # Will be filled during evaluation
        })
    
    filepath = data_dir / filename
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, indent=2, ensure_ascii=False)
    
    print(f"Saved {len(test_data)} test questions to {filepath}")
    return filepath


def load_cached_test_questions(filename: str = "redis_sdg_questions.json") -> Optional[List[Dict[str, Any]]]:
    """
    Load cached test questions for evaluation with RAGAS field names.
    
    Args:
        filename: Name of the cached file to load
        
    Returns:
        List of test questions or None if file doesn't exist
    """
    filepath = Path("../data/sdg") / filename
    if not filepath.exists():
        print(f"Cache file not found: {filepath}")
        return None
    
    with open(filepath, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    print(f"Loaded {len(test_data)} cached test questions from {filepath}")
    return test_data


def create_evaluation_dataset(test_questions: List[Dict[str, Any]], chain) -> List[Dict[str, Any]]:
    """
    Create evaluation dataset by running RAG pipeline on test questions using Runnable chain.
    
    Args:
        test_questions: List of test questions with RAGAS field names
        chain: LangChain Runnable chain for RAG pipeline
        
    Returns:
        List of evaluation data with responses and retrieved contexts
    """
    evaluation_data = []
    
    print(f"Creating evaluation dataset from {len(test_questions)} test questions...")
    
    for i, test_item in enumerate(test_questions, 1):
        print(f"Processing question {i}/{len(test_questions)}: {test_item['user_input'][:50]}...")
        
        # Run RAG pipeline using Runnable chain
        user_input = test_item['user_input']
        reference = test_item['reference']
        reference_contexts = test_item['reference_contexts']
        
        # Get answer and contexts from our Runnable chain
        result = chain.invoke({"question": user_input})
        response = result["response"]
        retrieved_contexts = result["contexts"]  # Use pre-extracted strings
        
        # Ensure retrieved_contexts is a list of strings, not Document objects
        if retrieved_contexts and len(retrieved_contexts) > 0:
            # If it's a list of Document objects, extract the page_content
            if hasattr(retrieved_contexts[0], 'page_content'):
                retrieved_contexts = [doc.page_content for doc in retrieved_contexts]
            # If it's already strings, keep as is
            elif isinstance(retrieved_contexts[0], str):
                retrieved_contexts = retrieved_contexts
            else:
                # Convert to strings if needed
                retrieved_contexts = [str(ctx) for ctx in retrieved_contexts]
        else:
            retrieved_contexts = []
        
        evaluation_data.append({
            'user_input': user_input,
            'reference': reference,
            'reference_contexts': reference_contexts,
            'retrieved_contexts': retrieved_contexts,
            'response': response
        })
    
    return evaluation_data
